{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Model Distillation\n",
    "\n",
    "This notebook demonstrates the knowledge distillation process from Perplexity-AI/r1-1776 (guru) to our VishwamAI model (shishya). Modifications have been made to ensure compatibility between PyTorch and JAX, validate layer mappings, and enhance robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from vishwamai.distillation import VishwamaiGuruKnowledge, VishwamaiShaalaTrainer\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Verify JAX setup\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Perplexity-R1 distillation configuration\n",
    "try:\n",
    "    config = OmegaConf.load('vishwamai/configs/training/perplexity_r1_distillation.yaml')\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Configuration file not found. Please check the path.\")\n",
    "\n",
    "# Display configuration with validation\n",
    "print(\"Teacher Model:\", config.teacher_model.path)\n",
    "print(\"Student Model:\", config.student_model.path)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"Hidden Size: {config.student_model.hidden_size}\")\n",
    "print(f\"Num Layers: {config.student_model.num_layers}\")\n",
    "print(f\"Attention Heads: {config.student_model.num_heads}\")\n",
    "print(f\"KV Heads: {config.student_model.num_kv_heads}\")\n",
    "\n",
    "print(\"\\nDistillation Parameters:\")\n",
    "print(f\"Temperature: {config.teacher_model.temperature}\")\n",
    "print(f\"Alpha: {config.teacher_model.alpha}\")\n",
    "print(f\"Feature Layers: {config.distillation.feature_distillation.layers}\")\n",
    "\n",
    "# Validate key parameters\n",
    "assert config.teacher_model.temperature > 0, \"Temperature must be positive\"\n",
    "assert 0 <= config.teacher_model.alpha <= 1, \"Alpha must be between 0 and 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Teacher Model (Guru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the teacher model (Perplexity r1-1776)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(config.teacher_model.path)\n",
    "teacher_config = AutoConfig.from_pretrained(config.teacher_model.path)\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.teacher_model.path,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16  # Specify dtype for consistency\n",
    ")\n",
    "\n",
    "# Move to evaluation mode and ensure GPU placement\n",
    "teacher_model.eval()\n",
    "print(f\"Teacher model loaded: {teacher_config.model_type}\")\n",
    "print(f\"Teacher layers: {teacher_config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Student Model (Shishya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize student model with explicit parameters\n",
    "student_config = ModelConfig(\n",
    "    vocab_size=teacher_config.vocab_size,\n",
    "    hidden_size=config.student_model.hidden_size,\n",
    "    num_layers=config.student_model.num_layers,\n",
    "    num_attention_heads=config.student_model.num_heads,\n",
    "    intermediate_size=config.student_model.intermediate_size,\n",
    "    max_position_embeddings=config.student_model.max_seq_len,\n",
    "    use_flash_attention=True,\n",
    "    use_gqa=True,\n",
    "    num_key_value_heads=config.student_model.num_kv_heads\n",
    ")\n",
    "\n",
    "student_model = VishwamAIModel(student_config)\n",
    "print(f\"Student model initialized with {student_config.num_layers} layers\")\n",
    "\n",
    "# Validate layer mapping for feature distillation\n",
    "if config.distillation.feature_distillation.layers:\n",
    "    assert max(config.distillation.feature_distillation.layers) < student_config.num_layers, \\\n",
    "        \"Feature distillation layers exceed student model layers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and prepare training data\n",
    "try:\n",
    "    train_dataset = load_dataset(config.data.path, split='train')\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to load dataset from {config.data.path}: {str(e)}\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize inputs using teacher's tokenizer\n",
    "    model_inputs = teacher_tokenizer(\n",
    "        examples['text'],\n",
    "        max_length=config.model.max_seq_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'  # Return PyTorch tensors initially\n",
    "    )\n",
    "    return {k: v.numpy() for k, v in model_inputs.items()}  # Convert to numpy for JAX compatibility\n",
    "\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(tokenized_dataset)}\")\n",
    "# Validate dataset\n",
    "assert len(tokenized_dataset) > 0, \"Tokenized dataset is empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the guru-shishya training process\n",
    "trainer = VishwamaiShaalaTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "# Initialize training state with seed\n",
    "rng = jax.random.PRNGKey(config.training.seed)\n",
    "state = trainer.create_train_state(rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training loop with progress tracking and error handling\n",
    "num_epochs = config.training.num_epochs\n",
    "batch_size = config.training.batch_size\n",
    "steps_per_epoch = len(tokenized_dataset) // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm(total=steps_per_epoch, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "        dataloader = trainer.get_train_dataloader(tokenized_dataset)\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                # Ensure batch is in JAX format\n",
    "                batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "                state, loss_dict, rng = trainer.train_step(state, batch, step, rng)\n",
    "                \n",
    "                if step % config.training.log_every == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f\"{float(loss_dict['total_loss']):.4f}\",\n",
    "                        'kd_loss': f\"{float(loss_dict['kd_loss']):.4f}\"\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in step {step}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        # Save checkpoint at end of epoch\n",
    "        if (epoch + 1) % config.training.save_every == 0:\n",
    "            checkpoint_path = f\"{config.student_model.path}/checkpoint-{epoch+1}\"\n",
    "            trainer.save_checkpoint(state, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the final distilled model with metadata\n",
    "model_card = {\n",
    "    \"base_model\": \"perplexity-ai/r1-1776\",\n",
    "    \"model_type\": \"distilled-language-model\",\n",
    "    \"distillation_method\": \"VishwamAI guru-shishya knowledge transfer\",\n",
    "    \"architecture\": {\n",
    "        \"hidden_size\": config.student_model.hidden_size,\n",
    "        \"num_layers\": config.student_model.num_layers,\n",
    "        \"num_heads\": config.student_model.num_heads,\n",
    "        \"num_kv_heads\": config.student_model.num_kv_heads\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"distillation_temperature\": config.teacher_model.temperature,\n",
    "        \"alpha\": config.teacher_model.alpha,\n",
    "        \"epochs\": config.training.num_epochs\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer.save_model(\n",
    "    state,\n",
    "    config.student_model.path,\n",
    "    push_to_hub=True,\n",
    "    model_card=model_card\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Distillation complete!\")\n",
    "print(f\"ðŸ’¡ Model saved to: {config.student_model.path}\")\n",
    "print(\"ðŸ“Š Model card and weights pushed to Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Quick validation test\n",
    "test_input = \"What is the capital of France?\"\n",
    "\n",
    "# Get teacher output\n",
    "teacher_output = trainer.generate_text(teacher_model, test_input, max_length=50)\n",
    "print(\"Teacher output:\", teacher_output)\n",
    "\n",
    "# Get student output\n",
    "student_output = trainer.generate_text(student_model, test_input, max_length=50)\n",
    "print(\"Student output:\", student_output)\n",
    "\n",
    "# Basic output validation\n",
    "assert isinstance(teacher_output, str), \"Teacher output is not a string\"\n",
    "assert isinstance(student_output, str), \"Student output is not a string\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
