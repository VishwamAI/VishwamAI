distillation:
  teacher_model:
    path: "Qwen/QwQ-32B"
    config:
      hidden_size: 7168
      intermediate_size: 28672
      num_attention_heads: 56
      num_layers: 60
      num_key_value_heads: 8
      vocab_size: 151936
      max_position_embeddings: 2048
      hidden_dropout_prob: 0.1
      attention_dropout_prob: 0.1
      initializer_range: 0.02
      layer_norm_eps: 1e-5
      use_cache: true
      pad_token_id: 0
      bos_token_id: 1
      eos_token_id: 2
      tie_word_embeddings: true
      use_flash_attention: true
      use_rope: true
      use_alibi: false
      use_gqa: true
      dtype: "bfloat16"

  student_model:
    config:
      hidden_size: 2048
      intermediate_size: 8192
      num_attention_heads: 32
      num_layers: 24
      num_key_value_heads: 8
      vocab_size: 151936
      max_position_embeddings: 2048
      hidden_dropout_prob: 0.1
      attention_dropout_prob: 0.1
      initializer_range: 0.02
      layer_norm_eps: 1e-5
      use_cache: true
      pad_token_id: 0
      bos_token_id: 1
      eos_token_id: 2
      tie_word_embeddings: true
      use_flash_attention: true
      use_rope: true
      use_alibi: false
      use_gqa: true
      dtype: "bfloat16"

training:
  batch_size: 1  # Per device
  gradient_accumulation_steps: 16  # Total effective batch size = 16
  learning_rate: 5e-5
  warmup_steps: 1000
  max_steps: 50000
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  use_flash_attention: true
  use_dualpipe: true
  use_eplb: true

memory_optimization:
  chunk_size: 32  # Size for chunked loading
  clear_cache_steps: 10  # Steps between memory clearing
  prefetch_blocks: 2  # Number of blocks to prefetch
  max_memory_gb: 80  # Maximum memory target in GB

distillation_params:
  temperature: 2.0
  alpha_ce: 0.2
  alpha_kd: 0.8
  feature_loss_weight: 0.1
  attention_loss_weight: 0.1
  quantization:
    enabled: true
    bits: 8
    scheme: "symmetric"

tpu_config:
  use_bfloat16: true
  use_dynamic_scale: true
  use_scanned_attention: true
  attention_partition_spec: "DP"  # Data parallel
  preallocate: false
  memory_fraction: 0.95
