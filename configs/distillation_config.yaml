distillation:
  teacher_model:
    path: "Qwen/QwQ-32B"
    config:
      hidden_size: 7168
      intermediate_size: 28672
      num_attention_heads: 56
      num_layers: 60
      num_key_value_heads: 8
      vocab_size: 151936
      max_position_embeddings: 2048
      hidden_dropout_prob: 0.1
      attention_dropout_prob: 0.1
      initializer_range: 0.02
      layer_norm_eps: 1e-5
      use_cache: true
      pad_token_id: 0
      bos_token_id: 1
      eos_token_id: 2
      tie_word_embeddings: true
      use_flash_attention: true
      use_rope: true
      use_alibi: false
      use_gqa: true
      dtype: "bfloat16"

  student_model:
    config:
      hidden_size: 2048
      intermediate_size: 8192
      num_attention_heads: 32
      num_layers: 24
      num_key_value_heads: 8
      vocab_size: 151936
      max_position_embeddings: 2048
      hidden_dropout_prob: 0.1
      attention_dropout_prob: 0.1
      initializer_range: 0.02
      layer_norm_eps: 1e-5
      use_cache: true
      pad_token_id: 0
      bos_token_id: 1
      eos_token_id: 2
      tie_word_embeddings: true
      use_flash_attention: true
      use_rope: true
      use_alibi: false
      use_gqa: true
      dtype: "bfloat16"

training:
  batch_size: 1  # Per device batch size
  gradient_accumulation_steps: 16
  learning_rate: 5e-5
  warmup_steps: 1000
  max_steps: 50000
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  use_flash_attention: true
  use_dualpipe: true
  use_eplb: true

memory_optimization:
  chunk_size: 32  # Default chunk size for loading
  clear_cache_steps: 10  # Steps between memory clearing
  prefetch_blocks: 2  # Number of blocks to prefetch
  max_memory_gb: 80  # Maximum memory target
  cache_size_gb: 4  # Size of tensor cache
  stream_chunks: true  # Enable streaming chunks
  optimize_conv_ops: true  # Optimize convolution operations
  low_memory_mode: true  # Enable low memory mode optimizations
  tensor_parallel_size: 8  # Number of tensor parallel shards
  shard_strategy: "auto"  # Auto-determine best sharding strategy
  memory_monitoring:
    enabled: true
    log_frequency: 100  # Steps between memory logs
    alert_threshold_gb: 70  # Memory alert threshold

distillation_params:
  temperature: 2.0
  alpha_ce: 0.2  # Cross entropy loss weight
  alpha_kd: 0.8  # Knowledge distillation loss weight
  feature_loss_weight: 0.1  # Feature matching loss weight
  attention_loss_weight: 0.1  # Attention matching loss weight
  dynamic_temperature: true  # Enable dynamic temperature
  min_temperature: 1.0
  max_temperature: 4.0
  layer_adaptative_weights: true  # Layer-wise adaptive weights
  quantization:
    enabled: true
    bits: 8
    scheme: "symmetric"
    calibration_samples: 100

tpu_config:
  use_bfloat16: true
  use_dynamic_scale: true
  use_scanned_attention: true
  attention_partition_spec: "DP"  # Data parallel
  preallocate: false  # Don't preallocate memory
  memory_fraction: 0.95  # Memory fraction to use
  enforce_memory_usage: true
  pjit_barrier_sync: false
  skip_xla_compilation_cache: true
  force_memory_defrag: true
  pad_vocab_size: false
  checkpoint_in_bfloat16: true
  use_hardware_rng: true
  opt_level: 3  # Maximum optimization level
  max_comp_cache_size_gb: 10
  prefer_int8_quantize: true
  enforce_deterministic_ops: false

monitoring:
  aim_experiment: "QwQ-32B-Distillation"
  tensorboard: true
  profile_steps: [100, 200, 300]  # Steps to profile
  memory_profile_frequency: 100
  log_metrics:
    - loss
    - kd_loss
    - feature_loss
    - attention_loss
    - memory_usage
    - throughput
  alert_on:
    memory_threshold_gb: 70
    loss_spike: 2.0
    training_stall: 300  # Seconds
