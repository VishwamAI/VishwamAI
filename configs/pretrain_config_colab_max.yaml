experiment:
  name: vishwamai_pretrain_colab_max
  version: "1.0"
  platform: colab_pro

hardware:
  gpu_type: A100  # Colab Pro's best GPU
  gpu_memory: 40GB
  ram: 52GB
  runtime_restrictions: true
  tpu_access: false

model:
  # Maximum viable size for Colab Pro A100
  total_params: 13B  # Maximum practical size
  architecture:
    hidden_size: 3072
    num_layers: 36
    num_heads: 24
    intermediate_size: 12288
    max_position_embeddings: 2048
    vocab_size: 32000
  
  memory_optimizations:
    gradient_checkpointing: true
    activation_checkpointing: true
    cpu_offloading: true
    attention_implementation: "flash_attention"
    memory_efficient_attention: true
    selective_activation_caching: true
    
  precision:
    training: "fp8"  # Use 8-bit training
    inference: "int8"
    static_quantization: true
    kv_cache_quantization: true
    
  efficiency:
    parameter_sharing: true
    adaptive_batch_size: true
    progressive_layer_dropping: true
    mixed_device_execution: true

training:
  scheduling:
    micro_batch_size: 1
    gradient_accumulation_steps: 128
    effective_batch_size: 128  # micro_batch * accumulation
    max_steps_per_session: 500  # Colab runtime limit consideration
    checkpoint_frequency_steps: 50
    
  optimization:
    optimizer: "adafactor"  # Memory efficient optimizer
    learning_rate:
      initial: 5e-5
      min: 1e-6
      schedule: "cosine"
      warmup_steps: 100
    gradient_clipping: 1.0
    weight_decay: 0.01
    
  memory_management:
    dynamic_memory_allocation: true
    garbage_collection_interval: 50
    cuda_empty_cache_frequency: 10
    pin_memory: false  # Save GPU memory
    
  runtime_adaptation:
    auto_reduce_batch: true
    monitoring_interval_seconds: 30
    memory_threshold_gb: 38  # Leave 2GB buffer
    recovery_strategies:
      - reduce_batch_size
      - increase_gradient_accumulation
      - reduce_precision
      
  checkpointing:
    save_strategy:
      format: "safetensors"
      compression: true
      shard_size: 2GB
    google_drive_backup: true
    keep_last_checkpoints: 2

data:
  streaming: true  # Enable dataset streaming
  prefetch_factor: 2
  num_workers: 2
  datasets:
    # Prioritize smaller, high-quality datasets
    primary:
      - name: "openai/gsm8k"
        weight: 0.3
      - name: "cais/mmlu"
        weight: 0.3
    secondary:
      - name: "wikimedia/wikipedia"
        weight: 0.2
        streaming: true
      - name: "sentence-transformers/codesearchnet"
        weight: 0.2
        streaming: true

monitoring:
  memory_tracking: true
  performance_monitoring: true
  auto_shutdown_on_memory_error: true
  alerts:
    memory_threshold: 0.95
    runtime_remaining_minutes: 30

continuation:
  save_state_frequency_minutes: 20
  checkpoint_recovery: true
  session_management:
    max_runtime_hours: 11
    cool_down_minutes: 30
    auto_restart: true
