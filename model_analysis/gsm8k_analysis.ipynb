{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI GSM8K Model Analysis\n",
    "\n",
    "This notebook analyzes the performance of the VishwamAI model on the GSM8K dataset, focusing on:\n",
    "1. Step-by-step solution accuracy\n",
    "2. Reasoning patterns\n",
    "3. Error analysis\n",
    "4. Performance comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from vishwamai.model.transformer import create_transformer_model\n",
    "from vishwamai.utils.visualization import plot_attention_patterns\n",
    "from vishwamai.utils.profiling import analyze_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model from HuggingFace\n",
    "model_name = \"VishwamAI/VishwamAI\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = create_transformer_model.from_pretrained(model_name)\n",
    "\n",
    "# Load test dataset\n",
    "test_data = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_solution_steps(\n",
    "    question: str,\n",
    "    generated_answer: str,\n",
    "    reference_answer: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyze solution steps for a math problem.\"\"\"\n",
    "    \n",
    "    # Split into steps\n",
    "    gen_steps = generated_answer.split('\\n')\n",
    "    ref_steps = reference_answer.split('\\n')\n",
    "    \n",
    "    # Extract numbers from each step\n",
    "    gen_numbers = [extract_numbers(step) for step in gen_steps]\n",
    "    ref_numbers = [extract_numbers(step) for step in ref_steps]\n",
    "    \n",
    "    # Compare steps\n",
    "    step_matches = []\n",
    "    for gen_step, ref_step in zip(gen_steps, ref_steps):\n",
    "        step_matches.append({\n",
    "            \"generated\": gen_step,\n",
    "            \"reference\": ref_step,\n",
    "            \"numbers_match\": set(gen_numbers[i]) == set(ref_numbers[i])\n",
    "        })\n",
    "        \n",
    "    return {\n",
    "        \"num_steps\": len(gen_steps),\n",
    "        \"step_accuracy\": sum(s[\"numbers_match\"] for s in step_matches) / len(step_matches),\n",
    "        \"steps\": step_matches\n",
    "    }\n",
    "\n",
    "# Analyze a batch of examples\n",
    "results = []\n",
    "for example in test_data[:100]:  # Analyze first 100 examples\n",
    "    generated = generate_answer(model, tokenizer, example[\"question\"])\n",
    "    analysis = analyze_solution_steps(\n",
    "        example[\"question\"],\n",
    "        generated,\n",
    "        example[\"answer\"]\n",
    "    )\n",
    "    results.append(analysis)\n",
    "    \n",
    "# Plot step accuracy distribution\n",
    "accuracies = [r[\"step_accuracy\"] for r in results]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(accuracies, bins=20)\n",
    "plt.title(\"Distribution of Step-by-Step Accuracy\")\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Utilization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_expert_usage(model, inputs: List[Dict[str, torch.Tensor]]):\n",
    "    \"\"\"Analyze which experts are used for different math operations.\"\"\"\n",
    "    expert_assignments = []\n",
    "    \n",
    "    # Get expert assignments for each input\n",
    "    with torch.no_grad():\n",
    "        for inp in inputs:\n",
    "            outputs = model(\n",
    "                **inp,\n",
    "                output_router_logits=True,\n",
    "                output_attentions=True\n",
    "            )\n",
    "            expert_assignments.append(outputs[\"router_logits\"])\n",
    "            \n",
    "    # Analyze patterns\n",
    "    expert_specialization = analyze_expert_patterns(expert_assignments)\n",
    "    \n",
    "    # Visualize\n",
    "    plot_expert_heatmap(expert_specialization)\n",
    "    \n",
    "# Run analysis\n",
    "inputs = prepare_batch(test_data[:50], tokenizer)\n",
    "analyze_expert_usage(model, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_attention_patterns(model, example):\n",
    "    \"\"\"Analyze multi-level attention patterns.\"\"\"\n",
    "    # Prepare input\n",
    "    inputs = tokenizer(\n",
    "        example[\"question\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Get attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_attentions=True,\n",
    "            output_attention_levels=True\n",
    "        )\n",
    "        \n",
    "    # Visualize attention at different levels\n",
    "    attention_weights = outputs[\"attentions\"]\n",
    "    attention_levels = outputs[\"attention_levels\"]\n",
    "    \n",
    "    plot_attention_patterns(\n",
    "        attention_weights,\n",
    "        attention_levels,\n",
    "        tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    )\n",
    "    \n",
    "# Analyze a complex example\n",
    "complex_example = find_complex_example(test_data)\n",
    "analyze_attention_patterns(model, complex_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def categorize_errors(results):\n",
    "    \"\"\"Categorize different types of errors.\"\"\"\n",
    "    error_categories = {\n",
    "        \"numerical\": [],  # Wrong calculations\n",
    "        \"reasoning\": [],  # Wrong logic\n",
    "        \"steps\": [],     # Missing/extra steps\n",
    "        \"context\": []    # Misunderstanding context\n",
    "    }\n",
    "    \n",
    "    for result in results:\n",
    "        if result[\"step_accuracy\"] < 1.0:\n",
    "            error_type = analyze_error_type(result)\n",
    "            error_categories[error_type].append(result)\n",
    "            \n",
    "    # Plot error distribution\n",
    "    counts = [len(v) for v in error_categories.values()]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(error_categories.keys(), counts)\n",
    "    plt.title(\"Error Type Distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "# Analyze errors\n",
    "categorize_errors(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_performance():\n",
    "    \"\"\"Compare with other models.\"\"\"\n",
    "    models = {\n",
    "        \"VishwamAI\": model,\n",
    "        \"GPT-3.5\": load_comparison_results(\"gpt35\"),\n",
    "        \"PaLM\": load_comparison_results(\"palm\"),\n",
    "        \"Claude\": load_comparison_results(\"claude\")\n",
    "    }\n",
    "    \n",
    "    metrics = [\n",
    "        \"accuracy\",\n",
    "        \"step_accuracy\",\n",
    "        \"reasoning_score\",\n",
    "        \"efficiency_score\"\n",
    "    ]\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plot_model_comparison(models, metrics)\n",
    "    \n",
    "# Run comparison\n",
    "compare_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Summary of findings:\n",
    "1. Step accuracy distribution\n",
    "2. Expert specialization patterns\n",
    "3. Attention level utilization\n",
    "4. Common error patterns\n",
    "5. Performance comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
