{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Distilling Step by Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Student Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the teacher (LLM) and student model\n",
    "STUDENT_MODEL = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(STUDENT_MODEL)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small Hugging Face dataset (Change this to your preferred dataset)\n",
    "dataset = load_dataset(\"sst2\", split=\"train[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEACHER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the chat model\n",
    "llm_engine = ChatOllama(\n",
    "    model=\"gemma3:latest\",  # Changed to match the installed model name from your 'ollama list' output\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "def generate_rationale(input_text):\n",
    "    \"\"\"\n",
    "    Uses Ollama's gemma model to generate a step-by-step rationale for the given input.\n",
    "    \"\"\"\n",
    "    prompt = f\"Explain step-by-step reasoning before answering: {input_text}\"\n",
    "    \n",
    "    response = llm_engine.invoke(prompt)  # Using LangChain's invoke method\n",
    "    \n",
    "    return response.content if hasattr(response, \"content\") else response\n",
    "\n",
    "print(generate_rationale(\"Explain AI in one sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset with rationales\n",
    "def process_data(example):\n",
    "    input_text = example[\"sentence\"]  # Change this depending on your dataset format\n",
    "    rationale = generate_rationale(input_text)\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    # Tokenize input and rationale\n",
    "    input_enc = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    rationale_enc = tokenizer(rationale, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"labels\": label,\n",
    "        \"rationale_ids\": rationale_enc[\"input_ids\"],\n",
    "        \"rationale_mask\": rationale_enc[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Apply function to dataset\n",
    "processed_dataset = dataset.map(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming 'dataset' is your Dataset object\n",
    "processed_dataset.save_to_disk('preprocessed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the saved directory\n",
    "processed_dataset = load_from_disk('preprocessed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save the model and checkpoints\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        rationale_ids = inputs.pop(\"rationale_ids\", None)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        label_loss = loss_fn(outputs.logits, labels)\n",
    "        \n",
    "        if rationale_ids is not None:\n",
    "            rationale_outputs = model(input_ids=rationale_ids, attention_mask=inputs[\"attention_mask\"])\n",
    "            rationale_loss = loss_fn(rationale_outputs.logits, rationale_ids)\n",
    "            loss = label_loss + 0.5 * rationale_loss  # Weighted loss\n",
    "        else:\n",
    "            loss = label_loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    eval_dataset=processed_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./results\")\n",
    "print(\"âœ… Distillation Complete! Smaller model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For AutoTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "data = {\n",
    "    \"text\": [],\n",
    "    \"rationale\": [],\n",
    "    \"target\": []\n",
    "}\n",
    "\n",
    "for example in dataset:\n",
    "    input_text = example[\"sentence\"]\n",
    "    label = example[\"label\"]\n",
    "    rationale = generate_rationale(input_text)\n",
    "    \n",
    "    data[\"text\"].append(input_text)\n",
    "    data[\"rationale\"].append(rationale)\n",
    "    data[\"target\"].append(label)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
