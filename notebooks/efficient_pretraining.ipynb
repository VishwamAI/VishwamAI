{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Pre-training on Limited TPU Resources\n",
    "\n",
    "This notebook demonstrates how to use VishwamAI's efficient pre-training capabilities with curriculum learning and TPU optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import jax\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from vishwamai.model import VishwamAIModel\n",
    "from vishwamai.tokenizer import VishwamAITokenizer\n",
    "from vishwamai.pretrain_efficient import setup_tpu_devices, create_model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TPU Setup and Configuration\n",
    "\n",
    "First, let's set up our TPU environment and load our optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up TPU devices with optimal settings\n",
    "devices = setup_tpu_devices()\n",
    "print(f\"Available devices: {jax.device_count()}\")\n",
    "\n",
    "# Load our efficient pre-training config\n",
    "config = OmegaConf.load(\"../vishwamai/configs/training/efficient_pretrain.yaml\")\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(OmegaConf.to_yaml(config.training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model and Tokenizer Initialization\n",
    "\n",
    "Now we'll create our model and tokenizer with the optimized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create model with TPU optimizations\n",
    "model_config = create_model_config(config)\n",
    "model = VishwamAIModel(model_config)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = VishwamAITokenizer(vocab_size=config.model.vocab_size)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.size for p in jax.tree_leaves(model.params)):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curriculum Learning Demo\n",
    "\n",
    "Let's examine how curriculum learning progresses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from vishwamai.training import DataProcessor\n",
    "\n",
    "# Create data processor with curriculum learning\n",
    "data_processor = DataProcessor(tokenizer, config)\n",
    "\n",
    "# Demonstrate curriculum progression\n",
    "print(\"Curriculum Learning Progress:\")\n",
    "print(f\"Initial sequence length: {data_processor.curriculum_scheduler['current_max_length']}\")\n",
    "\n",
    "# Simulate curriculum updates\n",
    "for step in range(3):\n",
    "    data_processor.update_curriculum()\n",
    "    if step > 0 and step % (config.training.curriculum.update_every - 1) == 0:\n",
    "        print(f\"Step {step+1}: sequence length = {data_processor.curriculum_scheduler['current_max_length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mixed Precision Training\n",
    "\n",
    "Demonstrate the memory benefits of mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sample batch\n",
    "batch_size = config.training.batch_size\n",
    "seq_length = 64  # Start with curriculum's initial length\n",
    "\n",
    "# Compare memory usage\n",
    "sample_fp32 = np.random.randn(batch_size, seq_length, config.model.hidden_size).astype(np.float32)\n",
    "sample_bf16 = sample_fp32.astype(np.float16)\n",
    "\n",
    "print(\"Memory Usage Comparison:\")\n",
    "print(f\"FP32: {sample_fp32.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"BF16: {sample_bf16.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Memory Savings: {(1 - sample_bf16.nbytes/sample_fp32.nbytes) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Example\n",
    "\n",
    "Run a small training example to demonstrate all optimizations working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from vishwamai.training import train, create_train_dataloader, create_val_dataloader\n",
    "\n",
    "# Modify config for quick demo\n",
    "demo_config = OmegaConf.create(config)\n",
    "demo_config.training.max_steps = 100\n",
    "demo_config.training.log_every_n_steps = 10\n",
    "demo_config.monitoring.save_every_n_steps = 50\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_train_dataloader(demo_config, tokenizer)\n",
    "val_loader = create_val_dataloader(demo_config, tokenizer)\n",
    "\n",
    "# Run training demo\n",
    "checkpoint_dir = \"demo_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "final_state = train(\n",
    "    model=model,\n",
    "    config=demo_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    num_steps=demo_config.training.max_steps,\n",
    "    log_every=demo_config.training.log_every_n_steps,\n",
    "    eval_every=demo_config.monitoring.save_every_n_steps,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    accum_steps=demo_config.training.gradient_accumulation_steps,\n",
    "    mesh=devices\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Demo Complete!\")\n",
    "print(f\"Final Metrics: {final_state.best_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory and Performance Analysis\n",
    "\n",
    "Examine training efficiency metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def format_metrics(metrics):\n",
    "    return {\n",
    "        'loss': f\"{metrics['loss']:.4f}\",\n",
    "        'accuracy': f\"{metrics['accuracy']:.4f}\",\n",
    "        'learning_rate': f\"{metrics.get('learning_rate', 0.0):.6f}\"\n",
    "    }\n",
    "\n",
    "print(\"Training Efficiency Summary:\")\n",
    "print(f\"Steps Completed: {final_state.step}\")\n",
    "print(f\"Best Metrics: {format_metrics(final_state.best_metrics)}\")\n",
    "\n",
    "if hasattr(final_state, 'tot_state') and final_state.tot_state['enabled']:\n",
    "    print(f\"\\nTree of Thoughts Performance:\")\n",
    "    print(f\"Thoughts per batch: {final_state.tot_state['thoughts_per_batch']}\")\n",
    "    print(f\"Best thought score: {final_state.tot_state['best_thought_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
