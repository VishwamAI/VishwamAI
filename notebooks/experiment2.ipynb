{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Pre-training Experiment\n",
    "\n",
    "This notebook implements TPU v2 optimized pre-training using:\n",
    "- Model: Custom Phi-1.6 TPU implementation (microsoft/phi-4)\n",
    "- Dataset: GSM8K\n",
    "- Hardware: TPU v2\n",
    "- Training Types: Normal and Distillation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import jax.tools.colab_tpu\n",
    "from vishwamai.transformer import (\n",
    "    create_vishwamai_transformer,\n",
    "    create_train_state,\n",
    "    train_step,\n",
    "    evaluate_step,\n",
    "    setup_distributed_training,\n",
    "    data_loader\n",
    ")\n",
    "\n",
    "# Check TPU configuration\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"Number of devices:\", jax.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize TPU system\n",
    "jax.tools.colab_tpu.setup_tpu()\n",
    "\n",
    "# Update model configuration\n",
    "config['model_config'].update({\n",
    "    'vocab_size': 32000,\n",
    "    'num_layers': 12,\n",
    "    'num_heads': 12,\n",
    "    'head_dim': 64,\n",
    "    'hidden_dim': 768,\n",
    "    'mlp_dim': 3072,\n",
    "    'max_seq_len': 2048,\n",
    "    'use_flash_attn': True,\n",
    "    'use_rotary': True,\n",
    "    'use_rms_norm': True,\n",
    "    'dtype': 'bfloat16',\n",
    "    'compute_dtype': 'float32'\n",
    "})\n",
    "\n",
    "# Update training configuration\n",
    "config['training'].update({\n",
    "    'batch_size': 32 * jax.device_count(),  # Scale batch size by number of devices\n",
    "    'learning_rate': 1e-4,\n",
    "    'warmup_steps': 2000,\n",
    "    'decay_steps': 50000,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_checkpointing': True,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'mixed_precision': True,\n",
    "    'tpu_iterations_per_loop': 100\n",
    "})\n",
    "\n",
    "print(\"Model configuration:\", config['model_config'])\n",
    "print(\"\\nTraining configuration:\", config['training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and update config for TPU v2\n",
    "with open('vishwamai/configs/config_16b.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Update config for TPU v2 optimizations\n",
    "config['tpu_config'].update({\n",
    "    \"num_devices\": 8,\n",
    "    \"batch_partition_size\": 8,\n",
    "    \"model_partition_size\": 4,\n",
    "    \"block_size\": 128,\n",
    "    \"use_f8_training\": True,\n",
    "    \"rematerialize\": True\n",
    "})\n",
    "\n",
    "config['model_config'].update({\n",
    "    \"use_enhanced\": True,\n",
    "    \"use_rotary\": True,\n",
    "    \"use_flash_attn\": True,\n",
    "    \"use_rms_norm\": True,\n",
    "    \"dtype\": \"bfloat16\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load GSM8K dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", split=\"train\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = create_tokenizer(config['model_config'])\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess examples for TPU training\"\"\"\n",
    "    texts = [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "    \n",
    "    # Tokenize with padding\n",
    "    max_length = config['model_config']['max_seq_len']\n",
    "    tokenized = tokenizer.batch_encode(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(tokenized['input_ids']),\n",
    "        'attention_mask': np.array(tokenized['attention_mask'])\n",
    "    }\n",
    "\n",
    "# Preprocess dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_size = int(0.9 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "batch_size = config['training_config']['batch_size'] * config['tpu_config']['num_devices']\n",
    "train_loader = data_loader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = data_loader(eval_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create model and training state\n",
    "print(\"Initializing model...\")\n",
    "rng = jax.random.PRNGKey(config['seed'])\n",
    "model = create_vishwamai_transformer(config['model_config'])\n",
    "\n",
    "# Create training state\n",
    "state = create_train_state(\n",
    "    rng=rng,\n",
    "    config=config,\n",
    "    learning_rate_schedule=lambda step: config['training_config']['learning_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create student model and training state\n",
    "print(\"Initializing distillation training...\")\n",
    "distill_rng = jax.random.PRNGKey(config['seed'] + 1)\n",
    "\n",
    "# Initialize distillation training state\n",
    "distill_state = create_distillation_train_state(\n",
    "    rng=distill_rng,\n",
    "    config=config,\n",
    "    learning_rate_schedule=lambda step: config['training_config']['learning_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(state, is_distill=False):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    train_metrics = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # Get next PRNG key\n",
    "        rng = jax.random.PRNGKey(int(time.time()))\n",
    "        \n",
    "        # Training step\n",
    "        if is_distill:\n",
    "            state, metrics = distillation_train_step(\n",
    "                state=state,\n",
    "                batch=batch,\n",
    "                dropout_rng=rng,\n",
    "                temperature=config['distillation_config']['temperature'],\n",
    "                alpha=config['distillation_config']['alpha']\n",
    "            )\n",
    "        else:\n",
    "            state, metrics = train_step(\n",
    "                state=state,\n",
    "                batch=batch,\n",
    "                dropout_rng=rng\n",
    "            )\n",
    "            \n",
    "        train_metrics.append(metrics)\n",
    "    \n",
    "    # Compute mean of metrics\n",
    "    metrics_np = jax.device_get(train_metrics)\n",
    "    metrics_mean = {\n",
    "        k: np.mean([metrics[k] for metrics in metrics_np])\n",
    "        for k in metrics_np[0]\n",
    "    }\n",
    "    \n",
    "    return state, metrics_mean\n",
    "\n",
    "def evaluate(state):\n",
    "    \"\"\"Run evaluation\"\"\"\n",
    "    eval_metrics = []\n",
    "    \n",
    "    for batch in eval_loader:\n",
    "        metrics = evaluate_step(state, batch)\n",
    "        eval_metrics.append(metrics)\n",
    "    \n",
    "    # Compute mean of metrics\n",
    "    metrics_np = jax.device_get(eval_metrics)\n",
    "    metrics_mean = {\n",
    "        k: np.mean([metrics[k] for metrics in metrics_np])\n",
    "        for k in metrics_np[0]\n",
    "    }\n",
    "    \n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Normal training\n",
    "print(\"Starting normal training...\")\n",
    "num_epochs = config['training_config']['num_epochs']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    state, train_metrics = train_epoch(state)\n",
    "    print(f\"Training metrics: {train_metrics}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_metrics = evaluate(state)\n",
    "    print(f\"Evaluation metrics: {eval_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distillation training\n",
    "print(\"\\nStarting distillation training...\")\n",
    "\n",
    "save_dir = 'experiment2_final'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "trainer.save_checkpoint(f\"{save_dir}/model\")\n",
    "\n",
    "# Save configuration\n",
    "with open(f\"{save_dir}/config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Model and configuration saved to {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
