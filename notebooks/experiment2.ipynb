{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Experiment 2: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates training and evaluating a VishwamAI transformer model with distillation and TPU optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install jax[tpu] flax optax sentencepiece transformers tqdm safetensors duckdb dm-haiku matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import optax\n",
    "import haiku as hk\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from vishwamai import (\n",
    "    create_vishwamai_transformer,\n",
    "    create_train_state,\n",
    "    VishwamAITrainer,\n",
    "    DuckDBLogger,\n",
    "    compute_distillation_loss\n",
    ")\n",
    "\n",
    "# Enable TPU if available\n",
    "if 'TPU_NAME' in os.environ:\n",
    "    print(f\"TPU devices: {jax.devices()}\")\n",
    "else:\n",
    "    print(f\"Using devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config\n",
    "config = {\n",
    "    'model_config': {\n",
    "        'vocab_size': 32000,\n",
    "        'num_layers': 12,\n",
    "        'num_heads': 12,\n",
    "        'hidden_dim': 768,\n",
    "        'mlp_dim': 3072,\n",
    "        'max_seq_len': 512,\n",
    "        'dropout_rate': 0.1,\n",
    "        'attention_dropout_rate': 0.1,\n",
    "        'use_flash_attn': True,\n",
    "        'use_rotary': True\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 1e-4,\n",
    "        'warmup_steps': 1000,\n",
    "        'max_steps': 10000,\n",
    "        'eval_frequency': 500,\n",
    "        'save_frequency': 1000\n",
    "    },\n",
    "    'distillation': {\n",
    "        'temperature': 2.0,\n",
    "        'alpha': 0.5,\n",
    "        'label_smoothing': 0.1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_data(num_samples=1000):\n",
    "    \"\"\"Create dummy data for demonstration\"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "    \n",
    "    # Create random input sequences\n",
    "    input_ids = rng.integers(\n",
    "        low=0,\n",
    "        high=config['model_config']['vocab_size'],\n",
    "        size=(num_samples, config['model_config']['max_seq_len'])\n",
    "    )\n",
    "    \n",
    "    # Create random labels\n",
    "    labels = rng.integers(\n",
    "        low=0,\n",
    "        high=config['model_config']['vocab_size'],\n",
    "        size=(num_samples, config['model_config']['max_seq_len'])\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': jnp.array(input_ids),\n",
    "        'labels': jnp.array(labels)\n",
    "    }\n",
    "\n",
    "# Create train and eval datasets\n",
    "train_data = create_dummy_data(1000)\n",
    "eval_data = create_dummy_data(200)\n",
    "\n",
    "print(\"Training data shape:\", train_data['input_ids'].shape)\n",
    "print(\"Evaluation data shape:\", eval_data['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rng = jax.random.PRNGKey(42)\n",
    "model = create_vishwamai_transformer(config['model_config'])\n",
    "\n",
    "# Create learning rate schedule\n",
    "def create_learning_rate_schedule():\n",
    "    base_lr = config['training']['learning_rate']\n",
    "    warmup_steps = config['training']['warmup_steps']\n",
    "    \n",
    "    def schedule(step):\n",
    "        warmup_factor = jnp.minimum(step / warmup_steps, 1.0)\n",
    "        return base_lr * warmup_factor\n",
    "    \n",
    "    return schedule\n",
    "\n",
    "# Create trainer\n",
    "trainer = VishwamAITrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    learning_rate_schedule=create_learning_rate_schedule(),\n",
    "    logger=DuckDBLogger('experiment2.db', 'experiment2')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DuckDB Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional logging tables for transformer behavior\n",
    "logger = trainer.logger\n",
    "logger.conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS attention_stats (\n",
    "        experiment_id VARCHAR,\n",
    "        step INTEGER,\n",
    "        layer INTEGER,\n",
    "        head INTEGER,\n",
    "        attention_entropy DOUBLE,\n",
    "        attention_sparsity DOUBLE,\n",
    "        FOREIGN KEY (experiment_id) REFERENCES experiments(experiment_id)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "logger.conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS activation_stats (\n",
    "        experiment_id VARCHAR,\n",
    "        step INTEGER,\n",
    "        layer INTEGER,\n",
    "        activation_mean DOUBLE,\n",
    "        activation_std DOUBLE,\n",
    "        zero_rate DOUBLE,\n",
    "        FOREIGN KEY (experiment_id) REFERENCES experiments(experiment_id)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created additional logging tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_patterns(model_output, layer_idx):\n",
    "    \"\"\"Analyze attention patterns for a given layer\"\"\"\n",
    "    attention_weights = model_output['attention_weights'][layer_idx]\n",
    "    \n",
    "    # Compute attention entropy\n",
    "    entropy = -jnp.sum(attention_weights * jnp.log(attention_weights + 1e-10), axis=-1)\n",
    "    \n",
    "    # Compute sparsity (% of attention weights below threshold)\n",
    "    sparsity = jnp.mean(attention_weights < 0.01)\n",
    "    \n",
    "    return {\n",
    "        'entropy': entropy,\n",
    "        'sparsity': sparsity\n",
    "    }\n",
    "\n",
    "def log_attention_metrics(logger, step, model_outputs):\n",
    "    \"\"\"Log attention metrics to DuckDB\"\"\"\n",
    "    for layer_idx in range(config['model_config']['num_layers']):\n",
    "        metrics = analyze_attention_patterns(model_outputs, layer_idx)\n",
    "        \n",
    "        for head_idx in range(config['model_config']['num_heads']):\n",
    "            logger.conn.execute(\"\"\"\n",
    "                INSERT INTO attention_stats\n",
    "                (experiment_id, step, layer, head, attention_entropy, attention_sparsity)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", [\n",
    "                trainer.experiment_name,\n",
    "                step,\n",
    "                layer_idx,\n",
    "                head_idx,\n",
    "                float(metrics['entropy'][head_idx].mean()),\n",
    "                float(metrics['sparsity'])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_activations(model_output, layer_idx):\n",
    "    \"\"\"Analyze activation statistics for a given layer\"\"\"\n",
    "    activations = model_output['hidden_states'][layer_idx]\n",
    "    \n",
    "    # Compute basic statistics\n",
    "    mean = jnp.mean(activations)\n",
    "    std = jnp.std(activations)\n",
    "    zero_rate = jnp.mean(jnp.abs(activations) < 1e-6)\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'zero_rate': zero_rate\n",
    "    }\n",
    "\n",
    "def log_activation_metrics(logger, step, model_outputs):\n",
    "    \"\"\"Log activation metrics to DuckDB\"\"\"\n",
    "    for layer_idx in range(config['model_config']['num_layers']):\n",
    "        metrics = analyze_activations(model_outputs, layer_idx)\n",
    "        \n",
    "        logger.conn.execute(\"\"\"\n",
    "            INSERT INTO activation_stats\n",
    "            (experiment_id, step, layer, activation_mean, activation_std, zero_rate)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", [\n",
    "            trainer.experiment_name,\n",
    "            step,\n",
    "            layer_idx,\n",
    "            float(metrics['mean']),\n",
    "            float(metrics['std']),\n",
    "            float(metrics['zero_rate'])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Validation and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_inputs(batch):\n",
    "    \"\"\"Validate model inputs before training\"\"\"\n",
    "    required_keys = ['input_ids', 'labels']\n",
    "    for key in required_keys:\n",
    "        if key not in batch:\n",
    "            raise ValueError(f\"Missing required key '{key}' in batch\")\n",
    "            \n",
    "        if not isinstance(batch[key], jnp.ndarray):\n",
    "            raise TypeError(f\"Batch['{key}'] must be a JAX array\")\n",
    "            \n",
    "        if batch[key].shape[0] != config['training']['batch_size']:\n",
    "            raise ValueError(f\"Batch size mismatch in '{key}': \"\n",
    "                           f\"expected {config['training']['batch_size']}, \"\n",
    "                           f\"got {batch[key].shape[0]}\")\n",
    "            \n",
    "        if batch[key].shape[1] != config['model_config']['max_seq_len']:\n",
    "            raise ValueError(f\"Sequence length mismatch in '{key}': \"\n",
    "                           f\"expected {config['model_config']['max_seq_len']}, \"\n",
    "                           f\"got {batch[key].shape[1]}\")\n",
    "\n",
    "def log_training_error(logger, step, error):\n",
    "    \"\"\"Log training errors to DuckDB\"\"\"\n",
    "    logger.conn.execute(\"\"\"\n",
    "        INSERT INTO training_errors\n",
    "        (experiment_id, step, error_type, error_message)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    \"\"\", [\n",
    "        trainer.experiment_name,\n",
    "        step,\n",
    "        type(error).__name__,\n",
    "        str(error)\n",
    "    ])\n",
    "\n",
    "class ExperimentMonitor:\n",
    "    \"\"\"Monitor training progress and detect issues\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.loss_history = []\n",
    "        self.nan_count = 0\n",
    "        self.inf_count = 0\n",
    "        \n",
    "    def check_metrics(self, metrics):\n",
    "        \"\"\"Check training metrics for issues\"\"\"\n",
    "        loss = metrics.get('loss')\n",
    "        if loss is not None:\n",
    "            self.loss_history.append(float(loss))\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            if jnp.isnan(loss):\n",
    "                self.nan_count += 1\n",
    "            if jnp.isinf(loss):\n",
    "                self.inf_count += 1\n",
    "                \n",
    "            # Check for loss explosion\n",
    "            if len(self.loss_history) > 10:\n",
    "                recent_mean = np.mean(self.loss_history[-10:])\n",
    "                if recent_mean > 1000 * np.mean(self.loss_history[:10]):\n",
    "                    raise ValueError(\"Loss explosion detected\")\n",
    "                    \n",
    "            # Check for loss stagnation\n",
    "            if len(self.loss_history) > 100:\n",
    "                recent_std = np.std(self.loss_history[-100:])\n",
    "                if recent_std < 1e-6:\n",
    "                    print(\"Warning: Loss may be stagnating\")\n",
    "                    \n",
    "        # Check NaN/Inf counts\n",
    "        if self.nan_count > 5:\n",
    "            raise ValueError(\"Too many NaN values in loss\")\n",
    "        if self.inf_count > 5:\n",
    "            raise ValueError(\"Too many Inf values in loss\")\n",
    "\n",
    "# Create monitor\n",
    "monitor = ExperimentMonitor(config)\n",
    "\n",
    "# Create error logging table\n",
    "logger.conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS training_errors (\n",
    "        experiment_id VARCHAR,\n",
    "        step INTEGER,\n",
    "        error_type VARCHAR,\n",
    "        error_message TEXT,\n",
    "        FOREIGN KEY (experiment_id) REFERENCES experiments(experiment_id)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Training Loop with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update training loop with validation\n",
    "for step in tqdm(range(config['training']['max_steps'])):\n",
    "    try:\n",
    "        # Get and validate batch\n",
    "        batch_idx = step % (len(train_data['input_ids']) // config['training']['batch_size'])\n",
    "        start_idx = batch_idx * config['training']['batch_size']\n",
    "        end_idx = start_idx + config['training']['batch_size']\n",
    "        \n",
    "        batch = {\n",
    "            'input_ids': train_data['input_ids'][start_idx:end_idx],\n",
    "            'labels': train_data['labels'][start_idx:end_idx]\n",
    "        }\n",
    "        \n",
    "        validate_model_inputs(batch)\n",
    "        \n",
    "        # Training step\n",
    "        metrics = trainer.train_step(batch)\n",
    "        \n",
    "        # Monitor training progress\n",
    "        monitor.check_metrics(metrics)\n",
    "        \n",
    "        # Log metrics\n",
    "        log_attention_metrics(trainer.logger, step, metrics['model_outputs'])\n",
    "        log_activation_metrics(trainer.logger, step, metrics['model_outputs'])\n",
    "        \n",
    "        # Evaluation\n",
    "        if step > 0 and step % config['training']['eval_frequency'] == 0:\n",
    "            eval_metrics = trainer.evaluate(eval_data)\n",
    "            print(f\"\\nStep {step} evaluation:\")\n",
    "            print(f\"Loss: {eval_metrics['loss']:.4f}\")\n",
    "            print(f\"Accuracy: {eval_metrics.get('accuracy', 0):.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_training_error(trainer.logger, step, e)\n",
    "        print(f\"Error at step {step}: {str(e)}\")\n",
    "        if isinstance(e, (ValueError, RuntimeError)):\n",
    "            # Critical error - stop training\n",
    "            break\n",
    "        # For other errors, continue training\n",
    "        continue\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for step in tqdm(range(config['training']['max_steps'])):\n",
    "    # Get batch\n",
    "    batch_idx = step % (len(train_data['input_ids']) // config['training']['batch_size'])\n",
    "    start_idx = batch_idx * config['training']['batch_size']\n",
    "    end_idx = start_idx + config['training']['batch_size']\n",
    "    \n",
    "    batch = {\n",
    "        'input_ids': train_data['input_ids'][start_idx:end_idx],\n",
    "        'labels': train_data['labels'][start_idx:end_idx]\n",
    "    }\n",
    "    \n",
    "    # Training step\n",
    "    metrics = trainer.train_step(batch)\n",
    "    \n",
    "    # Log attention and activation metrics\n",
    "    log_attention_metrics(trainer.logger, step, metrics['model_outputs'])\n",
    "    log_activation_metrics(trainer.logger, step, metrics['model_outputs'])\n",
    "    \n",
    "    # Evaluation\n",
    "    if step > 0 and step % config['training']['eval_frequency'] == 0:\n",
    "        eval_metrics = trainer.evaluate(eval_data)\n",
    "        print(f\"\\nStep {step} evaluation:\")\n",
    "        print(f\"Loss: {eval_metrics['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {eval_metrics.get('accuracy', 0):.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training history from logger\n",
    "training_history = trainer.logger.get_experiment_summary()\n",
    "\n",
    "# Plot training metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history['metrics_summary']['loss']['history'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if 'accuracy' in training_history['metrics_summary']:\n",
    "    plt.plot(training_history['metrics_summary']['accuracy']['history'])\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query attention behavior across layers\n",
    "attention_df = logger.conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        layer,\n",
    "        AVG(attention_entropy) as avg_entropy,\n",
    "        AVG(attention_sparsity) as avg_sparsity\n",
    "    FROM attention_stats\n",
    "    WHERE experiment_id = ?\n",
    "    GROUP BY layer\n",
    "    ORDER BY layer\n",
    "\"\"\", [trainer.experiment_name]).fetchdf()\n",
    "\n",
    "# Plot attention statistics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(attention_df['layer'], attention_df['avg_entropy'])\n",
    "plt.title('Average Attention Entropy by Layer')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Entropy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(attention_df['layer'], attention_df['avg_sparsity'])\n",
    "plt.title('Average Attention Sparsity by Layer')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Sparsity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_activations():\n",
    "    \"\"\"Plot activation statistics across layers\"\"\"\n",
    "    activation_df = logger.conn.execute(\"\"\"\n",
    "        SELECT \n",
    "            layer,\n",
    "            AVG(activation_mean) as avg_mean,\n",
    "            AVG(activation_std) as avg_std,\n",
    "            AVG(zero_rate) as avg_zero_rate\n",
    "        FROM activation_stats\n",
    "        WHERE experiment_id = ?\n",
    "        GROUP BY layer\n",
    "        ORDER BY layer\n",
    "    \"\"\", [trainer.experiment_name]).fetchdf()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(activation_df['layer'], activation_df['avg_mean'])\n",
    "    plt.title('Average Activation Mean')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Mean')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(activation_df['layer'], activation_df['avg_std'])\n",
    "    plt.title('Average Activation Std')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Standard Deviation')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(activation_df['layer'], activation_df['avg_zero_rate'])\n",
    "    plt.title('Average Zero Activation Rate')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Zero Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call plotting function after training\n",
    "plot_layer_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "save_path = 'experiment2_model'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "trainer.save_checkpoint(f\"{save_path}/checkpoint\")\n",
    "\n",
    "# Save config\n",
    "with open(f\"{save_path}/config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Model and config saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
