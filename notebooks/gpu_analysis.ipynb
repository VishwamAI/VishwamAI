{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI GPU Performance Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of VishwamAI model performance on GPU, including:\n",
    "- Memory usage and efficiency\n",
    "- Inference latency\n",
    "- Throughput analysis\n",
    "- Mixed precision benefits\n",
    "- Layer-wise profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VishwamAI/VishwamAI\n",
    "%cd VishwamAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r jax flax optax dm-haiku torch sentencepiece smallpond  seaborn numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vishwamai.inference.optimized_inference import OptimizedInference\n",
    "from vishwamai.models.transformer import VishwamAITransformer\n",
    "from vishwamai.optimisation.profiling_tools import VishwamAIProfiler\n",
    "from vishwamai.optimisation.performance_tuning import PerformanceTuner\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the GPU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = VishwamAITransformer(\n",
    "    vocab_size=50000,\n",
    "    embed_dim=768,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    ff_dim=3072,\n",
    "    max_seq_len=512\n",
    ")\n",
    "\n",
    "# Initialize optimization tools\n",
    "optimizer = OptimizedInference()\n",
    "optimizer.set_device('gpu')\n",
    "optimizer.set_precision('fp16')\n",
    "model = optimizer.optimize_model(model)\n",
    "\n",
    "profiler = VishwamAIProfiler(model)\n",
    "tuner = PerformanceTuner(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "def generate_test_batch(batch_size, seq_length):\n",
    "    return torch.randint(0, 50000, (batch_size, seq_length), device='cuda')\n",
    "\n",
    "# Test different batch sizes\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "seq_length = 512\n",
    "latencies = []\n",
    "memory_usage = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    input_data = generate_test_batch(batch_size, seq_length)\n",
    "    \n",
    "    # Measure latency\n",
    "    stats = profiler.profile_model(input_data)\n",
    "    latencies.append(stats['avg_step_time_ms'])\n",
    "    memory_usage.append(stats['peak_memory_usage_mb'])\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(x=batch_sizes, y=latencies, ax=ax1)\n",
    "ax1.set_title('Inference Latency vs Batch Size')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "\n",
    "sns.lineplot(x=batch_sizes, y=memory_usage, ax=ax2)\n",
    "ax2.set_title('Memory Usage vs Batch Size')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Memory Usage (MB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer-wise profiling\n",
    "input_data = generate_test_batch(16, 512)\n",
    "layer_stats = profiler.layer_wise_profiling(input_data)\n",
    "\n",
    "# Extract data for visualization\n",
    "layer_names = [stat[0] for stat in layer_stats]\n",
    "layer_latencies = [stat[1] for stat in layer_stats]\n",
    "layer_memory = [stat[2] for stat in layer_stats]\n",
    "\n",
    "# Plot layer-wise metrics\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "sns.barplot(x=layer_names[:10], y=layer_latencies[:10], ax=ax1)\n",
    "ax1.set_title('Layer-wise Latency Analysis (Top 10 Layers)')\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "\n",
    "sns.barplot(x=layer_names[:10], y=layer_memory[:10], ax=ax2)\n",
    "ax2.set_title('Layer-wise Memory Usage (Top 10 Layers)')\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
    "ax2.set_ylabel('Memory Usage (MB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision analysis\n",
    "precisions = ['fp32', 'fp16', 'bf16']\n",
    "precision_latencies = []\n",
    "precision_memory = []\n",
    "\n",
    "input_data = generate_test_batch(16, 512)\n",
    "\n",
    "for precision in precisions:\n",
    "    try:\n",
    "        optimizer.set_precision(precision)\n",
    "        model = optimizer.optimize_model(model)\n",
    "        \n",
    "        stats = profiler.profile_model(input_data)\n",
    "        precision_latencies.append(stats['avg_step_time_ms'])\n",
    "        precision_memory.append(stats['peak_memory_usage_mb'])\n",
    "    except Exception as e:\n",
    "        print(f\"Precision {precision} not supported: {str(e)}\")\n",
    "\n",
    "# Plot precision comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.barplot(x=precisions, y=precision_latencies, ax=ax1)\n",
    "ax1.set_title('Inference Latency vs Precision')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "\n",
    "sns.barplot(x=precisions, y=precision_memory, ax=ax2)\n",
    "ax2.set_title('Memory Usage vs Precision')\n",
    "ax2.set_ylabel('Memory Usage (MB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput optimization\n",
    "optimal_batch_size = tuner.tune_batch_size((seq_length, model.embed_dim))\n",
    "print(f\"Optimal batch size for maximum throughput: {optimal_batch_size}\")\n",
    "\n",
    "# Test throughput with optimal batch size\n",
    "input_data = generate_test_batch(optimal_batch_size, seq_length)\n",
    "stats = profiler.profile_model(input_data)\n",
    "\n",
    "print(f\"\\nPerformance with optimal batch size:\")\n",
    "print(f\"Average latency: {stats['avg_step_time_ms']:.2f} ms\")\n",
    "print(f\"Peak memory usage: {stats['peak_memory_usage_mb']:.2f} MB\")\n",
    "print(f\"Throughput: {(optimal_batch_size * 1000 / stats['avg_step_time_ms']):.2f} samples/second\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
