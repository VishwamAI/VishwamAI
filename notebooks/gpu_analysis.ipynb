{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI GPU Analysis\n",
    "\n",
    "This notebook performs comprehensive GPU analysis for VishwamAI models including:\n",
    "- Memory utilization\n",
    "- Processing speed\n",
    "- Attention mechanism comparison\n",
    "- Model scaling analysis\n",
    "- Expert routing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from vishwamai.models.gpu.transformer import TransformerComputeLayer, TransformerMemoryLayer, HybridThoughtAwareAttention\n",
    "from vishwamai.models.tot_model import ToTModel\n",
    "from vishwamai.models.cot_model import CoTModel\n",
    "from vishwamai.models.kernel_layers import OptimizedLinear, FusedLayerNorm\n",
    "\n",
    "import torch.cuda.amp as amp\n",
    "import torch.cuda.profiler as profiler\n",
    "import torch.cuda as cuda\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"All imports successful (models and GPU-specific corrected)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Setup and Verification\n",
    "def setup_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError('GPU is required for this analysis')\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    print(f'GPU Device: {torch.cuda.get_device_name()}')\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "    print(f'GPU Count: {torch.cuda.device_count()}')\n",
    "    print(f'\\nCUDA Architecture:')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f'  Device {i}: {props.name}')\n",
    "        print(f'    Compute Capability: {props.major}.{props.minor}')\n",
    "        print(f'    SM Count: {props.multi_processor_count}')\n",
    "        print(f'    Max Threads per SM: {props.max_threads_per_multi_processor}')\n",
    "    return device\n",
    "\n",
    "device = setup_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced GPU Monitoring\n",
    "class GPUMonitor:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestamps = []\n",
    "        self.memory_allocated = []\n",
    "        self.memory_cached = []\n",
    "        self.utilization = []\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "    def sample(self):\n",
    "        self.timestamps.append(time.time())\n",
    "        self.memory_allocated.append(torch.cuda.memory_allocated() / 1e9)\n",
    "        self.memory_cached.append(torch.cuda.memory_reserved() / 1e9)\n",
    "        # Get GPU utilization using nvidia-smi if available\n",
    "        try:\n",
    "            import pynvml\n",
    "            pynvml.nvmlInit()\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            self.utilization.append(util.gpu)\n",
    "        except:\n",
    "            self.utilization.append(0)\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'timestamps': np.array(self.timestamps),\n",
    "            'memory_allocated': np.array(self.memory_allocated),\n",
    "            'memory_cached': np.array(self.memory_cached),\n",
    "            'utilization': np.array(self.utilization),\n",
    "            'peak_memory': torch.cuda.max_memory_allocated() / 1e9\n",
    "        }\n",
    "\n",
    "gpu_monitor = GPUMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization with Error Handling\n",
    "def create_model(model_type='transformer', attention_type='flash_mla', **kwargs):\n",
    "    try:\n",
    "        base_config = {\n",
    "            'vocab_size': kwargs.get('vocab_size', 50000),\n",
    "            'embed_dim': kwargs.get('embed_dim', 512),\n",
    "            'num_layers': kwargs.get('num_layers', 12),\n",
    "            'num_heads': kwargs.get('num_heads', 8),\n",
    "            'ff_dim': kwargs.get('ff_dim', 2048),\n",
    "            'max_seq_len': kwargs.get('max_seq_len', 512)\n",
    "        }\n",
    "        \n",
    "        if attention_type == 'flash_mla':\n",
    "            attention_class = FlashMLAAttention\n",
    "            attention_kwargs = {'use_amp': True}\n",
    "        else:\n",
    "            attention_class = OptimizedMoEAttention\n",
    "            attention_kwargs = {\n",
    "                'num_experts': kwargs.get('num_experts', 4),\n",
    "                'use_amp': True\n",
    "            }\n",
    "            \n",
    "        if model_type == 'transformer':\n",
    "            model = VishwamAITransformer(\n",
    "                **base_config,\n",
    "                attention_class=attention_class,\n",
    "                attention_kwargs=attention_kwargs\n",
    "            )\n",
    "        elif model_type == 'tot':\n",
    "            model = ToTModel(\n",
    "                **base_config,\n",
    "                num_experts=kwargs.get('num_experts', 4)\n",
    "            )\n",
    "        elif model_type == 'cot':\n",
    "            model = CoTModel(\n",
    "                **base_config,\n",
    "                num_experts=kwargs.get('num_experts', 4)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f'Unknown model type: {model_type}')\n",
    "            \n",
    "        return model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f'Error creating model: {str(e)}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets\n",
    "def load_datasets():\n",
    "    dataset_loader = DatasetLoader()\n",
    "    train_dataset = dataset_loader.load_train_dataset()\n",
    "    test_dataset = dataset_loader.load_test_dataset()\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Performance Analysis\n",
    "def analyze_model_performance(model, dataset, batch_size=32, num_batches=None):\n",
    "    model.eval()\n",
    "    gpu_monitor.reset()\n",
    "    \n",
    "    metrics = {\n",
    "        'batch_times': [],\n",
    "        'throughput': [],\n",
    "        'memory_peaks': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "        \n",
    "        if num_batches:\n",
    "            dataloader = list(dataloader)[:num_batches]\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc='Analyzing'):\n",
    "            # Reset CUDA cache\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Prepare batch\n",
    "            if isinstance(batch, tuple):\n",
    "                input_ids = batch[0].to(device)\n",
    "            else:\n",
    "                input_ids = batch.to(device)\n",
    "                \n",
    "            # Warmup\n",
    "            _ = model(input_ids)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Measure performance\n",
    "            start_time = time.perf_counter()\n",
    "            gpu_monitor.sample()\n",
    "            \n",
    "            _ = model(input_ids)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            gpu_monitor.sample()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_time = end_time - start_time\n",
    "            tokens_per_sec = (batch_size * input_ids.size(1)) / batch_time\n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "            \n",
    "            metrics['batch_times'].append(batch_time)\n",
    "            metrics['throughput'].append(tokens_per_sec)\n",
    "            metrics['memory_peaks'].append(peak_memory)\n",
    "    \n",
    "    return {\n",
    "        'avg_time_per_batch': np.mean(metrics['batch_times']),\n",
    "        'std_time_per_batch': np.std(metrics['batch_times']),\n",
    "        'avg_throughput': np.mean(metrics['throughput']),\n",
    "        'peak_memory': max(metrics['memory_peaks']),\n",
    "        'gpu_stats': gpu_monitor.get_stats(),\n",
    "        'raw_metrics': metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Analysis Pipeline\n",
    "def run_analysis(model_configs, dataset_sizes=[1000, 5000, 10000]):\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Load datasets\n",
    "        print(\"Loading datasets...\")\n",
    "        dataset_loader = DatasetLoader()\n",
    "        \n",
    "        for size in dataset_sizes:\n",
    "            print(f\"\\nAnalyzing with dataset size: {size}\")\n",
    "            train_data = dataset_loader.load_train_dataset(max_size=size)\n",
    "            test_data = dataset_loader.load_test_dataset(max_size=size)\n",
    "            \n",
    "            for config in model_configs:\n",
    "                model_name = f\"{config['model_type']}_{config['attention_type']}\"\n",
    "                print(f\"\\nTesting {model_name}...\")\n",
    "                \n",
    "                # Create and analyze model\n",
    "                model = create_model(**config)\n",
    "                \n",
    "                results[f\"{model_name}_{size}\"] = {\n",
    "                    'train': analyze_model_performance(model, train_data),\n",
    "                    'test': analyze_model_performance(model, test_data)\n",
    "                }\n",
    "                \n",
    "                # Clean up\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation - TinyShakespeare\n",
    "\n",
    "First, we'll download and prepare the TinyShakespeare dataset for training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TinyShakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "# Read the dataset\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Length of dataset in characters: {len(text)}')\n",
    "print('\\nFirst 1000 characters:\\n')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-level tokenizer\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self, text):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        print(f'Vocabulary size: {self.vocab_size}')\n",
    "    \n",
    "    def encode(self, text, return_tensors='pt'):\n",
    "        indices = [self.char_to_idx[c] for c in text]\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices, skip_special_tokens=False):\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.cpu().numpy()\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = CharacterTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "def create_training_data(text, seq_length=256, batch_size=32):\n",
    "    # Encode the full text\n",
    "    data = torch.tensor(tokenizer.encode(text, return_tensors=None), dtype=torch.long)\n",
    "    \n",
    "    # Create training examples\n",
    "    n = len(data) - seq_length\n",
    "    x = torch.stack([data[i:i+seq_length] for i in range(0, n-1, seq_length)])\n",
    "    y = torch.stack([data[i+1:i+seq_length+1] for i in range(0, n-1, seq_length)])\n",
    "    \n",
    "    # Create train/test split (90/10)\n",
    "    n_train = int(0.9 * len(x))\n",
    "    train_data = torch.utils.data.TensorDataset(x[:n_train], y[:n_train])\n",
    "    test_data = torch.utils.data.TensorDataset(x[n_train:], y[n_train:])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Create training and test dataloaders\n",
    "train_loader, test_loader = create_training_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model configuration for Shakespeare dataset\n",
    "shakespeare_model_configs = [\n",
    "    {\n",
    "        'model_type': 'transformer',\n",
    "        'attention_type': 'flash_mla',\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'max_seq_len': 256\n",
    "    },\n",
    "    {\n",
    "        'model_type': 'transformer',\n",
    "        'attention_type': 'moe',\n",
    "        'num_experts': 4,\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'max_seq_len': 256\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with GPU monitoring\n",
    "def train_epoch(model, train_loader, optimizer, gpu_monitor):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Monitor GPU before forward pass\n",
    "        gpu_monitor.sample()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output.view(-1, tokenizer.vocab_size), target.view(-1))\n",
    "        \n",
    "        # Monitor GPU after forward pass\n",
    "        gpu_monitor.sample()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Monitor GPU after backward pass\n",
    "        gpu_monitor.sample()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += F.cross_entropy(output.view(-1, tokenizer.vocab_size), target.view(-1)).item()\n",
    "    \n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and analyze models on Shakespeare dataset\n",
    "def train_and_analyze_models(model_configs, num_epochs=3):\n",
    "    results = {}\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = f\"{config['model_type']}_{config['attention_type']}\"\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_model(**config)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop with GPU monitoring\n",
    "        gpu_monitor.reset()\n",
    "        training_stats = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, gpu_monitor)\n",
    "            \n",
    "            # Evaluate\n",
    "            test_loss = evaluate(model, test_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "            \n",
    "            # Save stats\n",
    "            training_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'gpu_stats': gpu_monitor.get_stats()\n",
    "            })\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'training_stats': training_stats,\n",
    "            'final_train_loss': train_loss,\n",
    "            'final_test_loss': test_loss\n",
    "        }\n",
    "        \n",
    "        # Generate sample text\n",
    "        print(\"\\nGenerating sample text...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            context = \"ROMEO: \"\n",
    "            input_ids = tokenizer.encode(context, return_tensors='pt').to(device)\n",
    "            generated = model.generate(\n",
    "                input_ids,\n",
    "                max_length=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "            generated_text = tokenizer.decode(generated[0])\n",
    "            print(f\"Generated text:\\n{generated_text}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run training and analysis\n",
    "shakespeare_results = train_and_analyze_models(shakespeare_model_configs)\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for model_name, stats in shakespeare_results.items():\n",
    "    train_losses = [stat['train_loss'] for stat in stats['training_stats']]\n",
    "    test_losses = [stat['test_loss'] for stat in stats['training_stats']]\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, '-o', label=f'{model_name}_train')\n",
    "    plt.plot(epochs, test_losses, '--o', label=f'{model_name}_test')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for model_name, stats in shakespeare_results.items():\n",
    "    gpu_stats = stats['training_stats'][0]['gpu_stats']\n",
    "    plt.plot(\n",
    "        gpu_stats['timestamps'] - gpu_stats['timestamps'][0],\n",
    "        gpu_stats['memory_allocated'],\n",
    "        label=model_name\n",
    "    )\n",
    "plt.title('GPU Memory Usage During Training')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Memory (GB)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def plot_performance_comparison(results):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Throughput comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plot_data = []\n",
    "    labels = []\n",
    "    for model_name in results:\n",
    "        plot_data.extend([\n",
    "            results[model_name]['train']['avg_throughput'],\n",
    "            results[model_name]['test']['avg_throughput']\n",
    "        ])\n",
    "        labels.extend([f\"{model_name}_train\", f\"{model_name}_test\"])\n",
    "    sns.barplot(x=labels, y=plot_data)\n",
    "    plt.title('Model Throughput (tokens/sec)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Memory usage\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plot_data = []\n",
    "    for model_name in results:\n",
    "        plot_data.extend([\n",
    "            results[model_name]['train']['peak_memory'],\n",
    "            results[model_name]['test']['peak_memory']\n",
    "        ])\n",
    "    sns.barplot(x=labels, y=plot_data)\n",
    "    plt.title('Peak GPU Memory Usage (GB)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # GPU utilization over time\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for model_name in results:\n",
    "        stats = results[model_name]['train']['gpu_stats']\n",
    "        plt.plot(\n",
    "            stats['timestamps'] - stats['timestamps'][0],\n",
    "            stats['utilization'],\n",
    "            label=model_name\n",
    "        )\n",
    "    plt.title('GPU Utilization Over Time')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('GPU Utilization %')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Memory allocation over time\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for model_name in results:\n",
    "        stats = results[model_name]['train']['gpu_stats']\n",
    "        plt.plot(\n",
    "            stats['timestamps'] - stats['timestamps'][0],\n",
    "            stats['memory_allocated'],\n",
    "            label=model_name\n",
    "        )\n",
    "    plt.title('GPU Memory Allocation Over Time')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Memory (GB)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Analysis\n",
    "model_configs = [\n",
    "    {\n",
    "        'model_type': 'transformer',\n",
    "        'attention_type': 'flash_mla'\n",
    "    },\n",
    "    {\n",
    "        'model_type': 'transformer',\n",
    "        'attention_type': 'moe',\n",
    "        'num_experts': 4\n",
    "    },\n",
    "    {\n",
    "        'model_type': 'tot',\n",
    "        'attention_type': 'flash_mla'\n",
    "    },\n",
    "    {\n",
    "        'model_type': 'cot',\n",
    "        'attention_type': 'moe',\n",
    "        'num_experts': 4\n",
    "    }\n",
    "]\n",
    "\n",
    "results = run_analysis(model_configs)\n",
    "plot_performance_comparison(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
