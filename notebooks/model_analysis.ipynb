{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa3fa01",
   "metadata": {},
   "source": [
    "# VishwamAI Model Architecture Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the VishwamAI model architecture, with focus on:\n",
    "1. Architecture Components\n",
    "2. TPU Optimizations\n",
    "3. Attention Mechanisms\n",
    "4. Memory Efficiency\n",
    "5. Performance Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2030b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:26:02.189408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742468162.208229   33269 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742468162.213656   33269 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "from vishwamai.model import VishwamAI\n",
    "from vishwamai.kernels.kernel import fp8_gemm_optimized\n",
    "from vishwamai.layers.attention import FlashAttention\n",
    "from vishwamai.transformer import (\n",
    "    TransformerModel,\n",
    "    EnhancedTransformerModel,\n",
    "    create_vishwamai_transformer\n",
    ")\n",
    "from vishwamai.profiler import TPUProfiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94467347",
   "metadata": {},
   "source": [
    "## 1. Model Configurations\n",
    "\n",
    "VishwamAI supports multiple model sizes optimized for different use cases:\n",
    "\n",
    "1. Base Model (768M parameters)\n",
    "2. Large Model (1.5B parameters)\n",
    "3. XL Model (3B parameters)\n",
    "4. XXL Model (7B parameters)\n",
    "\n",
    "Let's analyze different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5a0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_configs():\n",
    "    \"\"\"Create different model size configurations\"\"\"\n",
    "    configs = {\n",
    "        'base': {\n",
    "            'vocab_size': 32000,\n",
    "            'num_layers': 12,\n",
    "            'num_heads': 12,\n",
    "            'head_dim': 64,\n",
    "            'hidden_dim': 768,\n",
    "            'mlp_dim': 3072,\n",
    "            'max_seq_len': 2048\n",
    "        },\n",
    "        'large': {\n",
    "            'vocab_size': 32000,\n",
    "            'num_layers': 24,\n",
    "            'num_heads': 16,\n",
    "            'head_dim': 96,\n",
    "            'hidden_dim': 1536,\n",
    "            'mlp_dim': 6144,\n",
    "            'max_seq_len': 2048\n",
    "        },\n",
    "        'xl': {\n",
    "            'vocab_size': 32000,\n",
    "            'num_layers': 32,\n",
    "            'num_heads': 24,\n",
    "            'head_dim': 128,\n",
    "            'hidden_dim': 2048,\n",
    "            'mlp_dim': 8192,\n",
    "            'max_seq_len': 2048\n",
    "        },\n",
    "        'xxl': {\n",
    "            'vocab_size': 32000,\n",
    "            'num_layers': 40,\n",
    "            'num_heads': 32,\n",
    "            'head_dim': 128,\n",
    "            'hidden_dim': 4096,\n",
    "            'mlp_dim': 16384,\n",
    "            'max_seq_len': 2048\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add common configurations\n",
    "    for config in configs.values():\n",
    "        config.update({\n",
    "            'dropout_rate': 0.1,\n",
    "            'use_enhanced': True,\n",
    "            'use_rotary': True,\n",
    "            'use_flash_attn': True,\n",
    "            'use_rms_norm': True,\n",
    "            'dtype': 'bfloat16'\n",
    "        })\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca72e9",
   "metadata": {},
   "source": [
    "## 2. Architecture Analysis\n",
    "\n",
    "VishwamAI implements several key optimizations:\n",
    "\n",
    "1. **Flash Attention**: Memory-efficient attention implementation\n",
    "2. **RMSNorm**: Faster alternative to LayerNorm\n",
    "3. **Rotary Position Embeddings**: Better positional encoding\n",
    "4. **TPU-Optimized Linear Layers**: Using fp8_gemm_optimized\n",
    "5. **Mixed Precision Training**: Using bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae003c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_architecture(config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze model architecture components and memory usage\"\"\"\n",
    "    \n",
    "    def calculate_memory(params: int, dtype: str = 'bfloat16') -> float:\n",
    "        \"\"\"Calculate memory usage in GB\"\"\"\n",
    "        bytes_per_param = 2 if dtype == 'bfloat16' else 4\n",
    "        return (params * bytes_per_param) / (1024 ** 3)\n",
    "    \n",
    "    # Calculate parameters per component\n",
    "    h = config['hidden_dim']\n",
    "    v = config['vocab_size']\n",
    "    l = config['num_layers']\n",
    "    m = config['mlp_dim']\n",
    "    \n",
    "    embedding_params = v * h\n",
    "    attention_params = l * (4 * h * h)  # QKV + output\n",
    "    ffn_params = l * (2 * h * m)  # Two linear layers\n",
    "    norm_params = l * 2 * h  # Two norms per layer\n",
    "    \n",
    "    total_params = embedding_params + attention_params + ffn_params + norm_params\n",
    "    \n",
    "    # Memory analysis\n",
    "    memory_analysis = {\n",
    "        'total_params': total_params,\n",
    "        'params_gb': calculate_memory(total_params),\n",
    "        'activation_gb': calculate_memory(config['max_seq_len'] * h * 4),  # Rough estimate\n",
    "        'attention_gb': calculate_memory(config['max_seq_len']**2 * config['num_heads']),\n",
    "        'components': {\n",
    "            'embedding': embedding_params,\n",
    "            'attention': attention_params,\n",
    "            'ffn': ffn_params,\n",
    "            'norm': norm_params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Theoretical throughput (tokens/second)\n",
    "    # Based on documented TPU v2/v3 performance characteristics\n",
    "    tflops = 420  # TPU v3-8 peak TFLOPS\n",
    "    flops_per_token = total_params * 2  # Forward + backward\n",
    "    memory_analysis['theoretical_throughput'] = (tflops * 1e12) / flops_per_token\n",
    "    \n",
    "    return memory_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e313a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASE Model Analysis ===\n",
      "Total Parameters: 109,529,088\n",
      "Model Size (GB): 0.20\n",
      "Peak Activation Memory (GB): 0.01\n",
      "Peak Attention Memory (GB): 0.09\n",
      "Theoretical Throughput: 1,917,299 tokens/sec\n",
      "\n",
      "Parameter Distribution:\n",
      " embedding:  22.44%\n",
      " attention:  25.85%\n",
      "       ffn:  51.70%\n",
      "      norm:   0.02%\n",
      "\n",
      "=== LARGE Model Analysis ===\n",
      "Total Parameters: 728,702,976\n",
      "Model Size (GB): 1.36\n",
      "Peak Activation Memory (GB): 0.02\n",
      "Peak Attention Memory (GB): 0.12\n",
      "Theoretical Throughput: 288,183 tokens/sec\n",
      "\n",
      "Parameter Distribution:\n",
      " embedding:   6.75%\n",
      " attention:  31.08%\n",
      "       ffn:  62.16%\n",
      "      norm:   0.01%\n",
      "\n",
      "=== XL Model Analysis ===\n",
      "Total Parameters: 1,676,279,808\n",
      "Model Size (GB): 3.12\n",
      "Peak Activation Memory (GB): 0.03\n",
      "Peak Attention Memory (GB): 0.19\n",
      "Theoretical Throughput: 125,277 tokens/sec\n",
      "\n",
      "Parameter Distribution:\n",
      " embedding:   3.91%\n",
      " attention:  32.03%\n",
      "       ffn:  64.06%\n",
      "      norm:   0.01%\n",
      "\n",
      "=== XXL Model Analysis ===\n",
      "Total Parameters: 8,184,463,360\n",
      "Model Size (GB): 15.24\n",
      "Peak Activation Memory (GB): 0.06\n",
      "Peak Attention Memory (GB): 0.25\n",
      "Theoretical Throughput: 25,658 tokens/sec\n",
      "\n",
      "Parameter Distribution:\n",
      " embedding:   1.60%\n",
      " attention:  32.80%\n",
      "       ffn:  65.60%\n",
      "      norm:   0.00%\n"
     ]
    }
   ],
   "source": [
    "# Analyze all configurations\n",
    "configs = create_model_configs()\n",
    "analyses = {name: analyze_architecture(config) \n",
    "           for name, config in configs.items()}\n",
    "\n",
    "# Print summary\n",
    "for name, analysis in analyses.items():\n",
    "    print(f\"\\n=== {name.upper()} Model Analysis ===\")\n",
    "    print(f\"Total Parameters: {analysis['total_params']:,}\")\n",
    "    print(f\"Model Size (GB): {analysis['params_gb']:.2f}\")\n",
    "    print(f\"Peak Activation Memory (GB): {analysis['activation_gb']:.2f}\")\n",
    "    print(f\"Peak Attention Memory (GB): {analysis['attention_gb']:.2f}\")\n",
    "    print(f\"Theoretical Throughput: {analysis['theoretical_throughput']:,.0f} tokens/sec\")\n",
    "    \n",
    "    # Component breakdown\n",
    "    print(\"\\nParameter Distribution:\")\n",
    "    for component, params in analysis['components'].items():\n",
    "        percentage = 100 * params / analysis['total_params']\n",
    "        print(f\"{component:>10}: {percentage:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955c181",
   "metadata": {},
   "source": [
    "## 3. TPU Optimizations\n",
    "\n",
    "Key TPU optimizations in VishwamAI:\n",
    "\n",
    "1. **Memory Layout**\n",
    "   - Aligned tensor dimensions for TPU cores\n",
    "   - Efficient sharding across TPU matrix units\n",
    "   - Optimized memory access patterns\n",
    "\n",
    "2. **Computation Optimizations**\n",
    "   - fp8_gemm_optimized for matrix operations\n",
    "   - Flash Attention for O(n) memory complexity\n",
    "   - Fused operations where possible\n",
    "\n",
    "3. **Data Pipeline**\n",
    "   - Efficient data loading and prefetching\n",
    "   - Optimized input processing\n",
    "   - Smart batching strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6c776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tpu_performance(config: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Benchmark TPU performance characteristics\"\"\"\n",
    "    # Initialize model\n",
    "    model = create_vishwamai_transformer(config)\n",
    "    \n",
    "    # Setup profiler\n",
    "    profiler = TPUProfiler({'log_dir': 'tpu_profiles'})\n",
    "    \n",
    "    # Generate sample batch\n",
    "    batch_size = 32\n",
    "    seq_len = 512\n",
    "    x = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n",
    "    \n",
    "    # Initialize model parameters\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    variables = model.init(rng, x)\n",
    "    \n",
    "    # Warmup\n",
    "    with profiler.profile_region('warmup'):\n",
    "        _ = model.apply(variables, x, train=False)\n",
    "    \n",
    "    # Benchmark forward pass\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        _ = model.apply(variables, x, train=False)\n",
    "        jax.tree_util.tree_map(lambda x: x.block_until_ready(), _)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    forward_latency = np.mean(times)\n",
    "    tokens_per_second = (batch_size * seq_len) / forward_latency\n",
    "    \n",
    "    # Get TPU utilization from profiler\n",
    "    metrics = profiler.get_metrics_summary()\n",
    "    tpu_utilization = metrics.get('tpu_utilization_mean', 0.0)\n",
    "    memory_efficiency = metrics.get('memory_efficiency_mean', 0.0)\n",
    "    \n",
    "    return {\n",
    "        'forward_latency': forward_latency,\n",
    "        'tokens_per_second': tokens_per_second,\n",
    "        'tpu_utilization': tpu_utilization,\n",
    "        'memory_efficiency': memory_efficiency\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386cfadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:29:37.586880: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 999.61MiB (1048170986 bytes) by rematerialization; only reduced to 1.95GiB (2097152000 bytes), down from 1.95GiB (2097152000 bytes) originally\n",
      "2025-03-20 16:29:47.587566: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.95GiB (rounded to 2097152000)requested by op \n",
      "2025-03-20 16:29:47.587833: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ******************************************__________________________________________________________\n",
      "2025-03-20 16:29:47.587566: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.95GiB (rounded to 2097152000)requested by op \n",
      "2025-03-20 16:29:47.587833: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ******************************************__________________________________________________________\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2097152000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Benchmark base model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m base_perf \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_tpu_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== TPU Performance Analysis (Base Model) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward Pass Latency: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_latency\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mbenchmark_tpu_performance\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize model parameters\u001b[39;00m\n\u001b[1;32m     15\u001b[0m rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Warmup\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profiler\u001b[38;5;241m.\u001b[39mprofile_region(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/VishwamAI/vishwamai/transformer.py:753\u001b[0m, in \u001b[0;36mEnhancedTransformerModel.__call__\u001b[0;34m(self, inputs, mask, deterministic)\u001b[0m\n\u001b[1;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m TPULayerNorm(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)(x)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Final projection to vocabulary\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mTPUGEMMLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/VishwamAI/vishwamai/transformer.py:71\u001b[0m, in \u001b[0;36mTPUGEMMLinear.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     chunk_scale \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mdynamic_slice(\n\u001b[1;32m     65\u001b[0m         input_scale,\n\u001b[1;32m     66\u001b[0m         (start_idx,) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0\u001b[39m,) \u001b[38;5;241m*\u001b[39m (input_scale\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     67\u001b[0m         (end_idx \u001b[38;5;241m-\u001b[39m start_idx,) \u001b[38;5;241m+\u001b[39m input_scale\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Perform FP8 matrix multiplication for chunk\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     chunk_output \u001b[38;5;241m=\u001b[39m \u001b[43mfp8_gemm_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(chunk_output)\n\u001b[1;32m     78\u001b[0m y \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate(outputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m num_chunks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/VishwamAI/vishwamai/kernels/kernel.py:38\u001b[0m, in \u001b[0;36mfp8_gemm_optimized\u001b[0;34m(a, b, transpose_a, transpose_b, dtype)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Optimized FP8 GEMM implementation.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Matrix multiplication result\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# For now, use standard matmul as placeholder\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# TODO: Implement actual FP8 optimization\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPrecision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHIGHEST\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/jax/_src/compiler.py:321\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    316\u001b[0m         built_c, compile_options\u001b[38;5;241m=\u001b[39moptions, host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    319\u001b[0m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    320\u001b[0m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    323\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2097152000 bytes."
     ]
    }
   ],
   "source": [
    "# Benchmark base model\n",
    "base_perf = benchmark_tpu_performance(configs['base'])\n",
    "\n",
    "print(\"\\n=== TPU Performance Analysis (Base Model) ===\")\n",
    "print(f\"Forward Pass Latency: {base_perf['forward_latency']*1000:.2f}ms\")\n",
    "print(f\"Throughput: {base_perf['tokens_per_second']:,.0f} tokens/sec\")\n",
    "print(f\"TPU Utilization: {base_perf['tpu_utilization']*100:.1f}%\")\n",
    "print(f\"Memory Efficiency: {base_perf['memory_efficiency']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccac4ee8",
   "metadata": {},
   "source": [
    "## 4. Memory Analysis and Optimization Strategy\n",
    "\n",
    "VishwamAI implements several memory optimization techniques:\n",
    "\n",
    "1. **Gradient Checkpointing**\n",
    "   - Trades computation for memory\n",
    "   - Configurable checkpointing frequency\n",
    "\n",
    "2. **Activation Recomputation**\n",
    "   - Selective recomputation of activations\n",
    "   - Smart caching strategies\n",
    "\n",
    "3. **Attention Optimization**\n",
    "   - Flash Attention for linear memory scaling\n",
    "   - Efficient KV cache management\n",
    "   - Smart attention patterns\n",
    "\n",
    "4. **Mixed Precision**\n",
    "   - bfloat16 for compute\n",
    "   - float32 for accumulation\n",
    "   - Selective precision control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficiency analysis function\n",
    "def analyze_memory_efficiency(config: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Analyze memory efficiency of different components\"\"\"\n",
    "    seq_len = config['max_seq_len']\n",
    "    hidden_dim = config['hidden_dim']\n",
    "    num_heads = config['num_heads']\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Calculate theoretical memory requirements\n",
    "    activation_memory = batch_size * seq_len * hidden_dim * 2  # bfloat16\n",
    "    attention_memory = batch_size * num_heads * seq_len * seq_len * 2\n",
    "    gradient_memory = activation_memory  # Roughly equal for simple backprop\n",
    "    \n",
    "    # Calculate optimized memory with techniques enabled\n",
    "    flash_attention_memory = batch_size * num_heads * seq_len * 2  # O(n) vs O(n^2)\n",
    "    gradient_checkpointing_memory = activation_memory / 4  # Rough estimate with optimal checkpointing\n",
    "    mixed_precision_memory = {\n",
    "        'compute': activation_memory,  # bfloat16\n",
    "        'accumulation': gradient_memory * 2  # float32 for stability\n",
    "    }\n",
    "    \n",
    "    efficiency_metrics = {\n",
    "        'baseline_memory': activation_memory + attention_memory + gradient_memory,\n",
    "        'optimized_memory': flash_attention_memory + gradient_checkpointing_memory + mixed_precision_memory['compute'],\n",
    "        'memory_savings': 1.0 - (flash_attention_memory + gradient_checkpointing_memory) / (activation_memory + attention_memory),\n",
    "        'techniques': {\n",
    "            'flash_attention_savings': 1.0 - (flash_attention_memory / attention_memory),\n",
    "            'gradient_checkpoint_savings': 1.0 - (gradient_checkpointing_memory / activation_memory),\n",
    "            'mixed_precision_ratio': mixed_precision_memory['compute'] / mixed_precision_memory['accumulation']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return efficiency_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d90f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add visualization of memory usage across different configurations\n",
    "def plot_memory_analysis(analyses):\n",
    "    \"\"\"Plot memory usage and efficiency metrics\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Memory breakdown\n",
    "    models = list(analyses.keys())\n",
    "    metrics = ['params_gb', 'activation_gb', 'attention_gb']\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [analyses[model][metric] for model in models]\n",
    "        ax1.bar(x + i*width, values, width, label=metric.replace('_gb', ''))\n",
    "    \n",
    "    ax1.set_ylabel('Memory (GB)')\n",
    "    ax1.set_title('Memory Usage Breakdown')\n",
    "    ax1.set_xticks(x + width)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Component distribution\n",
    "    for i, model in enumerate(models):\n",
    "        components = analyses[model]['components']\n",
    "        sizes = list(components.values())\n",
    "        labels = list(components.keys())\n",
    "        ax2.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "                startangle=90, radius=0.8 + i*0.2)\n",
    "    \n",
    "    ax2.set_title('Parameter Distribution by Component')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add performance analysis section\n",
    "def analyze_performance(config: Dict[str, Any], batch_sizes: List[int] = [1, 8, 32, 128]):\n",
    "    \"\"\"Analyze performance characteristics across different batch sizes\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Make a copy of config to avoid modifying the original\n",
    "        test_config = config.copy()\n",
    "        test_config['batch_size'] = batch_size  # Add batch_size to config\n",
    "        \n",
    "        # Run benchmark with the modified config\n",
    "        perf = benchmark_tpu_performance(test_config)\n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'throughput': perf['tokens_per_second'],\n",
    "            'latency': perf['forward_latency'],\n",
    "            'efficiency': perf['tpu_utilization']\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f174c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_analysis(perf_results: List[Dict[str, Any]]):\n",
    "    \"\"\"Plot performance analysis results\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    batch_sizes = [r['batch_size'] for r in perf_results]\n",
    "    throughputs = [r['throughput'] for r in perf_results]\n",
    "    latencies = [r['latency'] * 1000 for r in perf_results]  # Convert to ms\n",
    "    \n",
    "    ax1.plot(batch_sizes, throughputs, 'o-')\n",
    "    ax1.set_xlabel('Batch Size')\n",
    "    ax1.set_ylabel('Throughput (tokens/sec)')\n",
    "    ax1.set_title('Throughput vs Batch Size')\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    ax1.set_yscale('log', base=2)\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(batch_sizes, latencies, 'o-')\n",
    "    ax2.set_xlabel('Batch Size')\n",
    "    ax2.set_ylabel('Latency (ms)')\n",
    "    ax2.set_title('Latency vs Batch Size')\n",
    "    ax2.set_xscale('log', base=2)\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ed097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for base model\n",
    "print(\"\\n=== Memory Efficiency Analysis (Base Model) ===\")\n",
    "base_memory = analyze_memory_efficiency(configs['base'])\n",
    "print(f\"Memory Savings: {base_memory['memory_savings']*100:.1f}%\")\n",
    "print(\"\\nTechnique Breakdown:\")\n",
    "for technique, saving in base_memory['techniques'].items():\n",
    "    print(f\"{technique}: {saving*100:.1f}% improvement\")\n",
    "\n",
    "# Plot analyses\n",
    "plot_memory_analysis(analyses)\n",
    "\n",
    "# Run performance analysis\n",
    "print(\"\\n=== Performance Analysis (Base Model) ===\")\n",
    "perf_results = analyze_performance(configs['base'])\n",
    "plot_performance_analysis(perf_results)\n",
    "\n",
    "# Conclusions and model capabilities\n",
    "model_capabilities = {\n",
    "    'base': {\n",
    "        'recommended_batch_size': 32,\n",
    "        'max_seq_length': 2048,\n",
    "        'typical_throughput': base_perf['tokens_per_second'],\n",
    "        'memory_requirement': analyses['base']['params_gb'],\n",
    "        'optimal_use_cases': [\n",
    "            'Fine-tuning on single TPU',\n",
    "            'Low-latency inference',\n",
    "            'Memory-constrained environments'\n",
    "        ]\n",
    "    },\n",
    "    'xl': {\n",
    "        'recommended_batch_size': 16,\n",
    "        'max_seq_length': 2048,\n",
    "        'typical_throughput': analyses['xl']['theoretical_throughput'],\n",
    "        'memory_requirement': analyses['xl']['params_gb'],\n",
    "        'optimal_use_cases': [\n",
    "            'Large-scale training',\n",
    "            'Complex reasoning tasks',\n",
    "            'Multi-TPU training'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n=== Model Capabilities Summary ===\")\n",
    "for model_size, caps in model_capabilities.items():\n",
    "    print(f\"\\n{model_size.upper()} Model:\")\n",
    "    print(f\"Recommended batch size: {caps['recommended_batch_size']}\")\n",
    "    print(f\"Max sequence length: {caps['max_seq_length']}\")\n",
    "    print(f\"Typical throughput: {caps['typical_throughput']:,.0f} tokens/sec\")\n",
    "    print(f\"Memory requirement: {caps['memory_requirement']:.1f} GB\")\n",
    "    print(\"Optimal use cases:\")\n",
    "    for use_case in caps['optimal_use_cases']:\n",
    "        print(f\"- {use_case}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ae567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture Analysis\n",
    "def analyze_architecture(config):\n",
    "    \"\"\"Analyze model architecture and attention patterns\"\"\"\n",
    "    \n",
    "    # Architecture components\n",
    "    architecture = {\n",
    "        'embedding_dim': config['hidden_size'],\n",
    "        'num_layers': config['num_hidden_layers'],\n",
    "        'num_heads': config['num_attention_heads'],\n",
    "        'head_dim': config['hidden_size'] // config['num_attention_heads'],\n",
    "        'feed_forward_dim': config['intermediate_size'],\n",
    "        'vocab_size': config['vocab_size']\n",
    "    }\n",
    "    \n",
    "    # Attention analysis\n",
    "    attention_stats = {\n",
    "        'head_capacity': architecture['head_dim'],\n",
    "        'total_attention_capacity': architecture['head_dim'] * config['num_attention_heads'],\n",
    "        'attention_bottleneck_ratio': architecture['head_dim'] / architecture['embedding_dim'],\n",
    "        'heads_per_layer': config['num_attention_heads']\n",
    "    }\n",
    "    \n",
    "    # Depth analysis\n",
    "    depth_metrics = {\n",
    "        'depth_to_width_ratio': config['num_hidden_layers'] / (config['hidden_size'] / 64),\n",
    "        'computational_depth': config['num_hidden_layers'] * 2,  # Self-attention + FFN\n",
    "        'parameter_efficiency': config['hidden_size'] * config['num_hidden_layers'] / (config['vocab_size'] * config['hidden_size'])\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'architecture': architecture,\n",
    "        'attention': attention_stats,\n",
    "        'depth': depth_metrics\n",
    "    }\n",
    "\n",
    "def plot_attention_patterns(arch_analysis):\n",
    "    \"\"\"Visualize attention patterns and architecture characteristics\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot attention head distribution\n",
    "    heads_per_layer = arch_analysis['attention']['heads_per_layer']\n",
    "    head_dim = arch_analysis['attention']['head_capacity']\n",
    "    layers = range(arch_analysis['architecture']['num_layers'])\n",
    "    \n",
    "    attention_capacity = np.ones(len(layers)) * heads_per_layer * head_dim\n",
    "    ax1.plot(layers, attention_capacity, 'b-', label='Total Attention Capacity')\n",
    "    ax1.fill_between(layers, attention_capacity, alpha=0.3)\n",
    "    ax1.set_xlabel('Layer')\n",
    "    ax1.set_ylabel('Attention Capacity')\n",
    "    ax1.set_title('Attention Capacity Distribution')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot architecture dimensions\n",
    "    dims = {\n",
    "        'Embedding': arch_analysis['architecture']['embedding_dim'],\n",
    "        'Head': arch_analysis['attention']['head_capacity'],\n",
    "        'FFN': arch_analysis['architecture']['feed_forward_dim'] // 4,\n",
    "        'Total Attention': arch_analysis['attention']['total_attention_capacity']\n",
    "    }\n",
    "    \n",
    "    x = range(len(dims))\n",
    "    ax2.bar(x, list(dims.values()))\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(dims.keys(), rotation=45)\n",
    "    ax2.set_ylabel('Dimension Size')\n",
    "    ax2.set_title('Model Dimensions')\n",
    "    ax2.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return dims\n",
    "\n",
    "# Run architecture analysis\n",
    "print(\"\\n=== Architecture Analysis ===\")\n",
    "arch_analysis = analyze_architecture(configs['base'])\n",
    "\n",
    "print(\"\\nKey Architecture Metrics:\")\n",
    "print(f\"Embedding dimension: {arch_analysis['architecture']['embedding_dim']}\")\n",
    "print(f\"Number of layers: {arch_analysis['architecture']['num_layers']}\")\n",
    "print(f\"Attention heads per layer: {arch_analysis['attention']['heads_per_layer']}\")\n",
    "print(f\"Head dimension: {arch_analysis['attention']['head_capacity']}\")\n",
    "print(f\"Feed-forward dimension: {arch_analysis['architecture']['feed_forward_dim']}\")\n",
    "\n",
    "print(\"\\nAttention Analysis:\")\n",
    "print(f\"Total attention capacity: {arch_analysis['attention']['total_attention_capacity']}\")\n",
    "print(f\"Attention bottleneck ratio: {arch_analysis['attention']['attention_bottleneck_ratio']:.2f}\")\n",
    "\n",
    "print(\"\\nDepth Analysis:\")\n",
    "print(f\"Depth to width ratio: {arch_analysis['depth']['depth_to_width_ratio']:.2f}\")\n",
    "print(f\"Parameter efficiency: {arch_analysis['depth']['parameter_efficiency']:.2f}\")\n",
    "\n",
    "# Plot attention patterns\n",
    "dims = plot_attention_patterns(arch_analysis)\n",
    "\n",
    "# Add scaling analysis\n",
    "def analyze_scaling_characteristics(arch_analysis):\n",
    "    \"\"\"Analyze model scaling characteristics\"\"\"\n",
    "    \n",
    "    scaling_metrics = {\n",
    "        'compute_to_params_ratio': arch_analysis['architecture']['num_layers'] * \\\n",
    "                                 (arch_analysis['architecture']['embedding_dim'] ** 2) / \\\n",
    "                                 (arch_analysis['architecture']['vocab_size'] * arch_analysis['architecture']['embedding_dim']),\n",
    "        'attention_to_ffn_ratio': arch_analysis['attention']['total_attention_capacity'] / \\\n",
    "                                arch_analysis['architecture']['feed_forward_dim'],\n",
    "        'depth_to_width_ratio': arch_analysis['depth']['depth_to_width_ratio']\n",
    "    }\n",
    "    \n",
    "    scaling_recommendations = {\n",
    "        'next_scale_factor': min(2.0, scaling_metrics['compute_to_params_ratio'] / 6.0),\n",
    "        'suggested_width_scaling': scaling_metrics['attention_to_ffn_ratio'] > 0.25,\n",
    "        'suggested_depth_scaling': scaling_metrics['depth_to_width_ratio'] < 1.0\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'metrics': scaling_metrics,\n",
    "        'recommendations': scaling_recommendations\n",
    "    }\n",
    "\n",
    "# Run scaling analysis\n",
    "scaling_analysis = analyze_scaling_characteristics(arch_analysis)\n",
    "\n",
    "print(\"\\n=== Scaling Analysis ===\")\n",
    "print(\"\\nScaling Metrics:\")\n",
    "for metric, value in scaling_analysis['metrics'].items():\n",
    "    print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nScaling Recommendations:\")\n",
    "for rec, value in scaling_analysis['recommendations'].items():\n",
    "    print(f\"{rec}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Efficiency Analysis\n",
    "def analyze_memory_efficiency():\n",
    "    \"\"\"Analyze model memory usage and efficiency\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    def calculate_param_size(hidden_size, num_layers, vocab_size):\n",
    "        # Embedding parameters\n",
    "        embedding_params = vocab_size * hidden_size\n",
    "        \n",
    "        # Transformer layer parameters\n",
    "        attention_params = 4 * hidden_size * hidden_size  # Q,K,V + output projection\n",
    "        ffn_params = 4 * hidden_size * hidden_size  # Two linear layers\n",
    "        layer_norm_params = 4 * hidden_size  # Two layer norms\n",
    "        \n",
    "        params_per_layer = attention_params + ffn_params + layer_norm_params\n",
    "        total_layer_params = params_per_layer * num_layers\n",
    "        \n",
    "        # Total parameters\n",
    "        total_params = embedding_params + total_layer_params\n",
    "        return {\n",
    "            'embedding': embedding_params,\n",
    "            'attention': attention_params * num_layers,\n",
    "            'ffn': ffn_params * num_layers,\n",
    "            'layer_norm': layer_norm_params * num_layers,\n",
    "            'total': total_params\n",
    "        }\n",
    "    \n",
    "    # Calculate sizes for different precisions\n",
    "    def get_size_in_mb(num_params, bytes_per_param):\n",
    "        return (num_params * bytes_per_param) / (1024 * 1024)\n",
    "    \n",
    "    base_params = calculate_param_size(768, 12, 50000)\n",
    "    \n",
    "    memory_analysis = {\n",
    "        'fp32_size': {k: get_size_in_mb(v, 4) for k,v in base_params.items()},\n",
    "        'fp16_size': {k: get_size_in_mb(v, 2) for k,v in base_params.items()},\n",
    "        'int8_size': {k: get_size_in_mb(v, 1) for k,v in base_params.items()}\n",
    "    }\n",
    "    \n",
    "    # Activation memory estimation (batch_size=32, seq_len=512)\n",
    "    batch_size = 32\n",
    "    seq_len = 512\n",
    "    hidden_size = 768\n",
    "    \n",
    "    def estimate_activation_memory():\n",
    "        # Key activation sizes per layer\n",
    "        attn_activations = batch_size * seq_len * hidden_size * 4  # Q,K,V + output\n",
    "        ffn_activations = batch_size * seq_len * hidden_size * 4   # Two linear layers\n",
    "        residual_activations = batch_size * seq_len * hidden_size\n",
    "        \n",
    "        total_per_layer = attn_activations + ffn_activations + residual_activations\n",
    "        return get_size_in_mb(total_per_layer * 12, 2)  # Assuming FP16 training\n",
    "    \n",
    "    activation_mb = estimate_activation_memory()\n",
    "    \n",
    "    return memory_analysis, activation_mb\n",
    "\n",
    "# Run memory analysis\n",
    "memory_analysis, activation_mb = analyze_memory_efficiency()\n",
    "\n",
    "print(\"\\n=== Memory Efficiency Analysis ===\")\n",
    "print(\"\\nModel Size by Precision:\")\n",
    "for precision, sizes in memory_analysis.items():\n",
    "    print(f\"\\n{precision}:\")\n",
    "    for component, size in sizes.items():\n",
    "        print(f\"  {component}: {size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nEstimated Activation Memory (batch=32, seq=512): {activation_mb:.2f} MB\")\n",
    "\n",
    "# Visualize memory distribution\n",
    "def plot_memory_distribution(memory_analysis):\n",
    "    \"\"\"Plot memory usage distribution across components and precisions\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Component distribution for FP16\n",
    "    sizes = memory_analysis['fp16_size']\n",
    "    components = ['embedding', 'attention', 'ffn', 'layer_norm']\n",
    "    sizes_list = [sizes[c] for c in components]\n",
    "    \n",
    "    ax1.pie(sizes_list, labels=components, autopct='%1.1f%%')\n",
    "    ax1.set_title('Memory Distribution by Component (FP16)')\n",
    "    \n",
    "    # Precision comparison\n",
    "    precisions = ['fp32_size', 'fp16_size', 'int8_size']\n",
    "    total_sizes = [memory_analysis[p]['total'] for p in precisions]\n",
    "    labels = ['FP32', 'FP16', 'INT8']\n",
    "    \n",
    "    ax2.bar(labels, total_sizes)\n",
    "    ax2.set_ylabel('Size (MB)')\n",
    "    ax2.set_title('Model Size by Precision')\n",
    "    ax2.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot memory distribution\n",
    "plot_memory_distribution(memory_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93341bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Architecture Analysis\n",
    "def analyze_transformer_architecture():\n",
    "    \"\"\"Analyze transformer architecture components and interactions\"\"\"\n",
    "    \n",
    "    class TransformerBlockAnalysis:\n",
    "        def __init__(self, hidden_size=768, num_heads=12, ff_dim=3072):\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = hidden_size // num_heads\n",
    "            self.ff_dim = ff_dim\n",
    "            \n",
    "        def attention_complexity(self, seq_len):\n",
    "            # Complexity of attention computation\n",
    "            qk_multiply = seq_len * seq_len * self.head_dim * self.num_heads\n",
    "            attn_multiply = seq_len * seq_len * self.head_dim * self.num_heads\n",
    "            v_multiply = seq_len * self.hidden_size * self.hidden_size\n",
    "            return {\n",
    "                'qk_multiply': qk_multiply,\n",
    "                'attention_multiply': attn_multiply,\n",
    "                'value_multiply': v_multiply,\n",
    "                'total': qk_multiply + attn_multiply + v_multiply\n",
    "            }\n",
    "            \n",
    "        def ffn_complexity(self, seq_len):\n",
    "            # Complexity of feed-forward computation\n",
    "            first_layer = seq_len * self.hidden_size * self.ff_dim\n",
    "            second_layer = seq_len * self.ff_dim * self.hidden_size\n",
    "            return {\n",
    "                'first_layer': first_layer,\n",
    "                'second_layer': second_layer,\n",
    "                'total': first_layer + second_layer\n",
    "            }\n",
    "            \n",
    "        def receptive_field_analysis(self):\n",
    "            return {\n",
    "                'self_attention': 'Global (all positions)',\n",
    "                'ffn': 'Local (position-wise)',\n",
    "                'layer_norm': 'Local (position-wise)',\n",
    "                'residual': 'Identity mapping'\n",
    "            }\n",
    "            \n",
    "        def information_flow(self):\n",
    "            return {\n",
    "                'attention_bottleneck': self.head_dim,\n",
    "                'ffn_bottleneck': self.ff_dim,\n",
    "                'attention_paths': self.num_heads,\n",
    "                'max_path_length': 'Linear in number of layers'\n",
    "            }\n",
    "    \n",
    "    # Analyze model architecture\n",
    "    model = TransformerBlockAnalysis()\n",
    "    \n",
    "    print(\"\\n=== Transformer Architecture Analysis ===\")\n",
    "    \n",
    "    # Analyze computational complexity\n",
    "    seq_lengths = [128, 512, 1024]\n",
    "    print(\"\\nComputational Complexity Analysis:\")\n",
    "    for seq_len in seq_lengths:\n",
    "        attn_complex = model.attention_complexity(seq_len)\n",
    "        ffn_complex = model.ffn_complexity(seq_len)\n",
    "        \n",
    "        print(f\"\\nSequence Length: {seq_len}\")\n",
    "        print(f\"Self-Attention Operations: {attn_complex['total']:,}\")\n",
    "        print(f\"FFN Operations: {ffn_complex['total']:,}\")\n",
    "    \n",
    "    # Analyze receptive field\n",
    "    print(\"\\nReceptive Field Analysis:\")\n",
    "    for component, field in model.receptive_field_analysis().items():\n",
    "        print(f\"  {component}: {field}\")\n",
    "    \n",
    "    # Analyze information flow\n",
    "    print(\"\\nInformation Flow Analysis:\")\n",
    "    for metric, value in model.information_flow().items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run architecture analysis\n",
    "transformer_analysis = analyze_transformer_architecture()\n",
    "\n",
    "# Visualize attention patterns\n",
    "def plot_attention_patterns():\n",
    "    \"\"\"Visualize different attention pattern types\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    \n",
    "    # Local attention pattern\n",
    "    local = np.zeros((16, 16))\n",
    "    for i in range(16):\n",
    "        window = 3\n",
    "        start = max(0, i-window)\n",
    "        end = min(16, i+window+1)\n",
    "        local[i, start:end] = 1\n",
    "    axes[0,0].imshow(local)\n",
    "    axes[0,0].set_title('Local Attention Pattern')\n",
    "    \n",
    "    # Global attention pattern\n",
    "    global_attn = np.ones((16, 16))\n",
    "    axes[0,1].imshow(global_attn)\n",
    "    axes[0,1].set_title('Global Attention Pattern')\n",
    "    \n",
    "    # Causal attention pattern\n",
    "    causal = np.tril(np.ones((16, 16)))\n",
    "    axes[1,0].imshow(causal)\n",
    "    axes[1,0].set_title('Causal Attention Pattern')\n",
    "    \n",
    "    # Sparse attention pattern\n",
    "    sparse = np.zeros((16, 16))\n",
    "    sparse[::2, ::2] = 1\n",
    "    sparse[1::4, :] = 1\n",
    "    axes[1,1].imshow(sparse)\n",
    "    axes[1,1].set_title('Sparse Attention Pattern')\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot attention patterns\n",
    "plot_attention_patterns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
