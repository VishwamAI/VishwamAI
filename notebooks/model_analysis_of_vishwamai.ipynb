{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VishwamAI/VishwamAI/blob/main/notebooks/model_analysis_of_vishwamai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHBPBPa8Rj1f"
   },
   "source": [
    "# Task\n",
    "model analyisis of vishwamai and devlopments integrating with advancements of ai and devlopment https://github.com/VishwamAI/VishwamAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikEbffYpSb25"
   },
   "source": [
    "## Model analysis\n",
    "\n",
    "### Subtask:\n",
    "Train a VishwamAI model using the prepared data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBMTjIbjTDlx"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/VishwamAI/VishwamAI\n",
    "%cd VishwamAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qt3H8COoTHmf"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt kauldron albumentations kornia timm openai-whisper torch torchaudio torchvision -f https://download.pytorch.org/whl/cu118/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ssg4XFVJUzBi"
   },
   "outputs": [],
   "source": [
    "!python importtest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZZO5isCvwEX"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdErK93CnauD"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from vishwamai.layers.layers import TPUMoELayer\n",
    "from vishwamai.thoughts.tot import TreeOfThoughts, ThoughtNode\n",
    "from vishwamai.thoughts.cot import ChainOfThoughtPrompting\n",
    "from vishwamai.transformer import create_vishwamai_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaYMNwzNq5zK"
   },
   "outputs": [],
   "source": [
    "def analyze_moe_config(num_experts: int, expert_dim: int, capacity_factor: float = 1.0):\n",
    "    \"\"\"Analyze MoE configuration and compute resource requirements\"\"\"\n",
    "    config = {\n",
    "        'num_experts': num_experts,\n",
    "        'expert_dim': expert_dim,\n",
    "        'capacity_factor': capacity_factor,\n",
    "        'router_dim': 256,\n",
    "        'router_capacity': int(capacity_factor * (expert_dim / num_experts))\n",
    "    }\n",
    "\n",
    "    # Calculate parameter counts\n",
    "    router_params = config['router_dim'] * num_experts\n",
    "    expert_params = num_experts * (expert_dim * expert_dim * 4)  # FFN params per expert\n",
    "\n",
    "    return {\n",
    "        'config': config,\n",
    "        'router_params': router_params,\n",
    "        'expert_params': expert_params,\n",
    "        'total_params': router_params + expert_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYRnHXw6q8dG"
   },
   "outputs": [],
   "source": [
    "def create_sample_tree():\n",
    "    \"\"\"Create a sample thought tree for visualization\"\"\"\n",
    "    root = ThoughtNode(thought=\"Initial problem\", value=0.0)\n",
    "\n",
    "    # Create child thoughts\n",
    "    thought1 = ThoughtNode(\n",
    "        thought=\"Approach 1: Direct solution\",\n",
    "        value=0.7,\n",
    "        parent=root,\n",
    "        depth=1\n",
    "    )\n",
    "    thought2 = ThoughtNode(\n",
    "        thought=\"Approach 2: Break down problem\",\n",
    "        value=0.8,\n",
    "        parent=root,\n",
    "        depth=1\n",
    "    )\n",
    "\n",
    "    # Add sub-thoughts to thought2\n",
    "    sub1 = ThoughtNode(\n",
    "        thought=\"Sub-problem 1: Analyze components\",\n",
    "        value=0.85,\n",
    "        parent=thought2,\n",
    "        depth=2\n",
    "    )\n",
    "    sub2 = ThoughtNode(\n",
    "        thought=\"Sub-problem 2: Synthesize solution\",\n",
    "        value=0.9,\n",
    "        parent=thought2,\n",
    "        depth=2\n",
    "    )\n",
    "\n",
    "    # Set up the tree structure\n",
    "    root.children = [thought1, thought2]\n",
    "    thought2.children = [sub1, sub2]\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlSwrXmzrI8A"
   },
   "outputs": [],
   "source": [
    "def visualize_thought_tree(node: ThoughtNode, level: int = 0):\n",
    "    \"\"\"Visualize a thought tree with values\"\"\"\n",
    "    prefix = \"  \" * level\n",
    "    print(f\"{prefix}└─ {node.thought} (value: {node.value:.2f})\")\n",
    "\n",
    "    for child in node.children:\n",
    "        visualize_thought_tree(child, level + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mgQbhE3rK4w"
   },
   "outputs": [],
   "source": [
    "def create_advanced_config():\n",
    "    \"\"\"Create model configuration with advanced features\"\"\"\n",
    "    return {\n",
    "        'vocab_size': 32000,\n",
    "        'num_layers': 12,\n",
    "        'num_heads': 12,\n",
    "        'head_dim': 64,\n",
    "        'hidden_dim': 768,\n",
    "        'mlp_dim': 3072,\n",
    "        'max_seq_len': 2048,\n",
    "        'num_experts': 8,\n",
    "        'expert_dim': 3072,\n",
    "        'tot_max_steps': 10,\n",
    "        'tot_beam_width': 3,\n",
    "        'dropout_rate': 0.1\n",
    "    }\n",
    "\n",
    "config = create_advanced_config()\n",
    "moe_analysis = analyze_moe_config(config['num_experts'], config['expert_dim'])\n",
    "\n",
    "print(\"MoE Configuration:\")\n",
    "print(f\"Number of Experts: {config['num_experts']}\")\n",
    "print(f\"Expert Dimension: {config['expert_dim']}\")\n",
    "print(f\"Total MoE Parameters: {moe_analysis['total_params']:,}\")\n",
    "\n",
    "print(\"\\nSample Thought Tree:\")\n",
    "root = create_sample_tree()\n",
    "visualize_thought_tree(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQrYNzC2rNIG"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from vishwamai.model import VishwamAI\n",
    "from vishwamai.kernels.kernel import fp8_gemm_optimized\n",
    "from vishwamai.layers.attention import FlashAttention\n",
    "from vishwamai.transformer import (\n",
    "    TransformerModel,\n",
    "    EnhancedTransformerModel,\n",
    "    create_vishwamai_transformer\n",
    ")\n",
    "from vishwamai.training import TPUTrainingConfig, create_train_state_tpu, create_train_step_tpu\n",
    "from vishwamai.profiler import TPUProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUezh1iksKbk"
   },
   "outputs": [],
   "source": [
    "def create_sample_config():\n",
    "    \"\"\"Create TPU-optimized training configuration\"\"\"\n",
    "    model_config = {\n",
    "        'vocab_size': 32000,\n",
    "        'num_layers': 12,\n",
    "        'num_heads': 12,\n",
    "        'head_dim': 64,\n",
    "        'hidden_dim': 768,\n",
    "        'mlp_dim': 3072,\n",
    "        'max_seq_len': 2048,\n",
    "        'dropout_rate': 0.1,\n",
    "        'use_enhanced': True,\n",
    "        'use_rotary': True,\n",
    "        'use_flash_attn': True,\n",
    "        'use_rms_norm': False\n",
    "    }\n",
    "    \n",
    "    # Create TPUTrainingConfig\n",
    "    return TPUTrainingConfig(\n",
    "        model_config=model_config,\n",
    "        batch_size=32,\n",
    "        grad_accum_steps=4,\n",
    "        learning_rate=1e-4,\n",
    "        warmup_steps=2000,\n",
    "        max_steps=100000,\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        dtype='bfloat16',\n",
    "        enable_pjit=True,\n",
    "        block_size=128,\n",
    "        use_flash_attn=True,\n",
    "        mixed_precision=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v3mLE0RsMFy"
   },
   "outputs": [],
   "source": [
    "def analyze_model_size(config):\n",
    "    \"\"\"Calculate model size and component breakdown\"\"\"\n",
    "    if hasattr(config, 'model_config'):\n",
    "        # Handle TPUTrainingConfig object\n",
    "        model_config = config.model_config\n",
    "    else:\n",
    "        # Handle raw dictionary config\n",
    "        model_config = config\n",
    "    \n",
    "    vocab_size = model_config['vocab_size']\n",
    "    hidden_dim = model_config['hidden_dim']\n",
    "    num_layers = model_config['num_layers']\n",
    "    mlp_dim = model_config['mlp_dim']\n",
    "    \n",
    "    embedding_params = vocab_size * hidden_dim\n",
    "    attention_params = num_layers * (4 * hidden_dim * hidden_dim)\n",
    "    ffn_params = num_layers * (2 * hidden_dim * mlp_dim)\n",
    "    layer_norm_params = num_layers * 2 * hidden_dim\n",
    "    \n",
    "    return {\n",
    "        'total': embedding_params + attention_params + ffn_params + layer_norm_params,\n",
    "        'embedding': embedding_params,\n",
    "        'attention': attention_params,\n",
    "        'ffn': ffn_params,\n",
    "        'layer_norm': layer_norm_params\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KN0Mf5EHsNzm"
   },
   "outputs": [],
   "source": [
    "# Analyze model architecture\n",
    "config = create_sample_config()\n",
    "model_stats = analyze_model_size(config)\n",
    "\n",
    "# Plot parameter distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie([v/model_stats['total'] for v in model_stats.values()][1:],\n",
    "        labels=[k for k in model_stats.keys()][1:],\n",
    "        autopct='%1.1f%%')\n",
    "plt.title('Parameter Distribution Across Model Components')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpLrtmDQsRV3"
   },
   "outputs": [],
   "source": [
    "# Ensure the config is complete by using create_sample_config\n",
    "# The variable `config` is already defined in the notebook, so we don't need to redefine it.\n",
    "\n",
    "# Initialize model and print summary\n",
    "model = create_vishwamai_transformer(config.model_config)\n",
    "\n",
    "# Print model summary accessing config\n",
    "print(f\"Total Parameters: {model_stats['total']:,}\")\n",
    "print(f\"Hidden Dimension: {config.model_config['hidden_dim']}\")\n",
    "print(f\"Number of Layers: {config.model_config['num_layers']}\")\n",
    "print(f\"Number of Attention Heads: {config.model_config['num_heads']}\")\n",
    "print(f\"Maximum Sequence Length: {config.model_config['max_seq_len']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-ueO01YsT3Q"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from vishwamai.kernels.kernel import fp8_gemm_optimized\n",
    "from vishwamai.layers.attention import FlashAttention\n",
    "from vishwamai.layers.layers import TPUGEMMLinear, TPULayerNorm\n",
    "from vishwamai.transformer import create_vishwamai_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jR0ZoGT6s-yG"
   },
   "outputs": [],
   "source": [
    "def benchmark_gemm(batch_size: int, seq_len: int, hidden_dim: int):\n",
    "    \"\"\"Benchmark GEMM operations with and without optimizations\"\"\"\n",
    "    x = jnp.ones((batch_size, seq_len, hidden_dim))\n",
    "    w = jnp.ones((hidden_dim, hidden_dim))\n",
    "\n",
    "    # Standard GEMM\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = jnp.dot(x, w)\n",
    "    std_time = (time.time() - start) / 10\n",
    "\n",
    "    # Optimized GEMM\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = fp8_gemm_optimized(x, w)\n",
    "    opt_time = (time.time() - start) / 10\n",
    "\n",
    "    return std_time, opt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwGFaQVCtBV-"
   },
   "outputs": [],
   "source": [
    "def benchmark_attention(batch_size: int, seq_len: int, hidden_dim: int, num_heads: int):\n",
    "    \"\"\"Benchmark attention implementations\"\"\"\n",
    "    head_dim = hidden_dim // num_heads\n",
    "\n",
    "    # Initialize inputs\n",
    "    q = jnp.ones((batch_size, seq_len, num_heads, head_dim))\n",
    "    k = jnp.ones((batch_size, seq_len, num_heads, head_dim))\n",
    "    v = jnp.ones((batch_size, seq_len, num_heads, head_dim))\n",
    "\n",
    "    # Standard attention\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        scores = jnp.einsum('bthd,bshd->btsh', q, k)\n",
    "        scores = scores / jnp.sqrt(head_dim)\n",
    "        attn = jax.nn.softmax(scores)\n",
    "        output = jnp.einsum('btsh,bshd->bthd', attn, v)\n",
    "    std_time = (time.time() - start) / 10\n",
    "\n",
    "    # Flash attention\n",
    "    flash_attn = FlashAttention(num_heads=num_heads, head_dim=head_dim)\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = flash_attn(q, k, v)\n",
    "    flash_time = (time.time() - start) / 10\n",
    "\n",
    "    return std_time, flash_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOjTlfHbtDKU"
   },
   "outputs": [],
   "source": [
    "def analyze_memory_usage(config: Dict):\n",
    "    \"\"\"Analyze memory usage of different components\"\"\"\n",
    "    batch_size = 32\n",
    "    seq_len = config['max_seq_len']\n",
    "    hidden_dim = config['hidden_dim']\n",
    "\n",
    "    # Calculate memory requirements\n",
    "    activations = batch_size * seq_len * hidden_dim * 2  # BF16\n",
    "    attention = batch_size * seq_len * seq_len * config['num_heads'] * 2  # BF16\n",
    "    kv_cache = 2 * batch_size * seq_len * hidden_dim * 2  # BF16\n",
    "\n",
    "    # Convert to MB\n",
    "    mb = 1024 * 1024\n",
    "    return {\n",
    "        'activations': activations / mb,\n",
    "        'attention': attention / mb,\n",
    "        'kv_cache': kv_cache / mb\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDiwmIf0tEwT"
   },
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "config = {\n",
    "    'max_seq_len': 2048,\n",
    "    'hidden_dim': 768,\n",
    "    'num_heads': 12\n",
    "}\n",
    "\n",
    "# Memory analysis\n",
    "memory_usage = analyze_memory_usage(config)\n",
    "\n",
    "# Plot memory usage\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(memory_usage.keys(), memory_usage.values())\n",
    "plt.title('Memory Usage by Component (MB)')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Print summary\n",
    "print(\"Memory Usage Summary (MB):\")\n",
    "for k, v in memory_usage.items():\n",
    "    print(f\"{k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TPU system\n",
    "import jax\n",
    "\n",
    "# For TPUs in Colab\n",
    "print('Available devices:', jax.devices())\n",
    "print('Device count:', jax.device_count())\n",
    "\n",
    "# Create device mesh for training\n",
    "devices = jax.devices()\n",
    "mesh_shape = (len(devices),)\n",
    "device_mesh = jax.sharding.Mesh(devices, ('batch',))\n",
    "\n",
    "print(f'\\nDevice mesh shape: {mesh_shape}')\n",
    "print(f'Using {len(devices)} devices for training')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "cell_execution_strategy": "setup",
   "gpuType": "V28",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
