{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K Math Problem Training\n",
    "\n",
    "This notebook implements training on the GSM8K dataset using TPU acceleration and optimized configurations for mathematical reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import jax\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.experimental.maps import Mesh\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from vishwamai.training import train, create_train_state\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.tokenizer import VishwamAITokenizer\n",
    "from omegaconf import OmegaConf\n",
    "import logging\n",
    "from safetensors.flax import save_file\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configure logging and plotting\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU Setup\n",
    "\n",
    "Configure TPU environment and create device mesh for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TPU environment setup\n",
    "os.environ['TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD'] = '10000000000'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ['JAX_PLATFORMS'] = 'tpu'\n",
    "os.environ['JAX_ENABLE_X64'] = 'False'\n",
    "\n",
    "def setup_tpu_cluster():\n",
    "    \"\"\"Set up JAX TPU cluster configuration.\"\"\"\n",
    "    devices = jax.devices()\n",
    "    print(f\"Available devices: {devices}\")\n",
    "    \n",
    "    # Create mesh for data parallelism\n",
    "    mesh_shape = (8,)  # 8-core TPU\n",
    "    device_mesh = mesh_utils.create_device_mesh(mesh_shape)\n",
    "    mesh = Mesh(device_mesh, ('dp',))\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "mesh = setup_tpu_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n",
    "\n",
    "Load model and training configurations optimized for GSM8K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load configurations\n",
    "model_config = OmegaConf.load('../vishwamai/configs/model/10B.yaml')\n",
    "training_config = OmegaConf.load('../vishwamai/configs/training/gsm8k.yaml')\n",
    "\n",
    "print(\"Model config:\", model_config)\n",
    "print(\"\\nTraining config:\", training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Implement GSM8K dataset processing with step-by-step solution formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GSM8KProcessor:\n",
    "    \"\"\"Processor for GSM8K dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, config):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.max_length = config.dataset.max_length\n",
    "    \n",
    "    def format_example(self, example):\n",
    "        \"\"\"Format a GSM8K example for training.\"\"\"\n",
    "        question = example['question']\n",
    "        answer = example['answer']\n",
    "        # Extract final answer\n",
    "        final_answer = answer.split('####')[-1].strip()\n",
    "        # Format as instruction and response\n",
    "        formatted_text = f\"Question: {question}\\nLet's solve this step by step:\\n{answer}\\nFinal Answer: {final_answer}\"\n",
    "        return formatted_text\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize a batch of formatted examples.\"\"\"\n",
    "        formatted_texts = [self.format_example(ex) for ex in examples]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            formatted_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        \n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "    \n",
    "    def prepare_dataset(self, dataset):\n",
    "        \"\"\"Prepare GSM8K dataset.\"\"\"\n",
    "        tokenized_dataset = dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=self.config.dataset.num_workers,\n",
    "            remove_columns=dataset.column_names,\n",
    "        )\n",
    "        return tokenized_dataset\n",
    "\n",
    "def create_gsm8k_dataloader(config, split=\"train\"):\n",
    "    \"\"\"Create data loader for GSM8K dataset.\"\"\"\n",
    "    dataset = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n",
    "    \n",
    "    tokenizer = VishwamAITokenizer(\n",
    "        vocab_size=config.model.vocab_size,\n",
    "        model_prefix=config.model.name\n",
    "    )\n",
    "    \n",
    "    data_processor = GSM8KProcessor(tokenizer, config)\n",
    "    processed_dataset = data_processor.prepare_dataset(dataset)\n",
    "    \n",
    "    print(f\"Processed {len(processed_dataset)} examples for {split} split\")\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model\n",
    "model = VishwamAIModel(ModelConfig(**model_config))\n",
    "print(\"Model initialized with config:\", model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create dataloaders\n",
    "train_dataset = create_gsm8k_dataloader(training_config, split=\"train\")\n",
    "val_dataset = create_gsm8k_dataloader(training_config, split=\"validation\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'checkpoints', 'gsm8k')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def save_checkpoint_hook(state, path):\n",
    "    \"\"\"Save checkpoint in safetensors format.\"\"\"\n",
    "    numpy_params = jax.tree_map(lambda x: np.array(x), state.params)\n",
    "    save_file(numpy_params, f\"{path}.safetensors\")\n",
    "    print(f\"Saved checkpoint to {path}.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run training with TPU mesh\n",
    "with mesh:\n",
    "    final_state = train(\n",
    "        model,\n",
    "        training_config,\n",
    "        train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        num_steps=training_config.max_steps,\n",
    "        log_every=training_config.logging_steps,\n",
    "        eval_every=training_config.eval_steps,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        save_checkpoint_fn=save_checkpoint_hook\n",
    "    )\n",
    "\n",
    "# Save final model\n",
    "final_path = os.path.join(checkpoint_dir, \"gsm8k_final.safetensors\")\n",
    "numpy_params = jax.tree_map(lambda x: np.array(x), final_state.params)\n",
    "save_file(numpy_params, final_path)\n",
    "print(f\"\\nTraining completed! Final model saved to {final_path}\")\n",
    "print(f\"Best metrics: {final_state.best_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from safetensors.flax import load_file\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load model from safetensors checkpoint.\"\"\"\n",
    "    params = load_file(model_path)\n",
    "    # Convert numpy arrays to jax arrays\n",
    "    params = jax.tree_map(lambda x: jnp.array(x), params)\n",
    "    return params\n",
    "\n",
    "def evaluate_gsm8k_sample(model, tokenizer, question):\n",
    "    \"\"\"Test model on a single GSM8K question.\"\"\"\n",
    "    # Format input\n",
    "    input_text = f\"Question: {question}\\nLet's solve this step by step:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"jax\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=1024,\n",
    "        temperature=0.7,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the trained model\n",
    "model_path = os.path.join(checkpoint_dir, \"gsm8k_final.safetensors\")\n",
    "params = load_model(model_path)\n",
    "model.params = params\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = VishwamAITokenizer(\n",
    "    vocab_size=model_config.vocab_size,\n",
    "    model_prefix=model_config.name\n",
    ")\n",
    "\n",
    "# Test sample questions\n",
    "test_questions = [\n",
    "    \"Janet's ducks lay 16 eggs per day. She eats 3 for breakfast every morning and sells the rest to her neighbors for $2 per egg. How much money does she make per week?\",\n",
    "    \"A shop sells each ice cream cone for $2.50. On Monday they sold 45 cones, on Tuesday 52 cones, and on Wednesday 38 cones. What was their total revenue for these three days?\",\n",
    "    \"John has 5 times as many marbles as Peter. If Peter has 8 marbles, how many do they have together?\"\n",
    "]\n",
    "\n",
    "print(\"Testing model on sample questions:\\n\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"Question {i}:\\n{question}\")\n",
    "    response = evaluate_gsm8k_sample(model, tokenizer, question)\n",
    "    print(f\"\\nModel Response:\\n{response}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_test_set(model, tokenizer, num_samples=100):\n",
    "    \"\"\"Evaluate model on a subset of GSM8K test set.\"\"\"\n",
    "    # Load test dataset\n",
    "    test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "    if num_samples:\n",
    "        test_dataset = test_dataset.select(range(num_samples))\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(test_dataset)\n",
    "    \n",
    "    print(f\"Evaluating on {total} test samples...\\n\")\n",
    "    \n",
    "    for i, example in enumerate(test_dataset):\n",
    "        # Get model's response\n",
    "        response = evaluate_gsm8k_sample(model, tokenizer, example['question'])\n",
    "        \n",
    "        # Extract predicted answer\n",
    "        try:\n",
    "            pred_answer = float(response.split('Final Answer:')[-1].strip())\n",
    "            true_answer = float(example['answer'].split('####')[-1].strip())\n",
    "            \n",
    "            # Check if correct (allowing for small floating point differences)\n",
    "            is_correct = abs(pred_answer - true_answer) < 1e-6\n",
    "            correct += int(is_correct)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Progress: {i+1}/{total} samples evaluated\")\n",
    "                print(f\"Current accuracy: {(correct/(i+1))*100:.2f}%\\n\")\n",
    "                \n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not extract numeric answer for sample {i}\")\n",
    "    \n",
    "    final_accuracy = (correct/total) * 100\n",
    "    print(f\"\\nEvaluation complete!\")\n",
    "    print(f\"Final accuracy: {final_accuracy:.2f}%\")\n",
    "    print(f\"Correct: {correct}/{total}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': final_accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run batch evaluation\n",
    "print(\"Starting batch evaluation on test set...\")\n",
    "eval_results = evaluate_test_set(model, tokenizer, num_samples=100)\n",
    "print(\"\\nTest set evaluation results:\")\n",
    "print(f\"Accuracy: {eval_results['accuracy']:.2f}%\")\n",
    "print(f\"Correct: {eval_results['correct']}/{eval_results['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_results(eval_results, final_state):\n",
    "    \"\"\"Create visualizations of model performance.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Training Loss Curve\n",
    "    if hasattr(final_state, 'metrics_history'):\n",
    "        steps = range(len(final_state.metrics_history['loss']))\n",
    "        ax1.plot(steps, final_state.metrics_history['loss'], label='Training Loss')\n",
    "        ax1.set_title('Training Loss Over Time')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Plot 2: Test Set Performance\n",
    "    ax2.bar(['Correct', 'Incorrect'], \n",
    "            [eval_results['correct'], eval_results['total'] - eval_results['correct']],\n",
    "            color=['green', 'red'])\n",
    "    ax2.set_title('Test Set Performance')\n",
    "    ax2.text(0, eval_results['correct'], f\"{eval_results['accuracy']:.1f}%\",\n",
    "             ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create performance visualizations\n",
    "visualize_results(eval_results, final_state)\n",
    "\n",
    "# Save visualization\n",
    "plt.savefig(os.path.join(checkpoint_dir, 'performance_visualization.png'))\n",
    "print(f\"Performance visualization saved to {os.path.join(checkpoint_dir, 'performance_visualization.png')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
