{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Enhanced Distillation with ToT and Error Correction\n",
    "\n",
    "This notebook implements knowledge distillation using safetensors format for efficient model handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# JAX related imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Model loading and weights handling\n",
    "from huggingface_hub import login, snapshot_download\n",
    "import safetensors.flax as stf\n",
    "import safetensors\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"vishwamai_distillation\")\n",
    "\n",
    "# Import VishwamAI components\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.tokenizer import VishwamAITokenizer\n",
    "from vishwamai.tot import TreeOfThoughts\n",
    "from vishwamai.integration import ToTIntegrationLayer\n",
    "from vishwamai.training.distillation import DistillationTrainer\n",
    "from vishwamai.training.data import create_train_dataloader, create_val_dataloader\n",
    "from vishwamai.training.utils import set_seed, setup_training, get_training_args\n",
    "\n",
    "# Check available devices and set random seed\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_safetensor_weights(model_path: Path, memory_limit_gb: float = 4.0):\n",
    "    \"\"\"Load model weights from safetensors format\"\"\"\n",
    "    weights = {}\n",
    "    total_size = 0\n",
    "    current_shard_size = 0\n",
    "    memory_limit = memory_limit_gb * 1024 * 1024 * 1024\n",
    "    \n",
    "    shard_files = sorted(model_path.glob(\"*.safetensors\"))\n",
    "    if not shard_files:\n",
    "        raise ValueError(f\"No safetensors files found in {model_path}\")\n",
    "    \n",
    "    for shard_file in shard_files:\n",
    "        logger.info(f\"Loading shard: {shard_file.name}\")\n",
    "        shard = stf.load_file(str(shard_file))\n",
    "        shard_size = sum(tensor.nbytes for tensor in shard.values())\n",
    "        \n",
    "        if current_shard_size + shard_size > memory_limit:\n",
    "            logger.warning(\"Memory limit reached, skipping remaining shards\")\n",
    "            break\n",
    "            \n",
    "        weights.update(shard)\n",
    "        current_shard_size += shard_size\n",
    "        total_size += shard_size\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def download_partial_model(model_path: str, num_shards: int = 15):\n",
    "    \"\"\"Download model shards in safetensors format\"\"\"\n",
    "    patterns = [f\"model-{i+1:05d}-of-00252.safetensors\" for i in range(num_shards)]\n",
    "    patterns.extend([\"config.json\", \"tokenizer.model\", \"tokenizer_config.json\"])\n",
    "    \n",
    "    try:\n",
    "        local_path = snapshot_download(\n",
    "            repo_id=model_path,\n",
    "            allow_patterns=patterns,\n",
    "            local_files_only=False,\n",
    "            resume_download=True\n",
    "        )\n",
    "        logger.info(f\"Downloaded model to: {local_path}\")\n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error downloading model: {str(e)}\")\n",
    "\n",
    "# Configuration\n",
    "TEACHER_MODEL_ID = \"perplexity-ai/r1-1776\"\n",
    "HF_TOKEN = \"your_token_here\"  # Replace with actual token\n",
    "\n",
    "# Login to Hugging Face\n",
    "try:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Successfully logged in to Hugging Face Hub\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging in: {str(e)}\\nPlease check your token and try again.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate configuration\n",
    "config_path = Path(\"vishwamai/configs/training/perplexity_r1_distillation.yaml\")\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "# Get training arguments and setup\n",
    "training_args = get_training_args(config)\n",
    "training_setup = setup_training(config)\n",
    "\n",
    "# Initialize models\n",
    "try:\n",
    "    # Download and initialize teacher model\n",
    "    teacher_path = Path(download_partial_model(TEACHER_MODEL_ID, num_shards=5))\n",
    "    teacher_weights = load_safetensor_weights(teacher_path)\n",
    "    \n",
    "    # Initialize models with proper configuration\n",
    "    teacher_model = VishwamAIModel(ModelConfig(**config.distillation.teacher_model.config))\n",
    "    teacher_model.load_weights(teacher_weights)\n",
    "    \n",
    "    student_model = VishwamAIModel(ModelConfig(**config.distillation.student_model.config))\n",
    "    tokenizer = VishwamAITokenizer.from_pretrained(str(teacher_path))\n",
    "    \n",
    "    print(\"Models initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing models: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training components\n",
    "try:\n",
    "    # Create data loaders with proper batch size handling\n",
    "    train_loader = create_train_dataloader(\n",
    "        config,\n",
    "        tokenizer=tokenizer,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = create_val_dataloader(\n",
    "        config,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer with all components\n",
    "    trainer = DistillationTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        config=config,\n",
    "        training_args=training_args,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = Path(config.training.checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"Training components initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing training components: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with proper error handling\n",
    "try:\n",
    "    logger.info(\"Starting training\")\n",
    "    metrics = trainer.train(\n",
    "        num_steps=config.training.max_steps,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        save_steps=config.training.save_steps,\n",
    "        eval_steps=config.training.eval_steps\n",
    "    )\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Save current state even if training fails\n",
    "    try:\n",
    "        trainer.save_checkpoint(checkpoint_dir / \"interrupted_state\")\n",
    "    except:\n",
    "        logger.warning(\"Failed to save interrupted state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save training results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "steps = range(len(metrics['loss']))\n",
    "plt.plot(steps, metrics['loss'], label='Total Loss')\n",
    "plt.plot(steps, metrics['distill_loss'], label='Distillation Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, metrics['learning_rate'])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(checkpoint_dir / 'training_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "# Save detailed metrics and configuration\n",
    "metrics_path = checkpoint_dir / 'training_metrics.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'metrics': metrics,\n",
    "        'config': OmegaConf.to_container(config),\n",
    "        'training_args': training_args\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Final loss: {metrics['loss'][-1]:.4f}\")\n",
    "print(f\"Final distillation loss: {metrics['distill_loss'][-1]:.4f}\")\n",
    "print(f\"\\nResults saved to: {checkpoint_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
