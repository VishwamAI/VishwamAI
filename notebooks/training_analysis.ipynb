{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b069c32b",
   "metadata": {},
   "source": [
    "# VishwamAI Training Performance Analysis\n",
    "\n",
    "This notebook analyzes the training performance with TPU optimizations and fixed FlashAttention implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80475a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 17:43:46.445221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742386426.519071   16025 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742386426.539317   16025 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Any\n",
    "\n",
    "from vishwamai.training import create_train_state_tpu, create_train_step_tpu\n",
    "from vishwamai.profiler import TPUProfiler\n",
    "from vishwamai.transformer import create_vishwamai_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc9511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7473, 2)\n",
      "Test data shape: (1319, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load training and test data\n",
    "train_data = pd.read_parquet('/home/kasinadhsarma/VishwamAI/train-00000-of-00001.parquet')\n",
    "test_data = pd.read_parquet('/home/kasinadhsarma/VishwamAI/test-00000-of-00001.parquet')\n",
    "\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8177ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_config():\n",
    "    \"\"\"Create TPU-optimized training configuration\"\"\"\n",
    "    model_config = {\n",
    "        'vocab_size': 32000,\n",
    "        'num_layers': 12,\n",
    "        'num_heads': 12,\n",
    "        'head_dim': 64,\n",
    "        'hidden_dim': 768,\n",
    "        'mlp_dim': 3072,\n",
    "        'max_seq_len': 2048,\n",
    "        'dropout_rate': 0.1,\n",
    "        'use_flash_attn': True,\n",
    "        'use_rotary': True,\n",
    "        'use_rms_norm': False\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'model_config': model_config,\n",
    "        'batch_size': 32,\n",
    "        'grad_accum_steps': 4,\n",
    "        'learning_rate': 1e-4,\n",
    "        'warmup_steps': 2000,\n",
    "        'max_steps': 100000,\n",
    "        'dtype': jnp.bfloat16,\n",
    "        'enable_pjit': True,\n",
    "        'block_size': 128,\n",
    "        'mixed_precision': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d78b5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-03-19 17:43:54,603:jax._src.xla_bridge:966: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model and training state...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create model and initialize training state\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing model and training state...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_state_tpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m train_step \u001b[38;5;241m=\u001b[39m create_train_step_tpu(config, state)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize profiler\u001b[39;00m\n",
      "File \u001b[0;32m~/VishwamAI/vishwamai/training.py:72\u001b[0m, in \u001b[0;36mcreate_train_state_tpu\u001b[0;34m(config, rng, mesh)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mCreate training state optimized for TPU execution.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    mesh: Optional device mesh for model parallelism\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Create learning rate schedule\u001b[39;00m\n\u001b[1;32m     70\u001b[0m warmup_fn \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mlinear_schedule(\n\u001b[1;32m     71\u001b[0m     init_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m---> 72\u001b[0m     end_value\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m,\n\u001b[1;32m     73\u001b[0m     transition_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwarmup_steps\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m decay_fn \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mcosine_decay_schedule(\n\u001b[1;32m     76\u001b[0m     init_value\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m     77\u001b[0m     decay_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m-\u001b[39m config\u001b[38;5;241m.\u001b[39mwarmup_steps\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     79\u001b[0m lr_schedule \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mjoin_schedules(\n\u001b[1;32m     80\u001b[0m     schedules\u001b[38;5;241m=\u001b[39m[warmup_fn, decay_fn],\n\u001b[1;32m     81\u001b[0m     boundaries\u001b[38;5;241m=\u001b[39m[config\u001b[38;5;241m.\u001b[39mwarmup_steps]\n\u001b[1;32m     82\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# Initialize training components\n",
    "config = create_training_config()\n",
    "rng = jax.random.PRNGKey(42)\n",
    "\n",
    "# Create model and initialize training state\n",
    "print(\"Initializing model and training state...\")\n",
    "state = create_train_state_tpu(config, rng)\n",
    "train_step = create_train_step_tpu(config, state)\n",
    "\n",
    "# Initialize profiler\n",
    "profiler = TPUProfiler(config=config['model_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_metrics(profiler: TPUProfiler, num_steps: int = 100):\n",
    "    \"\"\"Analyze training metrics over multiple steps\"\"\"\n",
    "    metrics = {\n",
    "        'step_time': [],\n",
    "        'throughput': [],\n",
    "        'memory_used': [],\n",
    "        'tpu_utilization': []\n",
    "    }\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        profiler.start_step()\n",
    "        \n",
    "        # Simulate training step\n",
    "        batch_size = config['batch_size'] * config['grad_accum_steps']\n",
    "        profiler.record_batch_time(batch_size, 0.1)  # Example duration\n",
    "        profiler.measure_tpu_utilization()\n",
    "        \n",
    "        profiler.end_step()\n",
    "        \n",
    "        # Collect metrics\n",
    "        summary = profiler.get_metrics_summary()\n",
    "        metrics['step_time'].append(summary['step_time_mean'])\n",
    "        metrics['throughput'].append(summary.get('steps_per_second', 0))\n",
    "        metrics['memory_used'].append(summary.get('memory_accessed_mean', 0))\n",
    "        metrics['tpu_utilization'].append(summary.get('tpu_utilization_mean', 0))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run analysis\n",
    "training_metrics = analyze_training_metrics(profiler)\n",
    "\n",
    "# Plot metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training Performance Metrics')\n",
    "\n",
    "axes[0, 0].plot(training_metrics['step_time'])\n",
    "axes[0, 0].set_title('Step Time')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Time (s)')\n",
    "\n",
    "axes[0, 1].plot(training_metrics['throughput'])\n",
    "axes[0, 1].set_title('Throughput')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Steps/second')\n",
    "\n",
    "axes[1, 0].plot(training_metrics['memory_used'])\n",
    "axes[1, 0].set_title('Memory Usage')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Bytes')\n",
    "\n",
    "axes[1, 1].plot(training_metrics['tpu_utilization'])\n",
    "axes[1, 1].set_title('TPU Utilization')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Utilization %')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a84f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration with proper dtype settings\n",
    "config = create_training_config()\n",
    "\n",
    "# Initialize model and check params\n",
    "print(\"Creating model...\")\n",
    "model = create_vishwamai_transformer(config)\n",
    "\n",
    "# Initialize training components\n",
    "print(\"\\nInitializing training state...\")\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = create_train_state_tpu(config, rng)\n",
    "train_step = create_train_step_tpu(config, state)\n",
    "\n",
    "# Initialize profiler with proper config\n",
    "print(\"\\nSetting up profiler...\")\n",
    "profiler = TPUProfiler(config=config['model_config'])\n",
    "\n",
    "# Run analysis\n",
    "print(\"\\nAnalyzing training metrics...\")\n",
    "training_metrics = analyze_training_metrics(profiler)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training Performance Analysis')\n",
    "\n",
    "steps = range(len(training_metrics['step_time']))\n",
    "\n",
    "axes[0,0].plot(steps, training_metrics['step_time'])\n",
    "axes[0,0].set_title('Step Time')\n",
    "axes[0,0].set_xlabel('Step')\n",
    "axes[0,0].set_ylabel('Time (s)')\n",
    "\n",
    "axes[0,1].plot(steps, training_metrics['throughput'])\n",
    "axes[0,1].set_title('Training Throughput')\n",
    "axes[0,1].set_xlabel('Step')\n",
    "axes[0,1].set_ylabel('Steps/Second')\n",
    "\n",
    "axes[1,0].plot(steps, training_metrics['memory_used'])\n",
    "axes[1,0].set_title('Memory Usage')\n",
    "axes[1,0].set_xlabel('Step')\n",
    "axes[1,0].set_ylabel('Bytes')\n",
    "\n",
    "axes[1,1].plot(steps, training_metrics['tpu_utilization'])\n",
    "axes[1,1].set_title('TPU Utilization')\n",
    "axes[1,1].set_xlabel('Step')\n",
    "axes[1,1].set_ylabel('Utilization %')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52722ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance recommendations\n",
    "recommendations = profiler.get_performance_recommendations()\n",
    "print(\"\\nPerformance Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb875b",
   "metadata": {},
   "source": [
    "## Model Performance Analysis\n",
    "\n",
    "Current metrics from previous analysis:\n",
    "- Total Parameters: 109,529,088\n",
    "- MoE Parameters: 301,991,936\n",
    "- Memory Usage:\n",
    "  - Activations: 96.00 MB\n",
    "  - Attention: 3072.00 MB\n",
    "  - KV Cache: 192.00 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_model_performance(config: Dict[str, Any]):\n",
    "    \"\"\"Profile model inference performance\"\"\"\n",
    "    model = create_vishwamai_transformer(config)\n",
    "    batch_size = 1\n",
    "    seq_length = config['max_seq_len']\n",
    "    \n",
    "    # Create dummy input\n",
    "    dummy_input = jnp.ones((batch_size, seq_length), dtype=jnp.int32)\n",
    "    \n",
    "    # Profile memory\n",
    "    memory_profile = profiler.profile_memory_usage(\n",
    "        lambda x: model.apply({'params': state.params}, x),\n",
    "        {'input': dummy_input.shape}\n",
    "    )\n",
    "    \n",
    "    return memory_profile\n",
    "\n",
    "# Run performance profiling\n",
    "perf_metrics = profile_model_performance(config['model_config'])\n",
    "print(\"\\nModel Performance Profile:\")\n",
    "for k, v in perf_metrics.items():\n",
    "    print(f\"{k}: {v/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(batch_size: int = 32, seq_len: int = 512):\n",
    "    \"\"\"Benchmark different attention implementations\"\"\"\n",
    "    # Generate dummy inputs\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(rng, (batch_size, seq_len, config['model_config']['hidden_dim']))\n",
    "    \n",
    "    # Standard attention\n",
    "    def run_std_attention():\n",
    "        q = k = v = x\n",
    "        scores = jnp.einsum('bqd,bkd->bqk', q, k)\n",
    "        scores = scores / jnp.sqrt(config['model_config']['head_dim'])\n",
    "        attn = jax.nn.softmax(scores)\n",
    "        return jnp.einsum('bqk,bkd->bqd', attn, v)\n",
    "    \n",
    "    # Flash attention\n",
    "    def run_flash_attention():\n",
    "        q = k = v = x.reshape(batch_size, seq_len, \n",
    "                             config['model_config']['num_heads'], \n",
    "                             config['model_config']['head_dim'])\n",
    "        return FlashAttention(\n",
    "            num_heads=config['model_config']['num_heads'],\n",
    "            head_dim=config['model_config']['head_dim']\n",
    "        )(q, k, v)\n",
    "    \n",
    "    # Benchmark\n",
    "    std_time = %timeit -o -n 10 -r 3 -q run_std_attention()\n",
    "    flash_time = %timeit -o -n 10 -r 3 -q run_flash_attention()\n",
    "    \n",
    "    return {\n",
    "        'standard_attention_ms': std_time.best * 1000,\n",
    "        'flash_attention_ms': flash_time.best * 1000,\n",
    "        'speedup': std_time.best / flash_time.best\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks\n",
    "print(\"Running attention benchmarks...\")\n",
    "results_512 = benchmark_attention(seq_len=512)\n",
    "results_1024 = benchmark_attention(seq_len=1024)\n",
    "results_2048 = benchmark_attention(seq_len=2048)\n",
    "\n",
    "# Plot results\n",
    "seq_lens = [512, 1024, 2048]\n",
    "std_times = [results_512['standard_attention_ms'],\n",
    "            results_1024['standard_attention_ms'],\n",
    "            results_2048['standard_attention_ms']]\n",
    "flash_times = [results_512['flash_attention_ms'],\n",
    "              results_1024['flash_attention_ms'],\n",
    "              results_2048['flash_attention_ms']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lens, std_times, 'b-', label='Standard Attention')\n",
    "plt.plot(seq_lens, flash_times, 'r-', label='Flash Attention')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Attention Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSpeedup at different sequence lengths:\")\n",
    "print(f\"512: {results_512['speedup']:.2f}x\")\n",
    "print(f\"1024: {results_1024['speedup']:.2f}x\")\n",
    "print(f\"2048: {results_2048['speedup']:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
