{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Distillation Training\n",
    "\n",
    "This notebook demonstrates training VishwamAI through knowledge distillation from Google's Gemma-3b-it model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vishwamai import VishwamAI, VishwamAITokenizer, DistillationTrainer\n",
    "from vishwamai.training import TPUTrainingConfig\n",
    "import torch\n",
    "\n",
    "print(f\"Number of TPU devices: {jax.device_count()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Setting up a 1B parameter model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration for 1B parameters\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"vocab_size\": 131072,\n",
    "        \"hidden_dim\": 2048,  # Scaled for 1B params\n",
    "        \"num_layers\": 24,\n",
    "        \"num_heads\": 16,\n",
    "        \"head_dim\": 128,\n",
    "        \"mlp_dim\": 8192,\n",
    "        \"max_seq_len\": 2048,\n",
    "        \"dropout_rate\": 0.1\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"grad_accum_steps\": 4,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"warmup_steps\": 2000,\n",
    "        \"max_steps\": 100000,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"max_grad_norm\": 1.0\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"use_fp8\": True,\n",
    "        \"use_pjit\": True,\n",
    "        \"block_size\": 128,\n",
    "        \"mixed_precision\": True\n",
    "    },\n",
    "    \"distillation\": {\n",
    "        \"temperature\": 2.0,\n",
    "        \"alpha\": 0.5,\n",
    "        \"use_intermediate_distillation\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Teacher Model (Gemma-3b-it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Gemma model and tokenizer\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize student model\n",
    "student_model = VishwamAI.from_config(config)\n",
    "student_tokenizer = VishwamAITokenizer.from_pretrained(\"vishwamai/base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load a sample dataset (RedPajama)\n",
    "dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\")\n",
    "\n",
    "def prepare_data(examples):\n",
    "    # Tokenize input text\n",
    "    student_encodings = student_tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=config[\"model\"][\"max_seq_len\"],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "    \n",
    "    # Get teacher logits\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(\n",
    "            input_ids=torch.tensor(student_encodings[\"input_ids\"]).to(teacher_model.device)\n",
    "        )\n",
    "        teacher_logits = teacher_outputs.logits.cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": student_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": student_encodings[\"attention_mask\"],\n",
    "        \"teacher_logits\": teacher_logits\n",
    "    }\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_data,\n",
    "    batched=True,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "eval_dataset = dataset[\"validation\"].map(\n",
    "    prepare_data,\n",
    "    batched=True,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize distillation trainer\n",
    "training_config = TPUTrainingConfig(**config[\"training\"])\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    student_model=student_model,\n",
    "    teacher_logits_dim=teacher_model.config.vocab_size,\n",
    "    training_config=training_config,\n",
    "    temperature=config[\"distillation\"][\"temperature\"],\n",
    "    alpha=config[\"distillation\"][\"alpha\"]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = trainer.get_train_dataloader(train_dataset)\n",
    "eval_loader = trainer.get_eval_dataloader(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training with progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    # Training\n",
    "    trainer.train_epoch(train_loader)\n",
    "    \n",
    "    # Evaluation\n",
    "    metrics = trainer.evaluate(eval_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Eval loss: {metrics['eval_loss']:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    trainer.save_checkpoint(f\"checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare teacher and student model outputs\n",
    "def compare_models(prompt: str):\n",
    "    # Teacher model generation\n",
    "    teacher_inputs = teacher_tokenizer(prompt, return_tensors=\"pt\").to(teacher_model.device)\n",
    "    teacher_outputs = teacher_model.generate(\n",
    "        **teacher_inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    teacher_response = teacher_tokenizer.decode(teacher_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Student model generation\n",
    "    student_inputs = student_tokenizer(prompt, return_tensors=\"jax\")\n",
    "    student_outputs = student_model.generate(\n",
    "        student_inputs[\"input_ids\"],\n",
    "        max_length=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    student_response = student_tokenizer.decode(student_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"Teacher output:\")\n",
    "    print(teacher_response)\n",
    "    print(\"\\nStudent output:\")\n",
    "    print(student_response)\n",
    "\n",
    "# Test the models\n",
    "test_prompts = [\n",
    "    \"Explain how a computer processor works:\",\n",
    "    \"Write a short poem about artificial intelligence:\",\n",
    "    \"What are the key principles of machine learning?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    compare_models(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the trained student model\n",
    "output_dir = \"vishwamai_1b_distilled\"\n",
    "student_model.save_pretrained(output_dir)\n",
    "student_tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
