{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI: GSM8K Training Pipeline\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vishwamai/vishwamai/blob/main/train_gsm8k.ipynb)\n",
    "\n",
    "This notebook implements the training pipeline for VishwamAI on the GSM8K dataset.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q jax jaxlib\n",
    "!pip install -q flax optax\n",
    "!pip install -q datasets transformers huggingface_hub\n",
    "!pip install -q tqdm einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone VishwamAI repository\n",
    "!git clone https://github.com/vishwamai/vishwamai.git\n",
    "!cd vishwamai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfFolder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import VishwamAI modules\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.tokenizer import VishwamAITokenizer\n",
    "from vishwamai.training import create_train_state, train_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "Let's set up authentication for Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GSM8K Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load GSM8K dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample question:\")\n",
    "print(dataset['train'][0]['question'])\n",
    "print(\"\\nSample answer:\")\n",
    "print(dataset['train'][0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Format GSM8K example for training.\"\"\"\n",
    "    return {\n",
    "        'text': f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n",
    "    }\n",
    "\n",
    "# Format datasets\n",
    "train_dataset = dataset['train'].map(format_example)\n",
    "test_dataset = dataset['test'].map(format_example)\n",
    "\n",
    "print(\"Sample formatted example:\")\n",
    "print(train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model configuration\n",
    "config_path = \"vishwamai/configs/config_10B.json\"\n",
    "with open(config_path) as f:\n",
    "    config = ModelConfig(**json.load(f))\n",
    "\n",
    "# Initialize model\n",
    "model = VishwamAIModel(config)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = VishwamAITokenizer.from_pretrained(\"gpt2\")  # Base tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_data_loader(dataset, tokenizer, batch_size):\n",
    "    \"\"\"Create a data loader for training.\"\"\"\n",
    "    def tokenize(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=config.max_seq_len,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "    \n",
    "    tokenized = dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized.with_format('numpy').iter(batch_size=batch_size)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader(train_dataset, tokenizer, batch_size)\n",
    "test_loader = create_data_loader(test_dataset, tokenizer, batch_size)\n",
    "\n",
    "# Initialize training state\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = create_train_state(model, config, learning_rate, rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"checkpoints\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    rng, epoch_rng = jax.random.split(rng)\n",
    "    state, metrics = train_epoch(\n",
    "        state=state,\n",
    "        train_loader=train_loader,\n",
    "        rng=epoch_rng,\n",
    "        error_correction=None,  # No error correction for initial training\n",
    "        epoch=epoch + 1\n",
    "    )\n",
    "    \n",
    "    print(f\"Train - Loss: {metrics['loss']:.4f}, Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint_dir = output_dir / f\"checkpoint-{epoch+1}\"\n",
    "        model.save_pretrained(checkpoint_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Push final model to hub\n",
    "model.push_to_hub(\"VishwamAI/VishwamAI\", commit_message=\"Trained on GSM8K\")\n",
    "tokenizer.push_to_hub(\"VishwamAI/VishwamAI\", commit_message=\"Updated tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "        \n",
    "        logits = outputs['logits']\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        correct = (predictions == batch['labels']) * batch['attention_mask']\n",
    "        total_correct += jnp.sum(correct)\n",
    "        total_samples += jnp.sum(batch['attention_mask'])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(logits, batch['labels'], batch['attention_mask'])\n",
    "        total_loss += loss * jnp.sum(batch['attention_mask'])\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / total_samples,\n",
    "        'accuracy': total_correct / total_samples\n",
    "    }\n",
    "\n",
    "# Evaluate final model\n",
    "metrics = evaluate_model(model, test_loader)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VishwamAI_GSM8K_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
