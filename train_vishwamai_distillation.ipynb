{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and setup repository\n",
    "!git clone https://github.com/VishwamAI/VishwamAI\n",
    "%cd VishwamAI\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q --upgrade transformers datasets accelerate bitsandbytes sentencepiece \\\n",
    "    flax optax omegaconf huggingface-hub einops aim>=3.17.5 safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "import gc\n",
    "\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.tokenizer import VishwamAITokenizer\n",
    "from vishwamai.distillation import VishwamaiGuruKnowledge, VishwamaiShaalaTrainer\n",
    "from vishwamai.data_utils import create_train_dataloader, create_val_dataloader\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure QwQ-32B teacher model\n",
    "teacher_config = ModelConfig(\n",
    "    vocab_size=151936,\n",
    "    hidden_size=7168,\n",
    "    num_layers=60,\n",
    "    num_attention_heads=56,\n",
    "    intermediate_size=28672,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_dropout_prob=0.1,\n",
    "    max_position_embeddings=2048,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-5,\n",
    "    use_cache=True,\n",
    "    use_flash_attention=True,\n",
    "    use_rope=True,\n",
    "    use_gqa=True,\n",
    "    num_key_value_heads=8,\n",
    "    dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Configure smaller student model\n",
    "student_config = ModelConfig(\n",
    "    vocab_size=151936,\n",
    "    hidden_size=2048,\n",
    "    num_layers=24,\n",
    "    num_attention_heads=32,\n",
    "    intermediate_size=8192,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_dropout_prob=0.1,\n",
    "    max_position_embeddings=2048,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-5,\n",
    "    use_cache=True,\n",
    "    use_flash_attention=True,\n",
    "    use_rope=True,\n",
    "    use_gqa=True,\n",
    "    num_key_value_heads=8,\n",
    "    dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and initialize QwQ-32B teacher model\n",
    "def download_model(model_path: str, num_shards: int = 5):\n",
    "    patterns = [f\"model-{i+1:05d}-of-00252.safetensors\" for i in range(num_shards)]\n",
    "    patterns.extend([\"config.json\", \"tokenizer.model\"])\n",
    "    return snapshot_download(\n",
    "        repo_id=model_path,\n",
    "        allow_patterns=patterns,\n",
    "        local_files_only=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "\n",
    "teacher_path = download_model(\"Qwen/QwQ-32B\", num_shards=5)\n",
    "print(f\"Downloaded teacher model to {teacher_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models, tokenizer and trainer\n",
    "teacher_model = VishwamAIModel(teacher_config)\n",
    "teacher_model.load_weights(teacher_path, reduced_size=True)\n",
    "\n",
    "student_model = VishwamAIModel(student_config)\n",
    "\n",
    "tokenizer = VishwamAITokenizer(\n",
    "    vocab_size=teacher_config.vocab_size,\n",
    "    model_prefix=\"vishwamai\"\n",
    ")\n",
    "\n",
    "# Load distillation config\n",
    "distillation_config = OmegaConf.load('configs/distillation_config.yaml')\n",
    "\n",
    "trainer = VishwamaiShaalaTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    cfg=distillation_config\n",
    ")\n",
    "\n",
    "print(\"Initialized models and trainer successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = create_train_dataloader(distillation_config)\n",
    "val_loader = create_val_dataloader(distillation_config)\n",
    "\n",
    "# Initialize training state\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = trainer.create_train_state(rng)\n",
    "\n",
    "print(\"Created data loaders and training state!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    with tqdm(total=steps_per_epoch) as pbar:\n",
    "        for step in range(steps_per_epoch):\n",
    "            # Get batch and teacher predictions\n",
    "            batch = next(train_loader)\n",
    "            rng, train_rng = jax.random.split(rng)\n",
    "            \n",
    "            # Training step\n",
    "            outputs, state = trainer.train_step(state, batch, train_rng)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{outputs['loss']:.4f}\",\n",
    "                'kd_loss': f\"{outputs['metrics']['kd_loss']:.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Clear memory\n",
    "            if step % 10 == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = trainer.eval_step(state, val_loader)\n",
    "        print(f\"Validation metrics: {val_metrics}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            trainer.save_checkpoint(f\"checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save distilled model to Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "def save_to_hub(model, tokenizer, config, repo_id, token=None):\n",
    "    \"\"\"Save distilled model to Hugging Face Hub\"\"\"\n",
    "    api = HfApi(token=token)\n",
    "    \n",
    "    # Create repo if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id, private=False, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repo: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Save model files locally first\n",
    "    tmp_dir = \"distilled_model\"\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save config\n",
    "    config_dict = {\n",
    "        \"model_type\": \"VishwamAI\",\n",
    "        \"architectures\": [\"VishwamAIModel\"],\n",
    "        **config.__dict__\n",
    "    }\n",
    "    with open(f\"{tmp_dir}/config.json\", 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save(tmp_dir)\n",
    "    \n",
    "    # Save model weights in safetensors format\n",
    "    weights_file = f\"{tmp_dir}/model.safetensors\"\n",
    "    save_params = {k: v for k, v in state.params.items()}\n",
    "    stf.save_file(save_params, weights_file)\n",
    "    \n",
    "    # Upload to Hub\n",
    "    api.upload_folder(\n",
    "        folder_path=tmp_dir,\n",
    "        repo_id=repo_id,\n",
    "        commit_message=\"Upload distilled model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully uploaded model to {repo_id}\")\n",
    "\n",
    "# Save model after training\n",
    "repo_id = \"VishwamAI/qwq-32b-distilled\"  # Change this to your desired repo name\n",
    "hf_token = \"your_huggingface_token\"  # Add your HF token here\n",
    "\n",
    "save_to_hub(\n",
    "    model=student_model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=student_config,\n",
    "    repo_id=repo_id,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model card\n",
    "model_card = \"\"\"\n",
    "---\n",
    "language:\n",
    "- en\n",
    "tags:\n",
    "- distillation\n",
    "- qwq-32b\n",
    "- vishwamai\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# QwQ-32B Distilled Model\n",
    "\n",
    "This is a distilled version of the QwQ-32B model. The model has been trained using knowledge distillation to compress the knowledge from the larger teacher model into a smaller, more efficient student model.\n",
    "\n",
    "## Model Details\n",
    "- Teacher Model: QwQ-32B\n",
    "- Student Architecture: {student_arch}\n",
    "- Vocabulary Size: {vocab_size}\n",
    "- Training Data: Various high-quality datasets\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{repo_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{repo_id}\")\n",
    "```\n",
    "\"\"\".format(\n",
    "    student_arch=f\"Hidden Size: {student_config.hidden_size}, Layers: {student_config.num_layers}\",\n",
    "    vocab_size=student_config.vocab_size,\n",
    "    repo_id=repo_id\n",
    ")\n",
    "\n",
    "with open(f\"{tmp_dir}/README.md\", 'w') as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Upload model card\n",
    "api.upload_file(\n",
    "    path_or_fileobj=f\"{tmp_dir}/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    commit_message=\"Add model card\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model to HuggingFace Hub\n",
    "from huggingface_hub import HfApi\n",
    "import safetensors.flax as stf\n",
    "\n",
    "def push_to_hub(model_name: str = \"VishwamAI/VishwamAI\", token: str = None):\n",
    "    \"\"\"Push distilled model to HuggingFace Hub\"\"\"\n",
    "    api = HfApi(token=token)\n",
    "    \n",
    "    # Create local files\n",
    "    tmp_dir = \"distilled_model\"\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model configuration\n",
    "    student_config.save_pretrained(tmp_dir)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(tmp_dir)\n",
    "    \n",
    "    # Save model weights using safetensors\n",
    "    model_path = f\"{tmp_dir}/model.safetensors\"\n",
    "    params = student_model.params\n",
    "    stf.save_file(params, model_path)\n",
    "    \n",
    "    # Create model card\n",
    "    model_card = f\"\"\"\n",
    "    ---\n",
    "    language:\n",
    "    - en\n",
    "    tags:\n",
    "    - vishwamai\n",
    "    - distillation\n",
    "    - qwq-32b\n",
    "    license: apache-2.0\n",
    "    ---\n",
    "    # VishwamAI Distilled Model\n",
    "    \n",
    "    This is a distilled version of QwQ-32B model using VishwamAI's distillation pipeline.\n",
    "    \n",
    "    ## Model Details\n",
    "    - Teacher: QwQ-32B ({teacher_config.hidden_size} hidden size)\n",
    "    - Student: VishwamAI ({student_config.hidden_size} hidden size)\n",
    "    - Vocab Size: {student_config.vocab_size}\n",
    "    - Context Length: {student_config.max_position_embeddings}\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{tmp_dir}/README.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    # Push to hub\n",
    "    api.create_repo(model_name, exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=tmp_dir,\n",
    "        repo_id=model_name,\n",
    "        commit_message=\"Upload distilled model\"\n",
    "    )\n",
    "    print(f\"Model successfully pushed to {model_name}\")\n",
    "\n",
    "# Push the model (use your HF token)\n",
    "push_to_hub(token=\"your_huggingface_token_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the uploaded model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the model from Hub\n",
    "model = AutoModelForCausalLM.from_pretrained(\"VishwamAI/VishwamAI\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"VishwamAI/VishwamAI\")\n",
    "\n",
    "# Test inference\n",
    "text = \"Hello, my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
