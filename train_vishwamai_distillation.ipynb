{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI QwQ-32B Distillation Training\n",
    "\n",
    "This notebook implements the distillation of QwQ-32B into a smaller model.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/VishwamAI/VishwamAI/blob/main/train_vishwamai_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VishwamAI/VishwamAI\n",
    "%cd VishwamAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q --upgrade transformers datasets accelerate bitsandbytes sentencepiece \\\n",
    "    flax optax omegaconf huggingface-hub einops aim>=3.17.5 safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.training import train_state\n",
    "import gc\n",
    "import jax\n",
    "\n",
    "# Now these imports should work\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "import safetensors\n",
    "from vishwamai.tokenizer import VishwamAITokenizer\n",
    "from vishwamai.distillation import VishwamaiGuruKnowledge, VishwamaiShaalaTrainer\n",
    "from vishwamai.data_utils import create_train_dataloader, create_val_dataloader\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = OmegaConf.load('configs/distillation_config.yaml')\n",
    "\n",
    "# Filter out unsupported config parameters\n",
    "teacher_model_config = {\n",
    "    k: v for k, v in config.distillation.teacher_model.config.items()\n",
    "    if k not in ['rope_scaling', 'seq_length']  # Remove unsupported params\n",
    "}\n",
    "\n",
    "# Initialize teacher and student configs\n",
    "teacher_config = ModelConfig(**teacher_model_config)\n",
    "student_config = ModelConfig(**config.distillation.student_model)\n",
    "\n",
    "print(\"Teacher config:\\n\", OmegaConf.to_yaml(teacher_config))\n",
    "print(\"\\nStudent config:\\n\", OmegaConf.to_yaml(student_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download QwQ-32B model shards\n",
    "def download_model_shards(model_path: str, num_shards: int = 5):\n",
    "    \"\"\"Download model shards efficiently.\"\"\"\n",
    "    patterns = [f\"model-{i+1:05d}-of-00252.safetensors\" for i in range(num_shards)]\n",
    "    patterns.extend([\"config.json\", \"tokenizer.model\"])\n",
    "    \n",
    "    return snapshot_download(\n",
    "        repo_id=model_path,\n",
    "        allow_patterns=patterns,\n",
    "        local_files_only=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "\n",
    "teacher_path = download_model_shards(\n",
    "    config.distillation.teacher_model.path,\n",
    "    num_shards=5  # Using 5 shards for memory efficiency\n",
    ")\n",
    "print(f\"Downloaded QwQ-32B model to {teacher_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and tokenizer\n",
    "teacher_model = VishwamAIModel(teacher_config)\n",
    "student_model = VishwamAIModel(student_config)\n",
    "\n",
    "print(\"Loading teacher weights...\")\n",
    "teacher_model.load_weights(teacher_path, reduced_size=True)\n",
    "\n",
    "print(\"\\nInitializing tokenizer...\")\n",
    "tokenizer = VishwamAITokenizer(\n",
    "    vocab_size=teacher_config.vocab_size,\n",
    "    model_prefix=\"vishwamai\"\n",
    ")\n",
    "\n",
    "print(\"Models and tokenizer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distillation trainer\n",
    "trainer = VishwamaiShaalaTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "# Create training state\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = trainer.create_train_state(rng)\n",
    "\n",
    "print(\"Trainer and training state initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "from vishwamai.data_utils import create_train_dataloader, create_val_dataloader\n",
    "\n",
    "print(\"Creating data loaders...\")\n",
    "train_loader = create_train_dataloader(config)\n",
    "val_loader = create_val_dataloader(config)\n",
    "print(\"Data loaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "num_epochs = 10\n",
    "steps_per_epoch = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    with tqdm(total=steps_per_epoch) as pbar:\n",
    "        for step in range(steps_per_epoch):\n",
    "            # Get batch and train\n",
    "            batch = next(train_loader)\n",
    "            rng, train_rng = jax.random.split(rng)\n",
    "            \n",
    "            # Training step\n",
    "            outputs, state = trainer.train_step(state, batch, train_rng)\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{outputs['loss']:.4f}\",\n",
    "                'kd_loss': f\"{outputs['metrics']['kd_loss']:.4f}\",\n",
    "                'correction_rate': f\"{outputs['metrics'].get('error_correction_rate', 0.0):.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Memory management\n",
    "            if step % 10 == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = trainer.eval_step(state, val_loader)\n",
    "        print(f\"\\nValidation metrics: {val_metrics}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            trainer.save_checkpoint(f\"checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_save_path = \"final_distilled_model\"\n",
    "student_model.save_pretrained(final_save_path)\n",
    "tokenizer.save(final_save_path)\n",
    "\n",
    "print(f\"Saved distilled model to {final_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
