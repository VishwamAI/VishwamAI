{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI QwQ-32B Distillation\n",
    "\n",
    "Memory-efficient distillation from [Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) with chunked loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from omegaconf import OmegaConf\n",
    "import aim\n",
    "import gc\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.qwen_distiller import QwenDistillationTrainer\n",
    "from vishwamai.qwen_data import QwenDataLoader\n",
    "\n",
    "# Clear any existing cache\n",
    "jax.clear_caches()\n",
    "gc.collect()\n",
    "\n",
    "# Print device info\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Number of devices: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient settings\n",
    "CHUNK_SIZE = 32  # Size of chunks for loading\n",
    "BATCH_SIZE = 1   # Per device batch size\n",
    "GRAD_ACCUM_STEPS = 16  # Gradient accumulation steps\n",
    "\n",
    "# Load and update configuration\n",
    "config = OmegaConf.load('configs/distillation_config.yaml')\n",
    "config.training.batch_size = BATCH_SIZE\n",
    "config.training.gradient_accumulation_steps = GRAD_ACCUM_STEPS\n",
    "\n",
    "print(f\"Configuration loaded with:\")\n",
    "print(f\"- Chunk size: {CHUNK_SIZE}\")\n",
    "print(f\"- Batch size per device: {BATCH_SIZE}\")\n",
    "print(f\"- Gradient accumulation steps: {GRAD_ACCUM_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download QwQ model with progress tracking\n",
    "print(\"Downloading QwQ-32B model files...\")\n",
    "qwq_path = snapshot_download(\n",
    "    \"Qwen/QwQ-32B\",\n",
    "    allow_patterns=[\"*.safetensors\", \"config.json\", \"tokenizer.model\"],\n",
    "    local_files_only=False,\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "# Verify shard count\n",
    "shard_files = [f for f in os.listdir(qwq_path) if f.endswith('.safetensors')]\n",
    "print(f\"Found {len(shard_files)} safetensor shards\")\n",
    "assert len(shard_files) == 14, f\"Expected 14 safetensor files, found {len(shard_files)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader with memory-efficient settings\n",
    "loader = QwenDataLoader(\n",
    "    safetensor_dir=qwq_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_sequence_length=config.distillation.teacher_model.config.max_position_embeddings,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    chunk_size=CHUNK_SIZE\n",
    ")\n",
    "\n",
    "print(\"Data loader initialized with chunked loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "teacher_model = VishwamAIModel(ModelConfig(**config.distillation.teacher_model.config))\n",
    "student_model = VishwamAIModel(ModelConfig(**config.distillation.student_model.config))\n",
    "\n",
    "print(\"\\nLoading QwQ model weights in chunks...\")\n",
    "params = loader.load_all_shards()  # This now uses chunked loading\n",
    "teacher_model = teacher_model.bind({'params': params})\n",
    "\n",
    "# Clear memory after loading\n",
    "jax.clear_caches()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Models initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with experiment tracking\n",
    "aim_run = aim.Run(\n",
    "    experiment=\"VishwamAI-QwQ-Distillation\",\n",
    "    log_system_params=True\n",
    ")\n",
    "aim_run.set_params({\n",
    "    \"teacher_model\": \"QwQ-32B\",\n",
    "    \"student_model\": \"VishwamAI-7B\",\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    \"batch_size_per_device\": BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": GRAD_ACCUM_STEPS,\n",
    "    **OmegaConf.to_container(config, resolve=True)\n",
    "})\n",
    "\n",
    "trainer = QwenDistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = trainer.create_train_state(rng)\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with memory management\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    for epoch in range(5):  # 5 epochs\n",
    "        print(f\"\\nEpoch {epoch + 1}/5\")\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Process shards sequentially with memory cleanup\n",
    "        for shard_name, shard_params in tqdm(loader.get_shard_stream(), desc=\"Processing QwQ shards\"):\n",
    "            # Accumulate gradients over multiple steps\n",
    "            accumulated_gradients = None\n",
    "            \n",
    "            for accum_step in range(GRAD_ACCUM_STEPS):\n",
    "                # Create batch for current accumulation step\n",
    "                batch = loader.create_training_batch(\n",
    "                    input_ids=shard_params['input_ids'],\n",
    "                    labels=shard_params.get('labels')\n",
    "                )\n",
    "                \n",
    "                # Training step\n",
    "                state, metrics, grads = trainer.train_step_with_grads(\n",
    "                    state=state,\n",
    "                    batch=batch,\n",
    "                    rng=rng\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                if accumulated_gradients is None:\n",
    "                    accumulated_gradients = grads\n",
    "                else:\n",
    "                    accumulated_gradients = jax.tree_map(\n",
    "                        lambda x, y: x + y,\n",
    "                        accumulated_gradients,\n",
    "                        grads\n",
    "                    )\n",
    "                \n",
    "                # Clear intermediate memory\n",
    "                if accum_step % 4 == 0:\n",
    "                    jax.clear_caches()\n",
    "                    gc.collect()\n",
    "            \n",
    "            # Apply accumulated gradients\n",
    "            accumulated_gradients = jax.tree_map(\n",
    "                lambda x: x / GRAD_ACCUM_STEPS,\n",
    "                accumulated_gradients\n",
    "            )\n",
    "            state = state.apply_gradients(grads=accumulated_gradients)\n",
    "            \n",
    "            # Log metrics\n",
    "            if state.step % config.training.logging_steps == 0:\n",
    "                aim_run.track(\n",
    "                    metrics,\n",
    "                    step=state.step,\n",
    "                    context={\n",
    "                        'shard': shard_name,\n",
    "                        'epoch': epoch,\n",
    "                        'memory_usage': jax.numpy.zeros(1).device_buffer.physical_memory_allocated()\n",
    "                    }\n",
    "                )\n",
    "                print(f\"Step {state.step}: loss={metrics['loss']:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if state.step % config.training.save_steps == 0:\n",
    "                ckpt_path = f\"checkpoints/step_{state.step}\"\n",
    "                trainer.save_checkpoint(\n",
    "                    state=state,\n",
    "                    path=ckpt_path,\n",
    "                    extra_info={\n",
    "                        'epoch': epoch,\n",
    "                        'shard': shard_name,\n",
    "                        'metrics': metrics\n",
    "                    }\n",
    "                )\n",
    "                aim_run.track_artifact(ckpt_path, name=\"checkpoints\")\n",
    "            \n",
    "            # Memory cleanup after each shard\n",
    "            del accumulated_gradients\n",
    "            jax.clear_caches()\n",
    "            gc.collect()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch completed in {epoch_time:.2f}s\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted, saving checkpoint...\")\n",
    "    trainer.save_checkpoint(state, \"checkpoints/interrupted\")\n",
    "finally:\n",
    "    aim_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model with memory usage stats\n",
    "final_path = \"final_vishwamai_model\"\n",
    "\n",
    "# Clear memory before saving\n",
    "jax.clear_caches()\n",
    "gc.collect()\n",
    "\n",
    "trainer.save_model(\n",
    "    state=state,\n",
    "    path=final_path,\n",
    "    config_override={\n",
    "        \"parent_model\": \"Qwen/QwQ-32B\",\n",
    "        \"distillation_version\": \"v1.0\",\n",
    "        \"training_config\": {\n",
    "            \"chunk_size\": CHUNK_SIZE,\n",
    "            \"batch_size_per_device\": BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": GRAD_ACCUM_STEPS,\n",
    "            \"peak_memory_usage\": float(jax.numpy.zeros(1).device_buffer.physical_memory_allocated())\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Distilled model saved to {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
