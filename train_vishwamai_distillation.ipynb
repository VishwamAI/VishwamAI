{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishwamAI/VishwamAI/blob/main/train_vishwamai_distillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErS4-686qWr5"
      },
      "source": [
        "# VishwamAI QwQ-32B Distillation\n",
        "\n",
        "Memory-efficient distillation from [Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) with chunked loading."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
        "%cd VishwamAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hig7BwJqd-J",
        "outputId": "e032c115-5d46-4fd7-bec2-b0d70941d36e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VishwamAI'...\n",
            "remote: Enumerating objects: 2575, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 2575 (delta 35), reused 54 (delta 23), pack-reused 2490 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2575/2575), 35.46 MiB | 43.86 MiB/s, done.\n",
            "Resolving deltas: 100% (1289/1289), done.\n",
            "/content/VishwamAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q --upgrade transformers datasets accelerate bitsandbytes sentencepiece \\\n",
        "    flax optax omegaconf huggingface-hub einops aim>=3.17.5 safetensors"
      ],
      "metadata": {
        "id": "GrfVwMoTqoIe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYHc--GOqWr8",
        "outputId": "d28be9a0-af46-4e26-b7b0-7cf10b133950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
            "Number of devices: 8\n",
            "Initial memory usage: 1.60GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from omegaconf import OmegaConf\n",
        "import aim\n",
        "import gc\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "from vishwamai.model import VishwamAIModel, ModelConfig\n",
        "from vishwamai.qwen_distiller import QwenDistillationTrainer  # Updated import\n",
        "from vishwamai.qwen_data import QwenDataLoader\n",
        "from vishwamai.tensor_utils import get_memory_usage\n",
        "\n",
        "# Clear any existing cache\n",
        "jax.clear_caches()\n",
        "gc.collect()\n",
        "\n",
        "# Print device info\n",
        "print(f\"JAX devices: {jax.devices()}\")\n",
        "print(f\"Number of devices: {jax.device_count()}\")\n",
        "print(f\"Initial memory usage: {get_memory_usage():.2f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtDWiUAqWr9",
        "outputId": "fa298d53-9ff3-43ec-ac8a-a21e1824fe96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded with:\n",
            "- Chunk size: 32\n",
            "- Batch size per device: 1\n",
            "- Gradient accumulation steps: 16\n"
          ]
        }
      ],
      "source": [
        "# Memory-efficient settings\n",
        "CHUNK_SIZE = 32  # Size of chunks for loading\n",
        "BATCH_SIZE = 1   # Per device batch size\n",
        "GRAD_ACCUM_STEPS = 16  # Gradient accumulation steps\n",
        "\n",
        "# Load and update configuration\n",
        "config = OmegaConf.load('configs/distillation_config.yaml')\n",
        "config.training.batch_size = BATCH_SIZE\n",
        "config.training.gradient_accumulation_steps = GRAD_ACCUM_STEPS\n",
        "config.memory_optimization.chunk_size = CHUNK_SIZE\n",
        "\n",
        "print(f\"Configuration loaded with:\")\n",
        "print(f\"- Chunk size: {CHUNK_SIZE}\")\n",
        "print(f\"- Batch size per device: {BATCH_SIZE}\")\n",
        "print(f\"- Gradient accumulation steps: {GRAD_ACCUM_STEPS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "adc0885afd374d1eaf2e91475747a636",
            "c968ed62e1924b5ba9547cc071a44b28",
            "f7900ccb6ed7474a8b58e45d00fdec8c",
            "f4baccc271be4807b82c0cba0cd1c694",
            "7b2d56ec4e144f2a98070de5dcd8094b",
            "b1869a8004954ff387798acad7a88f81",
            "ded233f45e8149dcb36100eaa644953b",
            "812a866c754a4c4d8beb55f11b96b510",
            "d83395e9bed54b6f97f916d863a3a45f",
            "029688b9313140a997ee149ceb0644da",
            "71a406255fec48ec99fabbe2b95a5cfb"
          ]
        },
        "id": "X3P5Ut-VqWr-",
        "outputId": "98163d9b-18d6-44d7-935a-931f3541b00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading QwQ-32B model files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adc0885afd374d1eaf2e91475747a636"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14 safetensor shards\n"
          ]
        }
      ],
      "source": [
        "# Download QwQ model with progress tracking\n",
        "print(\"Downloading QwQ-32B model files...\")\n",
        "qwq_path = snapshot_download(\n",
        "    \"Qwen/QwQ-32B\",\n",
        "    allow_patterns=[\"*.safetensors\", \"config.json\", \"tokenizer.model\"],\n",
        "    local_files_only=False,\n",
        "    resume_download=True\n",
        ")\n",
        "\n",
        "# Verify shard count\n",
        "shard_files = [f for f in os.listdir(qwq_path) if f.endswith('.safetensors')]\n",
        "print(f\"Found {len(shard_files)} safetensor shards\")\n",
        "assert len(shard_files) == 14, f\"Expected 14 safetensor files, found {len(shard_files)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB7r8ffPqWr-",
        "outputId": "9e309f42-8169-4905-c2db-f0aa1140083c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using provided chunk size: 32\n",
            "Initialized loader with:\n",
            " - 8 devices\n",
            " - Global batch size: 16\n",
            " - Per-device batch size: 2\n",
            " - Gradient accumulation steps: 16\n",
            " - Chunk size: 32\n",
            " - Initial memory usage: 1.61GB\n",
            "Data loader initialized with chunked loading\n",
            "Memory usage: 1.61GB\n"
          ]
        }
      ],
      "source": [
        "# Initialize data loader with memory-efficient settings\n",
        "loader = QwenDataLoader(\n",
        "    safetensor_dir=qwq_path,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_sequence_length=config.distillation.teacher_model.config.max_position_embeddings,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    chunk_size=CHUNK_SIZE\n",
        ")\n",
        "\n",
        "print(\"Data loader initialized with chunked loading\")\n",
        "print(f\"Memory usage: {get_memory_usage():.2f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfsBlxZNqWr-",
        "outputId": "2a57d8f8-447b-493a-b9ff-64c7d6203969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing models...\n",
            "Memory before: 1.61GB\n",
            "\n",
            "Loading QwQ model weights in chunks...\n",
            "Loading shard model-00001-of-00014.safetensors...\n",
            "Memory before: 1.61GB\n"
          ]
        }
      ],
      "source": [
        "# Initialize models with memory tracking\n",
        "print(\"Initializing models...\")\n",
        "print(f\"Memory before: {get_memory_usage():.2f}GB\")\n",
        "\n",
        "teacher_model = VishwamAIModel(ModelConfig(**config.distillation.teacher_model.config))\n",
        "student_model = VishwamAIModel(ModelConfig(**config.distillation.student_model.config))\n",
        "\n",
        "print(\"\\nLoading QwQ model weights in chunks...\")\n",
        "params = loader.load_all_shards()  # This now uses chunked loading\n",
        "teacher_model = teacher_model.bind({'params': params})\n",
        "\n",
        "# Clear memory after loading\n",
        "jax.clear_caches()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Memory after: {get_memory_usage():.2f}GB\")\n",
        "print(\"Models initialized successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrahxMcOqWr_"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer with experiment tracking\n",
        "aim_run = aim.Run(\n",
        "    experiment=config.monitoring.aim_experiment,\n",
        "    log_system_params=True\n",
        ")\n",
        "aim_run.set_params({\n",
        "    \"teacher_model\": \"QwQ-32B\",\n",
        "    \"student_model\": \"VishwamAI-7B\",\n",
        "    \"chunk_size\": CHUNK_SIZE,\n",
        "    \"batch_size_per_device\": BATCH_SIZE,\n",
        "    \"gradient_accumulation_steps\": GRAD_ACCUM_STEPS,\n",
        "    \"memory_initial\": get_memory_usage(),\n",
        "    **OmegaConf.to_container(config, resolve=True)\n",
        "})\n",
        "\n",
        "trainer = QwenDistillationTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    student_model=student_model,\n",
        "    cfg=config\n",
        ")\n",
        "\n",
        "rng = jax.random.PRNGKey(42)\n",
        "state = trainer.create_train_state(rng)\n",
        "print(\"Training setup complete\")\n",
        "print(f\"Memory usage: {get_memory_usage():.2f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOMV9oGXqWsA"
      },
      "outputs": [],
      "source": [
        "# Training loop with memory management\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "try:\n",
        "    for epoch in range(5):  # 5 epochs\n",
        "        print(f\"\\nEpoch {epoch + 1}/5\")\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Process shards sequentially with memory cleanup\n",
        "        for shard_name, shard_params in tqdm(loader.get_shard_stream(), desc=\"Processing QwQ shards\"):\n",
        "            # Memory check\n",
        "            current_mem = get_memory_usage()\n",
        "            if current_mem > config.memory_optimization.max_memory_gb:\n",
        "                print(f\"Warning: High memory usage ({current_mem:.2f}GB)\")\n",
        "                jax.clear_caches()\n",
        "                gc.collect()\n",
        "\n",
        "            # Accumulate gradients over multiple steps\n",
        "            accumulated_gradients = None\n",
        "\n",
        "            for accum_step in range(GRAD_ACCUM_STEPS):\n",
        "                # Create batch for current accumulation step\n",
        "                batch = loader.create_training_batch(\n",
        "                    input_ids=shard_params['input_ids'],\n",
        "                    labels=shard_params.get('labels')\n",
        "                )\n",
        "\n",
        "                # Training step\n",
        "                state, metrics, grads = trainer.train_step_with_grads(\n",
        "                    state=state,\n",
        "                    batch=batch,\n",
        "                    rng=rng\n",
        "                )\n",
        "\n",
        "                # Accumulate gradients\n",
        "                if accumulated_gradients is None:\n",
        "                    accumulated_gradients = grads\n",
        "                else:\n",
        "                    accumulated_gradients = jax.tree_map(\n",
        "                        lambda x, y: x + y,\n",
        "                        accumulated_gradients,\n",
        "                        grads\n",
        "                    )\n",
        "\n",
        "                # Clear intermediate memory\n",
        "                if accum_step % 4 == 0:\n",
        "                    jax.clear_caches()\n",
        "                    gc.collect()\n",
        "\n",
        "            # Apply accumulated gradients\n",
        "            accumulated_gradients = jax.tree_map(\n",
        "                lambda x: x / GRAD_ACCUM_STEPS,\n",
        "                accumulated_gradients\n",
        "            )\n",
        "            state = state.apply_gradients(grads=accumulated_gradients)\n",
        "\n",
        "            # Log metrics with memory usage\n",
        "            if state.step % config.training.logging_steps == 0:\n",
        "                current_mem = get_memory_usage()\n",
        "                metrics['memory_usage'] = current_mem\n",
        "                aim_run.track(\n",
        "                    metrics,\n",
        "                    step=state.step,\n",
        "                    context={\n",
        "                        'shard': shard_name,\n",
        "                        'epoch': epoch,\n",
        "                        'memory_gb': current_mem\n",
        "                    }\n",
        "                )\n",
        "                print(f\"Step {state.step}: loss={metrics['loss']:.4f}, memory={current_mem:.2f}GB\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            if state.step % config.training.save_steps == 0:\n",
        "                ckpt_path = f\"checkpoints/step_{state.step}\"\n",
        "                trainer.save_checkpoint(\n",
        "                    state=state,\n",
        "                    path=ckpt_path,\n",
        "                    extra_info={\n",
        "                        'epoch': epoch,\n",
        "                        'shard': shard_name,\n",
        "                        'metrics': metrics,\n",
        "                        'memory_usage': get_memory_usage()\n",
        "                    }\n",
        "                )\n",
        "                aim_run.track_artifact(ckpt_path, name=\"checkpoints\")\n",
        "\n",
        "            # Memory cleanup after each shard\n",
        "            del accumulated_gradients\n",
        "            del shard_params\n",
        "            jax.clear_caches()\n",
        "            gc.collect()\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"Epoch completed in {epoch_time:.2f}s\")\n",
        "        print(f\"Memory usage: {get_memory_usage():.2f}GB\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted, saving checkpoint...\")\n",
        "    trainer.save_checkpoint(state, \"checkpoints/interrupted\")\n",
        "finally:\n",
        "    aim_run.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3-LU5OeqWsB"
      },
      "outputs": [],
      "source": [
        "# Save final model with memory usage stats\n",
        "final_path = \"final_vishwamai_model\"\n",
        "\n",
        "# Clear memory before saving\n",
        "jax.clear_caches()\n",
        "gc.collect()\n",
        "\n",
        "trainer.save_model(\n",
        "    state=state,\n",
        "    path=final_path,\n",
        "    config_override={\n",
        "        \"parent_model\": \"Qwen/QwQ-32B\",\n",
        "        \"distillation_version\": \"v1.0\",\n",
        "        \"training_config\": {\n",
        "            \"chunk_size\": CHUNK_SIZE,\n",
        "            \"batch_size_per_device\": BATCH_SIZE,\n",
        "            \"gradient_accumulation_steps\": GRAD_ACCUM_STEPS,\n",
        "            \"peak_memory_usage\": get_memory_usage()\n",
        "        }\n",
        "    }\n",
        ")\n",
        "print(f\"Distilled model saved to {final_path}\")\n",
        "print(f\"Final memory usage: {get_memory_usage():.2f}GB\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "adc0885afd374d1eaf2e91475747a636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c968ed62e1924b5ba9547cc071a44b28",
              "IPY_MODEL_f7900ccb6ed7474a8b58e45d00fdec8c",
              "IPY_MODEL_f4baccc271be4807b82c0cba0cd1c694"
            ],
            "layout": "IPY_MODEL_7b2d56ec4e144f2a98070de5dcd8094b"
          }
        },
        "c968ed62e1924b5ba9547cc071a44b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1869a8004954ff387798acad7a88f81",
            "placeholder": "​",
            "style": "IPY_MODEL_ded233f45e8149dcb36100eaa644953b",
            "value": "Fetching 15 files: 100%"
          }
        },
        "f7900ccb6ed7474a8b58e45d00fdec8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812a866c754a4c4d8beb55f11b96b510",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d83395e9bed54b6f97f916d863a3a45f",
            "value": 15
          }
        },
        "f4baccc271be4807b82c0cba0cd1c694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_029688b9313140a997ee149ceb0644da",
            "placeholder": "​",
            "style": "IPY_MODEL_71a406255fec48ec99fabbe2b95a5cfb",
            "value": " 15/15 [00:00&lt;00:00, 1585.99it/s]"
          }
        },
        "7b2d56ec4e144f2a98070de5dcd8094b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1869a8004954ff387798acad7a88f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded233f45e8149dcb36100eaa644953b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "812a866c754a4c4d8beb55f11b96b510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d83395e9bed54b6f97f916d863a3a45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "029688b9313140a997ee149ceb0644da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71a406255fec48ec99fabbe2b95a5cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}