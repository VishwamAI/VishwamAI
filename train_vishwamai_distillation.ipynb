{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbg5csLN9-x"
      },
      "source": [
        "# VishwamAI Distillation Training\n",
        "\n",
        "This notebook implements knowledge distillation from DeepSeek to a smaller VishwamAI model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0DJ8BItN9-z",
        "outputId": "e7542f88-2773-4a65-b4ad-060adaffa25a"
      },
      "outputs": [],
      "source": [
        "# Clone the repository and change directory\n",
        "!git clone https://github.com/VishwamAI/VishwamAI\n",
        "%cd VishwamAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McG6UzJpN9-0"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q --upgrade transformers datasets accelerate bitsandbytes sentencepiece \\\n",
        "    flax optax omegaconf huggingface-hub einops aim>=3.17.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmuE9ixWN9-0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from omegaconf import OmegaConf\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax.training import train_state\n",
        "\n",
        "from vishwamai.model import VishwamAIModel, ModelConfig\n",
        "from vishwamai.tokenizer import VishwamAITokenizer\n",
        "from vishwamai.distillation import VishwamaiGuruKnowledge, VishwamaiShaalaTrainer\n",
        "from vishwamai.data_utils import create_train_dataloader, create_val_dataloader\n",
        "from huggingface_hub import snapshot_download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXqn3fOmN9-0"
      },
      "source": [
        "## Configuration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dRmGenmN9-1"
      },
      "outputs": [],
      "source": [
        "# Teacher model config (DeepSeek)\n",
        "teacher_config = {\n",
        "    \"vocab_size\": 129280,\n",
        "    \"hidden_size\": 7168,\n",
        "    \"num_layers\": 61,\n",
        "    \"num_attention_heads\": 128,\n",
        "    \"intermediate_size\": 18432,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"attention_dropout_prob\": 0.1,\n",
        "    \"max_position_embeddings\": 163840,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_eps\": 1e-5,\n",
        "    \"use_cache\": True, # Changed from true to True\n",
        "    \"pad_token_id\": 0,\n",
        "    \"bos_token_id\": 1,\n",
        "    \"eos_token_id\": 2,\n",
        "    \"tie_word_embeddings\": True,\n",
        "    \"use_flash_attention\": True,\n",
        "    \"use_rope\": True,\n",
        "    \"use_alibi\": False, # Changed from false to False\n",
        "    \"use_gqa\": True,\n",
        "    \"num_key_value_heads\": 128,\n",
        "    \"dtype\": \"bfloat16\"\n",
        "}\n",
        "\n",
        "# Student model config (smaller VishwamAI)\n",
        "student_config = {\n",
        "    \"vocab_size\": 129280,\n",
        "    \"hidden_size\": 2048,  # Smaller hidden size\n",
        "    \"num_layers\": 24,  # Fewer layers\n",
        "    \"num_attention_heads\": 32,\n",
        "    \"intermediate_size\": 8192,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"attention_dropout_prob\": 0.1,\n",
        "    \"max_position_embeddings\": 163840,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_eps\": 1e-5,\n",
        "    \"use_cache\": True, # Changed from true to True\n",
        "    \"pad_token_id\": 0,\n",
        "    \"bos_token_id\": 1,\n",
        "    \"eos_token_id\": 2,\n",
        "    \"tie_word_embeddings\": True,\n",
        "    \"use_flash_attention\": True,\n",
        "    \"use_rope\": True,\n",
        "    \"use_alibi\": False, # Changed from false to False\n",
        "    \"use_gqa\": True,\n",
        "    \"num_key_value_heads\": 32,\n",
        "    \"dtype\": \"bfloat16\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIFwM6B3N9-1"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl2tVuXzN9-2"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "def download_partial_model(model_path: str, num_shards: int = 15):\n",
        "    \"\"\"Download only specified number of model shards\"\"\"\n",
        "\n",
        "    # Create pattern to match only first N safetensor files\n",
        "    patterns = [f\"model-{i+1:05d}-of-00252.safetensors\" for i in range(num_shards)]\n",
        "    patterns.extend([\"config.json\", \"tokenizer.model\"])  # Add other required files\n",
        "\n",
        "    try:\n",
        "        local_path = snapshot_download(\n",
        "            repo_id=model_path,\n",
        "            allow_patterns=patterns,\n",
        "            local_files_only=False,\n",
        "            resume_download=True\n",
        "        )\n",
        "        print(f\"Successfully downloaded {num_shards} model shards to {local_path}\")\n",
        "        return local_path\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error downloading model shards: {str(e)}\")\n",
        "\n",
        "# Load distillation configuration\n",
        "# Use os.path.join to create the full path\n",
        "distillation_config = OmegaConf.load(os.path.join(\"vishwamai\", \"configs\", \"training\", \"perplexity_r1_distillation.yaml\"))\n",
        "\n",
        "# Initialize teacher model with partial weights\n",
        "teacher_path = download_partial_model(\n",
        "    distillation_config['distillation']['teacher_model']['path'],\n",
        "    num_shards=15\n",
        ")\n",
        "\n",
        "teacher_config = distillation_config['distillation']['teacher_model']['config']\n",
        "student_config = distillation_config['distillation']['student_model']['config']\n",
        "\n",
        "teacher_model = VishwamAIModel(ModelConfig(**teacher_config))\n",
        "teacher_model.load_weights(teacher_path)\n",
        "\n",
        "# Initialize student model\n",
        "student_model = VishwamAIModel(ModelConfig(**student_config))\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = VishwamAITokenizer(\n",
        "    vocab_size=teacher_config[\"vocab_size\"],\n",
        "    model_prefix=\"vishwamai\"\n",
        ")\n",
        "\n",
        "# Initialize distillation trainer\n",
        "trainer = VishwamaiShaalaTrainer(\n",
        "    teacher_model=teacher_model,\n",
        "    student_model=student_model,\n",
        "    cfg=distillation_config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jPU9A9kN9-2"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fbjwR4KN9-2"
      },
      "outputs": [],
      "source": [
        "# Import Aim for experiment tracking\n",
        "import aim\n",
        "\n",
        "# Initialize Aim\n",
        "aim_run = aim.Run(experiment=\"VishwamAI-Distillation\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = create_train_dataloader(OmegaConf.create(distillation_config))\n",
        "val_loader = create_val_dataloader(OmegaConf.create(distillation_config))\n",
        "\n",
        "# Initialize training state\n",
        "rng = jax.random.PRNGKey(42)\n",
        "state = trainer.create_train_state(rng)\n",
        "\n",
        "# Initialize guru knowledge with feature matching\n",
        "guru = VishwamaiGuruKnowledge(OmegaConf.create(distillation_config))\n",
        "\n",
        "# Start training\n",
        "try:\n",
        "    for step in range(distillation_config['training']['max_steps']):\n",
        "        batch = next(train_loader)\n",
        "\n",
        "        # Get teacher predictions and features\n",
        "        teacher_outputs = teacher_model(\n",
        "            batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=True\n",
        "        )\n",
        "\n",
        "        # Training step with knowledge distillation\n",
        "        state, metrics, rng = trainer.train_step(\n",
        "            state=state,\n",
        "            batch=batch,\n",
        "            step=step,\n",
        "            rng=rng\n",
        "        )\n",
        "\n",
        "        # Enhanced logging with distillation metrics\n",
        "        if step % distillation_config['training']['logging_steps'] == 0:\n",
        "            distill_metrics = {\n",
        "                'kd_loss': metrics['kd_loss'],\n",
        "                'feature_loss': metrics.get('feature_loss', 0.0),\n",
        "                'attention_loss': metrics.get('attention_loss', 0.0),\n",
        "                'hidden_loss': metrics.get('hidden_loss', 0.0),\n",
        "                'total_loss': metrics['total_loss'],\n",
        "                'temperature': guru.temperature\n",
        "            }\n",
        "\n",
        "            # Log to Aim\n",
        "            for metric_name, metric_value in distill_metrics.items():\n",
        "                aim_run.track(metric_value, name=metric_name, step=step)\n",
        "\n",
        "            # Print current distillation progress\n",
        "            print(f\"\\nStep {step}:\")\n",
        "            print(f\"KD Loss: {distill_metrics['kd_loss']:.4f}\")\n",
        "            print(f\"Feature Loss: {distill_metrics['feature_loss']:.4f}\")\n",
        "            print(f\"Total Loss: {distill_metrics['total_loss']:.4f}\")\n",
        "\n",
        "        # Evaluation with feature matching\n",
        "        if step % distillation_config['training']['eval_steps'] == 0:\n",
        "            eval_metrics = trainer.evaluate(\n",
        "                state=state,\n",
        "                val_loader=val_loader,\n",
        "                teacher_model=teacher_model,\n",
        "                guru=guru\n",
        "            )\n",
        "\n",
        "            # Log evaluation metrics to Aim\n",
        "            for k, v in eval_metrics.items():\n",
        "                aim_run.track(v, name=f\"eval_{k}\", step=step)\n",
        "\n",
        "        # Dynamic temperature adjustment\n",
        "        if step % 1000 == 0:\n",
        "            guru.temperature = max(1.0, guru.temperature * 0.95)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if step % distillation_config['training']['save_steps'] == 0:\n",
        "            ckpt_path = f\"checkpoints/step_{step}\"\n",
        "            trainer.save_checkpoint(\n",
        "                state=state,\n",
        "                path=ckpt_path,\n",
        "                guru=guru,\n",
        "                metadata={\n",
        "                    'temperature': guru.temperature,\n",
        "                    'step': step,\n",
        "                    'metrics': metrics\n",
        "                }\n",
        "            )\n",
        "            # Track checkpoint as artifact in Aim\n",
        "            aim_run.track_artifact(ckpt_path, name=\"checkpoints\")\n",
        "\n",
        "    # Final quantization with teacher guidance\n",
        "    if distillation_config['distillation']['quantization']['enabled']:\n",
        "        state = trainer.quantize_model(\n",
        "            state=state,\n",
        "            val_loader=val_loader,\n",
        "            num_calibration_steps=100\n",
        "        )\n",
        "        trainer.save_checkpoint(state, \"checkpoints/quantized\")\n",
        "        aim_run.track_artifact(\"checkpoints/quantized\", name=\"quantized_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training failed: {str(e)}\")\n",
        "    aim_run.set_params({\"error\": str(e)})\n",
        "finally:\n",
        "    aim_run.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soQScC1IN9-3"
      },
      "source": [
        "## Model Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bfk-XvaN9-3"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Push distilled model to HuggingFace Hub\n",
        "api = HfApi()\n",
        "\n",
        "repo_id = \"VishwamAI/VishwamAI-small\"\n",
        "api.create_repo(repo_id, exist_ok=True)\n",
        "\n",
        "# Upload model files\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"configs/student_config.json\",\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=repo_id\n",
        ")\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"checkpoints/quantized/model.safetensors\",\n",
        "    path_in_repo=\"model.safetensors\",\n",
        "    repo_id=repo_id\n",
        ")\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"tokenizer/vishwamai.model\",\n",
        "    path_in_repo=\"tokenizer.model\",\n",
        "    repo_id=repo_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdwrWGndN9-3"
      },
      "source": [
        "## Evaluation and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4MuAzn9N9-3"
      },
      "outputs": [],
      "source": [
        "# Compare teacher and student model performance\n",
        "def evaluate_models(teacher, student, val_loader, num_batches=10):\n",
        "    teacher_metrics = []\n",
        "    student_metrics = []\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch = next(val_loader)\n",
        "\n",
        "        # Teacher predictions\n",
        "        teacher_output = teacher(batch['input_ids'], deterministic=True)\n",
        "        teacher_metrics.append({\n",
        "            'loss': float(teacher_output['loss']),\n",
        "            'accuracy': float(teacher_output.get('accuracy', 0))\n",
        "        })\n",
        "\n",
        "        # Student predictions\n",
        "        student_output = student(batch['input_ids'], deterministic=True)\n",
        "        student_metrics.append({\n",
        "            'loss': float(student_output['loss']),\n",
        "            'accuracy': float(student_output.get('accuracy', 0))\n",
        "        })\n",
        "\n",
        "    # Calculate averages\n",
        "    teacher_avg = {k: sum(m[k] for m in teacher_metrics) / len(teacher_metrics)\n",
        "                  for k in teacher_metrics[0]}\n",
        "    student_avg = {k: sum(m[k] for m in student_metrics) / len(student_metrics)\n",
        "                   for k in student_metrics[0]}\n",
        "\n",
        "    return {\n",
        "        'teacher': teacher_avg,\n",
        "        'student': student_avg,\n",
        "        'compression_ratio': f\"{teacher.param_count / student.param_count:.2f}x\"\n",
        "    }\n",
        "\n",
        "results = evaluate_models(teacher_model, student_model, val_loader)\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"Compression Ratio: {results['compression_ratio']}\")\n",
        "print(\"\\nTeacher Model:\")\n",
        "print(f\"Loss: {results['teacher']['loss']:.4f}\")\n",
        "print(f\"Accuracy: {results['teacher']['accuracy']:.4f}\")\n",
        "print(\"\\nStudent Model:\")\n",
        "print(f\"Loss: {results['student']['loss']:.4f}\")\n",
        "print(f\"Accuracy: {results['student']['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P85BYMmjN9-4"
      },
      "outputs": [],
      "source": [
        "# Update model paths\n",
        "TEACHER_MODEL_PATH = \"perplexity-ai/r1-1776\"\n",
        "OUTPUT_MODEL_PATH = \"VishwamAI/Perplexity_r1_disttled_experiment\"\n",
        "\n",
        "# Verify teacher model files exist\n",
        "import os\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# Download teacher model files\n",
        "teacher_path = snapshot_download(\n",
        "    repo_id=TEACHER_MODEL_PATH,\n",
        "    allow_patterns=[\"*.safetensors\", \"config.json\", \"tokenizer.model\"]\n",
        ")\n",
        "print(f\"Downloaded teacher model to {teacher_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwtMM3v8N9-4"
      },
      "outputs": [],
      "source": [
        "# Modified model saving logic\n",
        "def save_sharded_model(state, save_dir, num_shards=15):\n",
        "    \"\"\"Save model weights in sharded safetensor format.\"\"\"\n",
        "    import safetensors\n",
        "    import safetensors.flax as stf\n",
        "    import os\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Get model parameters\n",
        "    params = state.params\n",
        "\n",
        "    # Calculate parameters per shard\n",
        "    total_params = sum(p.size for p in jax.tree_leaves(params))\n",
        "    params_per_shard = total_params // num_shards\n",
        "\n",
        "    # Save in sharded format\n",
        "    current_shard = 0\n",
        "    current_size = 0\n",
        "    shard_dict = {}\n",
        "\n",
        "    for name, param in params.items():\n",
        "        param_size = param.size\n",
        "\n",
        "        if current_size + param_size > params_per_shard:\n",
        "            # Save current shard\n",
        "            shard_path = os.path.join(\n",
        "                save_dir,\n",
        "                f\"model-{current_shard+1:05d}-of-{num_shards:05d}.safetensors\"\n",
        "            )\n",
        "            stf.save_file(shard_dict, shard_path)  # Use stf instead of safetensors\n",
        "\n",
        "            # Start new shard\n",
        "            current_shard += 1\n",
        "            current_size = 0\n",
        "            shard_dict = {}\n",
        "\n",
        "        shard_dict[name] = param\n",
        "        current_size += param_size\n",
        "\n",
        "    # Save final shard\n",
        "    if shard_dict:\n",
        "        shard_path = os.path.join(\n",
        "            save_dir,\n",
        "            f\"model-{current_shard+1:05d}-of-{num_shards:05d}.safetensors\"\n",
        "        )\n",
        "        stf.save_file(shard_dict, shard_path)  # Use stf instead of safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63OiF9wON9-4"
      },
      "outputs": [],
      "source": [
        "# Modified model export cell\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Push distilled model to HuggingFace Hub\n",
        "api = HfApi()\n",
        "\n",
        "# Save model in sharded format\n",
        "save_dir = \"checkpoints/final\"\n",
        "save_sharded_model(state, save_dir)\n",
        "\n",
        "# Upload to HF Hub\n",
        "api.create_repo(OUTPUT_MODEL_PATH, exist_ok=True)\n",
        "\n",
        "# Upload configuration\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"configs/student_config.json\",\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=OUTPUT_MODEL_PATH\n",
        ")\n",
        "\n",
        "# Upload all model shards\n",
        "for shard_file in sorted(os.listdir(save_dir)):\n",
        "    if shard_file.endswith(\".safetensors\"):\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=os.path.join(save_dir, shard_file),\n",
        "            path_in_repo=shard_file,\n",
        "            repo_id=OUTPUT_MODEL_PATH\n",
        "        )\n",
        "\n",
        "# Upload tokenizer\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"tokenizer/vishwamai.model\",\n",
        "    path_in_repo=\"tokenizer.model\",\n",
        "    repo_id=OUTPUT_MODEL_PATH\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
