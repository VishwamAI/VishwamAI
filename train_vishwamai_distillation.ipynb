{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI QwQ-32B Distillation\n",
    "\n",
    "Optimized distillation from [Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) using gradient accumulation for TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VishwamAI/vishwamai.git\n",
    "%cd vishwamai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q --upgrade transformers datasets accelerate bitsandbytes sentencepiece \\\n",
    "    flax optax omegaconf huggingface-hub einops aim>=3.17.5 safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import safetensors.flax as stf\n",
    "from omegaconf import OmegaConf\n",
    "import aim\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.qwen_distiller import QwenDistillationTrainer\n",
    "from vishwamai.qwen_data import QwenDataLoader\n",
    "\n",
    "# Print device info\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Number of devices: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = OmegaConf.load('configs/distillation_config.yaml')\n",
    "\n",
    "# TPU-aware batch settings\n",
    "NUM_TPU_DEVICES = jax.device_count()\n",
    "BATCH_SIZE = 1  # Per device\n",
    "GRAD_ACCUM_STEPS = 16  # To achieve effective batch size of 16\n",
    "\n",
    "config.training.batch_size = BATCH_SIZE\n",
    "config.training.gradient_accumulation_steps = GRAD_ACCUM_STEPS\n",
    "\n",
    "print(f\"Configuration loaded with:\")\n",
    "print(f\"- Batch size per device: {BATCH_SIZE}\")\n",
    "print(f\"- Gradient accumulation steps: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"- Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS * NUM_TPU_DEVICES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download QwQ model\n",
    "print(\"Downloading QwQ-32B model files...\")\n",
    "qwq_path = snapshot_download(\n",
    "    \"Qwen/QwQ-32B\",\n",
    "    allow_patterns=[\"*.safetensors\", \"config.json\", \"tokenizer.model\"],\n",
    "    local_files_only=False,\n",
    "    resume_download=True\n",
    ")\n",
    "print(f\"Downloaded QwQ-32B to {qwq_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader with TPU-aware settings\n",
    "loader = QwenDataLoader(\n",
    "    safetensor_dir=qwq_path,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_sequence_length=config.distillation.teacher_model.config.max_position_embeddings,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS\n",
    ")\n",
    "\n",
    "print(\"Data loader initialized with gradient accumulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "teacher_model = VishwamAIModel(ModelConfig(**config.distillation.teacher_model.config))\n",
    "student_model = VishwamAIModel(ModelConfig(**config.distillation.student_model.config))\n",
    "\n",
    "print(\"Loading QwQ model weights...\")\n",
    "params = loader.load_all_shards()\n",
    "teacher_model = teacher_model.bind({'params': params})\n",
    "print(\"Models initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with experiment tracking\n",
    "aim_run = aim.Run(\n",
    "    experiment=\"VishwamAI-QwQ-Distillation\",\n",
    "    log_system_params=True\n",
    ")\n",
    "aim_run.set_params({\n",
    "    \"teacher_model\": \"QwQ-32B\",\n",
    "    \"student_model\": \"VishwamAI-7B\",\n",
    "    \"batch_size_per_device\": BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": GRAD_ACCUM_STEPS,\n",
    "    \"effective_batch_size\": BATCH_SIZE * GRAD_ACCUM_STEPS * NUM_TPU_DEVICES,\n",
    "    **OmegaConf.to_container(config, resolve=True)\n",
    "})\n",
    "\n",
    "trainer = QwenDistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = trainer.create_train_state(rng)\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with gradient accumulation\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    for epoch in range(5):  # 5 epochs\n",
    "        print(f\"\\nEpoch {epoch + 1}/5\")\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Process shards sequentially\n",
    "        for shard_name, shard_params in tqdm(loader.get_shard_stream(), desc=\"Processing QwQ shards\"):\n",
    "            # Accumulate gradients over multiple steps\n",
    "            accumulated_gradients = None\n",
    "            \n",
    "            for accum_step in range(GRAD_ACCUM_STEPS):\n",
    "                # Create batch for current accumulation step\n",
    "                batch = loader.create_training_batch(\n",
    "                    input_ids=shard_params['input_ids'],\n",
    "                    labels=shard_params.get('labels')\n",
    "                )\n",
    "                \n",
    "                # Training step\n",
    "                state, metrics, grads = trainer.train_step_with_grads(\n",
    "                    state=state,\n",
    "                    batch=batch,\n",
    "                    rng=rng\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                if accumulated_gradients is None:\n",
    "                    accumulated_gradients = grads\n",
    "                else:\n",
    "                    accumulated_gradients = jax.tree_map(\n",
    "                        lambda x, y: x + y,\n",
    "                        accumulated_gradients,\n",
    "                        grads\n",
    "                    )\n",
    "            \n",
    "            # Apply accumulated gradients\n",
    "            accumulated_gradients = jax.tree_map(\n",
    "                lambda x: x / GRAD_ACCUM_STEPS,\n",
    "                accumulated_gradients\n",
    "            )\n",
    "            state = state.apply_gradients(grads=accumulated_gradients)\n",
    "            \n",
    "            # Log metrics\n",
    "            if state.step % config.training.logging_steps == 0:\n",
    "                aim_run.track(\n",
    "                    metrics,\n",
    "                    step=state.step,\n",
    "                    context={\n",
    "                        'shard': shard_name,\n",
    "                        'epoch': epoch\n",
    "                    }\n",
    "                )\n",
    "                print(f\"Step {state.step}: loss={metrics['loss']:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if state.step % config.training.save_steps == 0:\n",
    "                ckpt_path = f\"checkpoints/step_{state.step}\"\n",
    "                trainer.save_checkpoint(\n",
    "                    state=state,\n",
    "                    path=ckpt_path,\n",
    "                    extra_info={\n",
    "                        'epoch': epoch,\n",
    "                        'shard': shard_name,\n",
    "                        'metrics': metrics\n",
    "                    }\n",
    "                )\n",
    "                aim_run.track_artifact(ckpt_path, name=\"checkpoints\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if state.step % 10 == 0:\n",
    "                jax.clear_caches()\n",
    "                gc.collect()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch completed in {epoch_time:.2f}s\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted, saving checkpoint...\")\n",
    "    trainer.save_checkpoint(state, \"checkpoints/interrupted\")\n",
    "finally:\n",
    "    aim_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final distilled model\n",
    "final_path = \"final_vishwamai_model\"\n",
    "trainer.save_model(\n",
    "    state=state,\n",
    "    path=final_path,\n",
    "    config_override={\n",
    "        \"parent_model\": \"Qwen/QwQ-32B\",\n",
    "        \"distillation_version\": \"v1.0\",\n",
    "        \"training_config\": {\n",
    "            \"batch_size_per_device\": BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": GRAD_ACCUM_STEPS,\n",
    "            \"effective_batch_size\": BATCH_SIZE * GRAD_ACCUM_STEPS * NUM_TPU_DEVICES\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Distilled model saved to {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
