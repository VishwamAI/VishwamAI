{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI QwQ-32B Distillation\n",
    "\n",
    "Optimized distillation from [Qwen/QwQ-32B](https://huggingface.co/Qwen/QwQ-32B) to VishwamAI-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import safetensors.flax as stf\n",
    "from omegaconf import OmegaConf\n",
    "import aim\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vishwamai.model import VishwamAIModel, ModelConfig\n",
    "from vishwamai.qwen_distiller import QwenDistillationTrainer\n",
    "from vishwamai.qwen_data import QwenDataLoader\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QwQ-specific configuration\n",
    "config = OmegaConf.load('configs/distillation_config.yaml')\n",
    "print(\"Configuration loaded successfully\")\n",
    "\n",
    "# Download QwQ model files\n",
    "qwq_path = snapshot_download(\n",
    "    \"Qwen/QwQ-32B\",\n",
    "    allow_patterns=[\"*.safetensors\", \"config.json\", \"tokenizer.model\"],\n",
    "    local_files_only=False,\n",
    "    resume_download=True\n",
    ")\n",
    "print(f\"Downloaded QwQ-32B to {qwq_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader optimized for QwQ architecture\n",
    "loader = QwenDataLoader(\n",
    "    safetensor_dir=qwq_path,\n",
    "    batch_size=config.training.batch_size,\n",
    "    max_sequence_length=config.distillation.teacher_model.config.max_position_embeddings\n",
    ")\n",
    "\n",
    "print(\"Data loader initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with QwQ-specific configurations\n",
    "teacher_model = VishwamAIModel(ModelConfig(**config.distillation.teacher_model.config))\n",
    "student_model = VishwamAIModel(ModelConfig(**config.distillation.student_model.config))\n",
    "\n",
    "print(\"Loading QwQ model weights...\")\n",
    "params = loader.load_all_shards()\n",
    "teacher_model = teacher_model.bind({'params': params})\n",
    "print(\"Models initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with experiment tracking\n",
    "aim_run = aim.Run(\n",
    "    experiment=\"VishwamAI-QwQ-Distillation\",\n",
    "    log_system_params=True\n",
    ")\n",
    "aim_run.set_params({\n",
    "    \"teacher_model\": \"QwQ-32B\",\n",
    "    \"student_model\": \"VishwamAI-7B\",\n",
    "    **OmegaConf.to_container(config, resolve=True)\n",
    "})\n",
    "\n",
    "trainer = QwenDistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    cfg=config\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = trainer.create_train_state(rng)\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop optimized for QwQ architecture\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    for epoch in range(5):  # 5 epochs through all shards\n",
    "        print(f\"\\nEpoch {epoch + 1}/5\")\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Process shards sequentially\n",
    "        for shard_name, shard_params in tqdm(loader.get_shard_stream(), desc=\"Processing QwQ shards\"):\n",
    "            # Create training batch\n",
    "            batch = loader.create_training_batch(\n",
    "                input_ids=shard_params['input_ids']\n",
    "            )\n",
    "            \n",
    "            # Training step with QwQ optimizations\n",
    "            state, metrics = trainer.train_step(\n",
    "                state=state,\n",
    "                batch=batch,\n",
    "                rng=rng\n",
    "            )\n",
    "            \n",
    "            # Log metrics with shard context\n",
    "            if state.step % config.training.logging_steps == 0:\n",
    "                aim_run.track(\n",
    "                    metrics,\n",
    "                    step=state.step,\n",
    "                    context={\n",
    "                        'shard': shard_name,\n",
    "                        'epoch': epoch\n",
    "                    }\n",
    "                )\n",
    "                print(f\"Step {state.step}: loss={metrics['loss']:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if state.step % config.training.save_steps == 0:\n",
    "                ckpt_path = f\"checkpoints/step_{state.step}\"\n",
    "                trainer.save_checkpoint(\n",
    "                    state=state,\n",
    "                    path=ckpt_path,\n",
    "                    extra_info={\n",
    "                        'epoch': epoch,\n",
    "                        'shard': shard_name,\n",
    "                        'metrics': metrics\n",
    "                    }\n",
    "                )\n",
    "                aim_run.track_artifact(ckpt_path, name=\"checkpoints\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if state.step % 10 == 0:\n",
    "                jax.clear_caches()\n",
    "                gc.collect()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Epoch completed in {epoch_time:.2f}s\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted, saving checkpoint...\")\n",
    "    trainer.save_checkpoint(state, \"checkpoints/interrupted\")\n",
    "finally:\n",
    "    aim_run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final distilled model\n",
    "final_path = \"final_vishwamai_model\"\n",
    "trainer.save_model(\n",
    "    state=state,\n",
    "    path=final_path,\n",
    "    config_override={\n",
    "        \"parent_model\": \"Qwen/QwQ-32B\",\n",
    "        \"distillation_version\": \"v1.0\"\n",
    "    }\n",
    ")\n",
    "print(f\"Distilled model saved to {final_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
