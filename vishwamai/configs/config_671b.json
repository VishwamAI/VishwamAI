{
    "model_config": {
        "dim": 8192,
        "n_layers": 120,
        "vocab_size": 64000,
        "max_seq_len": 32768,
        "num_attention_heads": 64,
        "intermediate_size": 32768,
        "hidden_dropout_prob": 0.1,
        "attention_dropout_prob": 0.1
    },
    
    "components": {
        "use_neural_memory": true,
        "use_tree_of_thoughts": true,
        "use_cache_augmentation": true,
        
        "memory_config": {
            "memory_size": 2048,
            "num_memory_layers": 3,
            "num_attention_heads": 32,
            "intermediate_size": 16384,
            "dropout": 0.1
        },
        
        "tree_config": {
            "beam_width": 4,
            "max_depth": 3,
            "temperature": 0.7,
            "top_k": 50,
            "pruning_threshold": 0.1,
            "rewrite_factor": 0.3
        },
        
        "cache_config": {
            "max_cache_length": 65536,
            "num_heads": 8,
            "dropout": 0.1,
            "retrieval_factor": 1.0,
            "update_freq": 100
        }
    },
    
    "training": {
        "batch_size": 8,
        "gradient_accumulation_steps": 4,
        "learning_rate": 1.2e-4,
        "weight_decay": 0.01,
        "warmup_steps": 1000,
        "max_steps": 100000,
        "save_steps": 1000,
        "eval_steps": 500,
        "logging_steps": 10
    },
    
    "optimization": {
        "fp16": true,
        "bf16": false,
        "gradient_checkpointing": true,
        "fsdp": "full_shard",
        "fsdp_transformer_layer_cls_to_wrap": "VishwamAILayer",
        "optim": "adamw_torch",
        "lr_scheduler_type": "cosine",
        "max_grad_norm": 1.0
    },
    
    "colab_specific": {
        "A100_optimized": {
            "batch_size": 8,
            "gradient_accumulation": 4,
            "memory_size": 2048,
            "tree_beam_width": 4,
            "cache_size": 65536
        },
        "V100_optimized": {
            "batch_size": 4,
            "gradient_accumulation": 8,
            "memory_size": 1024,
            "tree_beam_width": 3,
            "cache_size": 32768
        },
        "T4_optimized": {
            "batch_size": 2,
            "gradient_accumulation": 16,
            "memory_size": 512,
            "tree_beam_width": 2,
            "cache_size": 16384
        }
    },
    
    "tokenizer": {
        "model_max_length": 32768,
        "padding_side": "right",
        "truncation_side": "right",
        "special_tokens": {
            "bos_token": "<s>",
            "eos_token": "</s>",
            "unk_token": "<unk>",
            "pad_token": "<pad>"
        }
    },
    
    "generation": {
        "max_length": 32768,
        "min_length": 0,
        "temperature": 0.7,
        "top_k": 50,
        "top_p": 0.9,
        "repetition_penalty": 1.1,
        "length_penalty": 1.0,
        "no_repeat_ngram_size": 4
    }
}
