{
    "model_config": {
        "vocab_size": 102400,
        "dim": 2048,
        "inter_dim": 8192,
        "n_heads": 16,
        "n_layers": 24,
        "n_dense_layers": 1,
        "max_seq_len": 2048,
        "max_batch_size": 4,
        "dtype": "bf16",
        "n_routed_experts": 32,
        "n_shared_experts": 2,
        "n_activated_experts": 4,
        "moe_inter_dim": 4096,
        "use_cache_augmentation": true,
        "cache_hidden_size": 256,
        "cache_num_heads": 4,
        "use_neural_memory": true,
        "memory_size": 512,
        "use_tree_of_thoughts": true
    },
    "training_config": {
        "batch_size": 4,
        "gradient_accumulation_steps": 8,
        "learning_rate": 2e-5,
        "weight_decay": 0.01,
        "warmup_steps": 100,
        "max_steps": 1000,
        "save_steps": 200,
        "eval_steps": 100,
        "logging_steps": 10
    },
    "optimization_config": {
        "use_flash_attention": true,
        "gradient_checkpointing": true,
        "fp16": true,
        "bf16": true,
        "use_fused_adam": true,
        "offload_optimizer": false,
        "use_kernel_optimizations": true,
        "cache_size": "auto",
        "memory_efficient_attention": true
    },
    "hardware_config": {
        "gpu_memory_threshold": {
            "small": 8,
            "medium": 16,
            "large": 32
        },
        "scaling_factors": {
            "dim_scale": {
                "small": 0.5,
                "medium": 0.75,
                "large": 1.0
            },
            "batch_scale": {
                "small": 0.25,
                "medium": 0.5,
                "large": 1.0
            }
        }
    },
    "colab_specific": {
        "T4_optimized": {
            "dim": 1536,
            "batch_size": 2,
            "gradient_accumulation": 16,
            "use_fp16": true,
            "max_seq_len": 1024
        },
        "V100_optimized": {
            "dim": 2048,
            "batch_size": 4,
            "gradient_accumulation": 8,
            "use_fp16": true,
            "max_seq_len": 2048
        },
        "A100_optimized": {
            "dim": 2048,
            "batch_size": 8,
            "gradient_accumulation": 4,
            "use_fp8": true,
            "max_seq_len": 4096
        }
    }
}
