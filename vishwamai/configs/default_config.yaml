defaults:
  - model: 10B
  - training: default
  - training: pretraining_progress
  - override hydra/job_logging: colorlog
  - _self_

training:
  # Basic training parameters
  learning_rate: 0.0001
  warmup_steps: 1000
  max_steps: 100000
  batch_size: 32
  eval_batch_size: 8
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.01
  
  # Curriculum learning configuration
  curriculum:
    enabled: true
    type: "length"  # Options: length, difficulty
    initial_length: 64  # Starting sequence length
    length_increment: 32  # How much to increase length each update
    update_every: 1000  # Update curriculum every N steps
    
  # Mixed precision and TPU optimization
  mixed_precision:
    enabled: true
    dtype: "float16"
    dynamic_scale: true  # Enable dynamic loss scaling
    initial_scale: 128.0  # Initial loss scale for mixed precision
    
  # Model specific optimizations
  model_parallelism: false  # Enable model parallelism for large models
  gradient_checkpointing: true  # Save memory with gradient checkpointing
  enable_pjit: true  # Enable parallel training with pjit

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  
  # TPU-specific configuration
  device:
    type: "tpu"  # or "gpu", "cpu"
    num_devices: 8
    precision: "bfloat16"  # bfloat16 preferred for TPU
    memory_allocation: 0.85  # Fraction of memory to allocate

monitoring:
  enabled: true
  log_every_n_steps: 100
  save_every_n_steps: 1000
  memory_metrics: true  # Track memory usage
  profile_steps: 100  # Profile training steps
  mlflow:
    enabled: true
    tracking_uri: "mlruns"
    experiment_name: "vishwamai"
    run_name: ${now:%Y-%m-%d_%H-%M-%S}
    log_artifacts: true
  tensorboard:
    enabled: true
    log_dir: ${hydra.run.dir}/tensorboard

tuning:
  enabled: false
  framework: "optuna"  # or "ray"
  num_trials: 100
  metric: "validation_loss"
  direction: "minimize"
  pruning:
    enabled: true
    n_startup_trials: 5
    n_warmup_steps: 100

distributed:
  strategy: "data_parallel"  # or "model_parallel"
  sync_batch_norm: true
  gradient_as_bucket_view: true
  find_unused_parameters: false
