# Multi-Level Attention configuration

# Basic MLA settings
mla:
  enabled: true
  num_levels: 3
  fusion_type: weighted_sum  # {weighted_sum, gated, learned}
  residual_connection: true
  normalize_levels: true
  level_dropout: 0.1
  use_layer_norm: true
  layer_norm_eps: 1e-7

# Level configurations
levels:
  ratios: [1.0, 0.5, 0.25]  # Progressive downsampling ratios
  attention_heads: [32, 16, 8]  # Heads per level
  head_dims: [128, 128, 128]  # Dimension per head
  initializer:
    type: truncated_normal
    mean: 0.0
    std: 0.02
    min_val: -0.06
    max_val: 0.06

# Attention configurations
attention:
  dropout: 0.1
  attention_dropout: 0.1
  use_relative_positions: true
  max_relative_position: 32
  relative_attention_num_buckets: 32
  use_bias: true
  attention_type: flash  # {flash, standard, linear}
  causal_mask: true
  sliding_window: null
  sparse_computation: true

# Level-wise fusion
fusion:
  type: weighted_sum  # {weighted_sum, gated, learned, adaptive}
  initialization: uniform
  learnable_weights: true
  normalize_weights: true
  temperature: 1.0
  gating_network:
    hidden_size: 256
    activation: gelu
    dropout: 0.1
    num_layers: 2

# MLA-MoE integration
moe_integration:
  enabled: true
  fusion_strategy: parallel  # {parallel, sequential, hybrid}
  expert_attention_ratio: 0.5
  shared_experts: true
  expert_capacity_factor: 1.25
  routing_strategy: joint  # {joint, independent}
  balance_computation: true

# Hierarchical processing
hierarchical:
  downsampling:
    method: strided  # {strided, pooling, learned}
    pooling_type: max
    kernel_size: 2
    adaptive_pooling: false
  upsampling:
    method: interpolation  # {interpolation, transposed, learned}
    mode: linear
    align_corners: true
  skip_connections: true
  progressive_refinement: true

# Memory management
memory:
  efficient_attention: true
  gradient_checkpointing: true
  optimize_memory_use: true
  activation_recomputation: selective
  max_cache_size: null
  attention_block_size: 1024
  optimize_bandwidth: true

# Computation optimization
optimization:
  use_flash_attention: true
  fuse_operations: true
  optimize_communication: true
  jit_compile: true
  kernel_fusion: true
  recompute_threshold: 0.5
  mixed_precision: true
  quantization:
    enabled: false
    precision: int8
    method: dynamic

# Level-specific settings
level_configs:
  level_0:  # Fine-grained
    window_size: null
    sparse_factor: 1.0
    attention_type: full
    num_heads: 32
  level_1:  # Medium-grained
    window_size: 256
    sparse_factor: 0.5
    attention_type: sparse
    num_heads: 16
  level_2:  # Coarse-grained
    window_size: 512
    sparse_factor: 0.25
    attention_type: sparse
    num_heads: 8

# Monitoring and visualization
monitoring:
  track_attention_patterns: true
  track_level_contributions: true
  track_fusion_weights: true
  track_memory_usage: true
  save_attention_maps: false
  visualization:
    enabled: true
    plot_attention_weights: true
    plot_level_activations: true
    plot_fusion_weights: true
    max_samples: 10

# Training dynamics
training:
  warmup_steps: 2000
  adaptive_span: false
  adaptive_dropout: false
  layer_dropout: 0.1
  gradient_clipping: 1.0
  stabilize_training: true
  regularization:
    type: none
    coefficient: 0.01
