# Multi-Layer Attention Configuration

# Core MLA Parameters
use_mla: true
num_prev_layers: 4  # Number of previous layers to attend to
attention_window: 4  # Maximum number of layers back to look
layer_selection_method: "dynamic"  # Options: dynamic, fixed, learned

# Attention Configuration
num_mla_heads: 32  # Number of attention heads for MLA
head_dim: 64  # Dimension per attention head
use_rope: true  # Whether to use rotary embeddings
max_sequence_length: 4096
attention_dropout: 0.1
hidden_dropout: 0.1
use_flash_attention: true

# Layer State Management
normalize_cached: true  # Whether to normalize cached states
use_compressed_cache: false  # Whether to use compressed caching
compression_dim: null  # Dimension for compressed states
compression_method: "linear"  # Options: linear, svd, random_projection
eviction_policy: "lru"  # Options: lru, fifo, importance
max_cache_size: 8  # Maximum number of cached layer states

# Cross-Layer Gating
use_gate: true  # Whether to use gating mechanism
gate_type: "learnable"  # Options: learnable, fixed, adaptive
gate_temperature: 0.1  # Temperature for gate softmax
gate_init_eps: 0.1  # Initialization epsilon for gates
min_gate_value: 0.0  # Minimum gate value
max_gate_value: 1.0  # Maximum gate value

# Layer-wise Processing
layer_norm_eps: 1.0e-5
use_adaptive_layer_norm: false  # Whether to use adaptive layer norm
use_parallel_attention: true  # Whether to process layers in parallel
use_residual_connection: true  # Whether to use residual connections
use_pre_layer_norm: true  # Whether to use pre-layer normalization

# Memory Management
memory_efficient_attention: true  # Whether to use memory-efficient attention
use_gradient_checkpointing: true  # Whether to use gradient checkpointing
optimize_memory_usage: true  # Whether to optimize memory usage
max_memory_size: null  # Maximum memory size in bytes (null = auto)

# State Compression
compression_ratio: 4  # Compression ratio for cached states
use_quantization: false  # Whether to quantize cached states
quantization_bits: 8  # Number of bits for quantization
use_pruning: false  # Whether to prune cached states
pruning_threshold: 0.01  # Threshold for pruning

# Training
mla_learning_rate: 1.0e-4  # Learning rate for MLA components
mla_weight_decay: 0.01  # Weight decay for MLA components
gradient_clipping: 1.0  # Gradient clipping value
warmup_steps: 1000  # Number of warmup steps
enable_loss_scaling: true  # Whether to use loss scaling
loss_scale: "dynamic"  # Options: dynamic, static

# Regularization
attention_dropout_rate: 0.1  # Dropout rate for attention
hidden_dropout_rate: 0.1  # Dropout rate for hidden states
layer_dropout_rate: 0.0  # Dropout rate for layer selection
use_stochastic_depth: false  # Whether to use stochastic depth
path_dropout_rate: 0.0  # Dropout rate for residual paths

# Monitoring
monitor_attention_patterns: true  # Whether to monitor attention patterns
monitor_gate_values: true  # Whether to monitor gate values
monitor_cache_usage: true  # Whether to monitor cache usage
profiling_frequency: 100  # Steps between profiling
save_attention_maps: false  # Whether to save attention maps
detailed_logging: true  # Whether to enable detailed logging
