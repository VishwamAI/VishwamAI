dim: 6144
depth: 44
heads: 48
vocab_size: 50304
max_seq_len: 2048  # Reduced for math problems
dropout_rate: 0.1
expert_count: 4    # Reduced expert count for more focused training
expert_capacity: 8  # Increased capacity per expert
attention_type: "flash"  # More efficient attention
ffn_dim: 24576
head_dim: 128
rope_base: 10000
attention_bias: false
parallel_factor: 1

moe_config:
  num_experts: 4             # Reduced experts
  capacity_factor: 2.0       # Increased capacity
  expert_dropout: 0.05       # Reduced dropout for stability
  load_balance_weight: 0.02  # Increased balance weight
  router_z_loss_weight: 0.002
  router_aux_loss_weight: 0.002

error_correction:
  enabled: true
  hidden_size: 2048        # Increased for better error detection
  num_heads: 8            # More attention heads
  intermediate_size: 8192  # Larger intermediate size
  error_threshold: 0.05   # Lower threshold for math problems
  correction_weight: 0.7  # Higher correction weight

tot_config:
  enabled: true
  max_thoughts: 8        # More thoughts for complex math
  max_depth: 5          # Deeper reasoning
  beam_width: 5         # Wider beam search
  temperature: 0.5      # Lower temperature for precise outputs
  thought_dropout: 0.05 # Lower dropout for stability

optimization:
  gradient_checkpointing: true
  mixed_precision: true
  dtype: "bfloat16"
  param_init_scale: 0.02
  
model_parallel:
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  sequence_parallel: false
