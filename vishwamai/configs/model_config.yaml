# Model architecture configuration

# Core architecture
hidden_size: 2048
num_attention_heads: 32
num_hidden_layers: 24
head_dim: 64
intermediate_size: 8192
max_sequence_length: 4096

# Mixture of Experts
num_moe_layers: 4
moe_layer_frequency: 2
num_experts: 8
expert_capacity_factor: 1.25
num_experts_per_token: 2
expert_hidden_size: 8192
expert_dropout: 0.1

# Multi-Layer Attention
use_mla: true
num_prev_layers: 4
attention_window: 4
normalize_cached: true
use_compressed_cache: false
compression_dim: null

# Attention Configuration
use_flash_attention: true
use_rope: true
attention_dropout: 0.1
rope_scaling: 1.0
rope_theta: 10000.0

# General Configuration
hidden_dropout: 0.1
drop_path: 0.0
activation: "swiglu"  # Options: gelu, swiglu, silu
layer_norm_eps: 1.0e-5
initializer_range: 0.02
use_bias: false

# Precision
dtype: "bfloat16"
param_dtype: "bfloat16"

# Router Configuration
router_jitter_noise: 0.1
router_dtype: "float32"
gate_type: "top_k"
gate_temperature: 0.1
gate_noise_type: "multiplicative"
gate_noise_scale: 1.0
z_loss_scale: 0.01
load_balance_scale: 0.01

# Embedding Configuration
vocab_size: 32000
factorized_embeddings: false
factorized_dim: null
tie_word_embeddings: true
position_embedding_type: "rotary"
position_embedding_scale: 1.0

# Additional Parameters
use_cache: true
gradient_checkpointing: false
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
use_parallel_training: true
use_pjit_attention: true
use_scan_mlp: true
