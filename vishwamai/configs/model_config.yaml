# Model architecture configuration

# General model parameters
model:
  vocab_size: 32000
  max_sequence_length: 2048
  hidden_size: 4096
  num_layers: 32
  num_attention_heads: 32
  intermediate_size: 11008
  hidden_dropout_prob: 0.1
  attention_dropout_prob: 0.1
  layer_norm_eps: 1e-7
  activation: gelu
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
  tie_word_embeddings: true
  position_embedding_type: rotary
  gradient_checkpointing: true

# MoE parameters
moe:
  num_experts: 8
  num_experts_per_token: 2
  router_type: top_k
  router_bias: true
  router_eps: 1e-8
  router_z_loss_coef: 0.01
  expert_capacity_factor: 1.25
  aux_loss_coef: 0.01
  router_jitter_noise: 0.1
  dropout_rate: 0.1
  expert_initialization: truncated_normal
  expert_placement:
    strategy: uniform
    expert_parallel: true
    balance_load: true

# Multi-Level Attention parameters
mla:
  enabled: true
  num_levels: 3
  level_ratios: [1.0, 0.5, 0.25]
  fusion_type: weighted_sum
  attention_dropout: 0.1
  level_dropout: 0.1
  residual_connection: true
  adaptive_weights: true
  sparse_computation: true

# Embeddings
embeddings:
  word_embedding_type: learned
  position_embedding_type: rotary
  token_type_embedding: false
  layer_norm_eps: 1e-7
  dropout: 0.1
  embedding_size: 4096
  padding_idx: 0

# Optimization
optimization:
  use_flash_attention: true
  use_fused_layernorm: true
  use_kernel_optimizations: true
  memory_efficient_backward: true
  gradient_checkpointing: true
  mixed_precision: true
  amp_dtype: bfloat16
  tp_degree: 1
  pp_degree: 1

# Architecture variants
variants:
  base:
    hidden_size: 2048
    num_layers: 24
    num_attention_heads: 16
    num_experts: 8
  large:
    hidden_size: 4096
    num_layers: 32
    num_attention_heads: 32
    num_experts: 8
  xl:
    hidden_size: 5120
    num_layers: 40
    num_attention_heads: 40
    num_experts: 16
  xxl:
    hidden_size: 6144
    num_layers: 48
    num_attention_heads: 48
    num_experts: 16

# Expert configurations
expert_configs:
  ffn:
    type: mlp
    hidden_ratio: 4
    activation: gelu
    dropout: 0.1
    layer_norm: true
    bias: true
  cross_expert:
    enabled: true
    num_shared_experts: 2
    sharing_strategy: grad_sync

# Monitor settings
monitoring:
  track_expert_capacity: true
  track_balance_metrics: true
  track_router_metrics: true
  track_attention_patterns: true
  log_interval: 100
  profile_compute: true
  profile_memory: true
