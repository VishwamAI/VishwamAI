# Mixture of Experts Configuration for VishwamAI

moe:
  enabled: true
  num_experts: 8
  num_experts_per_token: 2  # Top-k routing
  expert_capacity_factor: 1.25  # Overallocation factor for load balancing
  jitter_noise: 0.1  # Noise for load balancing
  
# Expert Configuration
experts:
  hidden_size: 8192  # 4x base hidden size for expert FFN
  activation: "swiglu"
  dropout: 0.1
  use_bias: true
  initialization:
    weight_init: "kaiming_uniform"
    bias_init: "zeros"

# Router Configuration
router:
  routing_algorithm: "top_k"
  capacity_factor: 1.25
  load_balancing:
    importance: 0.01  # Weight for auxiliary load balancing loss
    type: "entropy"  # Options: entropy, coefficient_of_variation
  dropout: 0.1
  
# Gating Configuration
gating:
  type: "top_k"  # Options: top_k, multiplicative
  softmax_temperature: 0.1
  noise_type: "multiplicative"  # Options: multiplicative, additive
  noise_scale: 1.0
  z_loss_scale: 0.01  # Scale for router z-loss

# Load Balancing
load_balancing:
  auxiliary_loss: true
  aux_loss_weight: 0.01
  aux_loss_type: "load_balance"  # Options: load_balance, importance
  monitor_expert_usage: true
  usage_threshold: 0.9  # Alert if expert usage exceeds this

# Communication
communication:
  all_to_all_chunk_size: 1024
  precision: "bfloat16"
  optimize_communication: true
