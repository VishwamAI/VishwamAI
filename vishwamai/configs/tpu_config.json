{
    "model": {
        "vocab_size": 131072,
        "hidden_dim": 768,
        "num_layers": 24,
        "num_heads": 12,
        "head_dim": 64,
        "mlp_dim": 3072,
        "max_seq_len": 2048,
        "dropout_rate": 0.1,
        "attention_dropout_rate": 0.1,
        "use_flash_attn": true,
        "use_rms_norm": true,
        "dtype": "bfloat16",
        "emb_size": 768,
        "key_size": 64,
        "num_q_heads": 12,
        "num_kv_heads": 4,
        "widening_factor": 4.0,
        "init_scale": 1.0,
        "attn_output_multiplier": 1.0
    },
    "training": {
        "batch_size": 8,
        "grad_accum_steps": 4,
        "learning_rate": 1e-4,
        "warmup_steps": 2000,
        "max_steps": 500000,
        "weight_decay": 0.01,
        "max_grad_norm": 1.0,
        "save_every": 5000,
        "eval_every": 1000,
        "log_every": 100,
        "dtype": "bfloat16",
        "mixed_precision": true
    },
    "distillation": {
        "teacher_model": "gemma-7b",
        "temperature": 2.0,
        "alpha": 0.5,
        "use_intermediate_distillation": true,
        "intermediate_layer_mapping": "uniform",
        "layer_match_strategy": "progressive",
        "enabled": true,
        "teacher_cache": true,
        "cache_teacher_outputs": true
    },
    "optimization": {
        "use_fp8": true,
        "use_pjit": true,
        "block_size": 128,
        "use_dynamic_scale": true,
        "mixed_precision": true,
        "shard_weights": true,
        "use_flash_attention": true,
        "enable_kv_cache": true,
        "fp8_margin": 0.01,
        "dynamic_scale": true
    },
    "tpu": {
        "tpu_cores": 8,
        "tpu_topology": "2x2x2",
        "device_strategy": "data_parallel",
        "use_bfloat16": true,
        "mesh_shape": {
            "data": 8,
            "model": 1
        },
        "rematerialize": true,
        "sharding": {
            "data_parallel": true,
            "model_parallel": false,
            "sequence_parallel": false
        }
    },
    "tot": {
        "max_branches": 3,
        "max_depth": 3,
        "beam_width": 5,
        "batch_size": 32,
        "temperature": 0.7,
        "cache_kv": true
    },
    "sequence": {
        "max_length": 2048,
        "min_length": 8,
        "pad_token_id": 0,
        "eos_token_id": 1,
        "chunk_size": 128
    },
    "memory": {
        "gradient_checkpointing": true,
        "attention_memory_efficient": true,
        "kv_cache_fp8": true,
        "use_memory_efficient_attention": true
    },
    "logging": {
        "log_every": 100,
        "eval_every": 1000,
        "save_every": 10000,
        "profile_every": 5000
    }
}