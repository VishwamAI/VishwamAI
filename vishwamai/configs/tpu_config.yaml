# TPU Configuration

# TPU Hardware Settings
tpu:
  # TPU pod settings
  topology: "2x2"  # Options: "2x2", "2x2x2", "4x4", "4x4x4"
  chips_per_host: 8
  num_tpu_cores: 8
  num_hosts: 1
  host_bounds: "1,1,1"  # For pod configuration
  
  # Performance settings
  use_bfloat16: true
  compile_ahead: true
  pjrt_distributed: true
  enable_spmd: true

# Data Sharding
data_sharding:
  strategy: "data_parallel"  # Options: "data_parallel", "model_parallel", "expert_parallel"
  batch_partitioning: true
  shuffle_buffer_size: 10000
  prefetch_size: 2

# Expert Sharding (for MoE)
expert_sharding:
  enabled: true
  strategy: "axis_0"  # Options: "axis_0", "axis_1", "replicated"
  recompute_grad: true
  balance_experts: true
  local_dispatch: true

# Memory Optimization
memory:
  gradient_checkpointing: true
  selective_checkpoint: true
  rematerialization: true
  dynamic_shape: true
  optimize_tpu_kernel: true

# Compilation Options
compilation:
  xla_optimization_level: 3
  enable_automatic_fusion: true
  fast_math: true
  shape_specialization: true
  debug_mode: false

# Training Optimizations
training:
  auto_batch_size: false
  max_batch_size: 2048
  dynamic_batch_size: false
  batch_size_finder:
    enabled: false
    init_batch_size: 32
    max_trials: 10
    growth_factor: 1.1

# Distribution Strategy
distribution:
  cross_replica_sync: true
  all_reduce_alg: "ring"  # Options: "ring", "hierarchical"
  gradient_clipping: "global"  # Options: "global", "local"
  replicated_vars_init: "broadcast"

# Monitoring and Profiling
monitoring:
  profile_steps: 100
  trace_steps: 1000
  memory_profile: true
  execution_profile: true
  hlo_cost_analysis: true

# Performance Tuning
performance:
  layout_optimizer: true
  constant_folding: true
  arithmetic_optimization: true
  auto_mixed_precision: true
  tensor_fusion_size: 10485760  # 10MB

# Expert Configuration (for MoE)
expert_config:
  local_expert_count: 2  # Number of experts per TPU core
  expert_capacity_factor: 1.25
  expert_routing:
    jitter_noise: 0.1
    load_balancing: true
    capacity_factor: 1.5
    dispatcher_algorithm: "gates"  # Options: "gates", "tokens"

# Network and Communication
network:
  all_reduce_combine_threshold: 10485760  # 10MB
  collective_backend: "xla"  # Options: "xla", "mpi"
  enable_rewrite: true
  enable_collective_fusion: true

# Debugging and Logging
debug:
  dump_hlo: false
  dump_schedules: false
  log_device_placement: false
  verbosity_level: 0  # 0=ERROR, 1=WARNING, 2=INFO, 3=DEBUG
  profile_dir: "tpu_profiles"
