# Basic training parameters
max_steps: 50000
eval_steps: 100
save_steps: 1000
warmup_steps: 2000
logging_steps: 10

optimizer:
  name: "adamw"
  lr: 2e-4
  end_lr: 1e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  epsilon: 1e-8
  clip_grad_norm: 1.0
  scheduler:
    name: "cosine_with_warmup"
    num_cycles: 1
    power: 1.0

# Advanced training features
gradient_accumulation:
  enabled: true
  steps: 4
  sync_every_step: true

amp:
  enabled: true
  dtype: "bfloat16"
  opt_level: "O2"
  keep_batchnorm_fp32: true
  loss_scale:
    enabled: true
    init_scale: 65536
    scale_window: 2000
    min_scale: 1
    growth_interval: 2000

# Dynamic batching
dynamic_batch_size:
  enabled: true
  initial_batch_size: 32
  target_batch_size: 128
  min_batch_size: 8
  growth_factor: 2
  shrink_factor: 0.5
  stable_steps: 100

# Checkpointing
checkpointing:
  save_optimizer_state: true
  keep_last_n: 5
  save_best: true
  metric: "validation_loss"
  mode: "min"
  save_zero_redundancy: true

# Advanced hyperparameter tuning
tuning:
  optuna:
    sampler:
      name: "tpe"
      n_startup_trials: 10
      consider_prior: true
      prior_weight: 1.0
    pruner:
      name: "hyperband"
      min_resource: 100
      reduction_factor: 3
      min_early_stopping_rate: 0
    parameters:
      lr:
        type: "loguniform"
        low: 1e-5
        high: 1e-3
      warmup_steps:
        type: "int"
        low: 100
        high: 5000
      dropout_rate:
        type: "uniform"
        low: 0.0
        high: 0.3
      weight_decay:
        type: "loguniform"
        low: 1e-5
        high: 1e-1

  ray:
    scheduler:
      name: "async_hyperband"
      max_t: 50000
      grace_period: 1000
      reduction_factor: 3
    search_alg:
      name: "bohb"
      max_concurrent: 4
      nested_size: 3

# Monitoring and profiling
profiling:
  enabled: true
  start_step: 100
  profile_steps: 10
  save_memory_snapshot: true
  trace_handler:
    on_trace_ready: "tb_trace"
    record_shapes: true
    profile_memory: true
    with_stack: true
    with_flops: true
