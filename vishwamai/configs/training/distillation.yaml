defaults:
  - default

distillation:
  enabled: true
  kd_temperature: 2.0
  alpha_kd: 0.5
  alpha_ce: 0.5
  teacher_model:
    path: "model-00001-to-00015-of-00252.safetensors"  # Completed batch after validation/test
    temperature: 2.0
  feature_distillation:
    enabled: true
    layers: [0, 6, 11]  # Layers to match (depends on teacher/student architecture)
    loss_weight: 0.1
  attention_distillation:
    enabled: true
    loss_weight: 0.1
  hidden_distillation:
    enabled: true
    loss_weight: 0.1
  pruning:
    enabled: false
    target_sparsity: 0.5
    pruning_schedule: cubic
    begin_step: 1000
    end_step: 10000
  quantization:
    enabled: false
    precision: "int8"
    calibration_steps: 100

training:
  # Override some training settings for distillation
  optimizer:
    lr: 5e-4  # Higher learning rate for distillation
    warmup_steps: 1000
  max_steps: 20000  # Usually needs fewer steps than full training
  checkpoint:
    completed_batch: "1-15"
    total_safetensors: 252
    progress_percentage: 13
    validation_complete: true
    test_complete: true
