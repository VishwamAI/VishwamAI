defaults:
  - default

training:
  # Basic parameters tuned for TPU
  learning_rate: 0.0001
  warmup_steps: 2000
  max_steps: 500000
  batch_size: 64  # Per device batch size
  gradient_accumulation_steps: 4
  eval_batch_size: 32
  
  # Curriculum learning setup
  curriculum:
    enabled: true
    type: "length"
    initial_length: 64
    length_increment: 32
    update_every: 1000
    
  # TPU optimization
  mixed_precision:
    enabled: true
    dtype: "bfloat16"  # Better for TPU vs float16
    dynamic_scale: true
    initial_scale: 128.0
  
  # Memory optimizations
  gradient_checkpointing: true
  enable_pjit: true
  max_grad_norm: 1.0
  
  # Optimizer settings
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.01
  eps: 1e-6
  eps_root: 1e-6

# Data processing
data:
  max_seq_length: 512
  preprocessing_num_workers: 8
  dataset_name: "openai/gsm8k"
  train_split: "train"
  val_split: "validation"

# Model configuration
model:
  vocab_size: 32000
  hidden_size: 768
  intermediate_size: 3072
  num_hidden_layers: 12
  num_attention_heads: 12
  hidden_act: "gelu"
  dropout_rate: 0.1
  attention_dropout: 0.1
  use_mod: true  # Enable mixture of denoisers
  max_position_embeddings: 512

hydra:
  device:
    type: "tpu"
    num_devices: 8
    precision: "bfloat16"
    memory_allocation: 0.85

monitoring:
  enabled: true
  log_every_n_steps: 100
  save_every_n_steps: 5000
  memory_metrics: true
  profile_steps: 1000

distributed:
  strategy: "data_parallel"
  sync_batch_norm: true
  gradient_as_bucket_view: true
