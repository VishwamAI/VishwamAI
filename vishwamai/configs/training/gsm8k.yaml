model:
  vocab_size: 32000
  hidden_size: 768
  num_layers: 12
  num_attention_heads: 12
  intermediate_size: 2048
  hidden_dropout_prob: 0.1
  attention_dropout_prob: 0.1
  max_position_embeddings: 2048
  layer_norm_eps: 1e-5
  use_cache: true
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
  tie_word_embeddings: true
  gradient_checkpointing: false
  use_flash_attention: true
  use_rope: true
  use_alibi: false
  use_gqa: true
  num_key_value_heads: 4
  dtype: "bfloat16"
  mlp_ratio: 4
  dropout_rate: 0.1
  embed_dropout_rate: 0.1
  use_bias: false
  param_dtype: 'bfloat16'
  precision: 'bfloat16'
  use_tot: true

training:
  seed: 42
  learning_rate: 5e-5
  warmup_steps: 100
  max_steps: 10000
  batch_size: 32
  eval_batch_size: 16
  max_grad_norm: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.01
  use_tot: true
  tot_max_thoughts: 5
  tot_max_depth: 3
  tot_beam_width: 5
  use_error_correction: true
  accum_steps: 4
  save_every: 1000
  log_every: 100
  eval_every: 500
  checkpoint_dir: "checkpoints"
  lr_schedule: "cosine"
  use_curriculum: true
  error_history_size: 100
  error_threshold_percentile: 85.0
  use_ema: false  # Exponential Moving Average for model parameters

data:
  dataset_name: "openai/gsm8k"
  max_seq_length: 1024
  batch_size: 8
  preprocessing_num_workers: 4
  train_split: "train"
  val_split: "test"

monitoring:
  log_every_n_steps: 10

evaluation:
  eval_steps: 500
  batch_size: 4
  num_val_batches: 10

checkpointing:
  dir: "./checkpoints/gsm8k"
  save_every: 1000
  keep_last_n: 3
