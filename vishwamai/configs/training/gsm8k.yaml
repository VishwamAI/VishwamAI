# GSM8K Training Configuration
max_steps: 10000  # Adjusted for dataset size
eval_steps: 50
save_steps: 200
warmup_steps: 500
logging_steps: 10

# Dataset config
dataset:
  name: "gsm8k"
  split: "main"
  max_length: 512
  batch_size: 16
  num_workers: 4

# TPU specific settings
tpu:
  enabled: true
  cores: 8
  topology: "2x2x2"

optimizer:
  name: "adamw"
  lr: 1e-4  # Slightly lower learning rate for fine-tuning
  end_lr: 5e-6
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  epsilon: 1e-8
  clip_grad_norm: 1.0
  scheduler:
    name: "cosine_with_warmup"
    num_cycles: 1
    power: 1.0

# Advanced training features
gradient_accumulation:
  enabled: true
  steps: 4
  sync_every_step: true

amp:
  enabled: true
  dtype: "bfloat16"  # TPU optimized
  opt_level: "O2"
  keep_batchnorm_fp32: true

# Checkpointing with safetensors
checkpointing:
  format: "safetensors"
  save_optimizer_state: true
  keep_last_n: 3
  save_best: true
  metric: "validation_accuracy"
  mode: "max"
  save_zero_redundancy: true

# Math-specific evaluation metrics
evaluation:
  metrics:
    - "accuracy"
    - "exact_match"
  num_beams: 4
  max_new_tokens: 256
  temperature: 0.7

# Memory optimization for TPU
optimization:
  gradient_checkpointing: true
  mixed_precision: true
  dtype: "bfloat16"
  tpu_metrics_debug: true
  xla_compile: true
