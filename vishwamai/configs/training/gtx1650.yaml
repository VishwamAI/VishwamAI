defaults:
  - efficient_pretrain

# Configuration optimized for NVIDIA GTX 1650 (4GB VRAM)
training:
  # Memory-optimized batch settings
  batch_size: 8
  gradient_accumulation_steps: 8
  eval_batch_size: 4
  
  # Precision settings
  mixed_precision:
    enabled: true
    dtype: "float16"
    dynamic_scale: true
    initial_scale: 32.0
  
  # Memory optimization
  gradient_checkpointing: true
  enable_pjit: true
  max_grad_norm: 1.0
  
  # Curriculum learning (memory efficient)
  curriculum:
    enabled: true
    type: "length"
    initial_length: 32  # Start smaller
    length_increment: 16
    update_every: 500
  
  # Optimizer settings
  learning_rate: 0.0001
  warmup_steps: 1000
  max_steps: 100000
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.01
  eps: 1e-6
  eps_root: 1e-6

# Model configuration (reduced size for 4GB VRAM)
model:
  vocab_size: 32000
  hidden_size: 512  # Reduced from 768
  intermediate_size: 2048  # Reduced from 3072
  num_hidden_layers: 8  # Reduced from 12
  num_attention_heads: 8
  hidden_act: "gelu"
  dropout_rate: 0.1
  attention_dropout: 0.1
  use_mod: true
  max_position_embeddings: 512

# Device specific settings
hydra:
  device:
    type: "gpu"
    precision: "float16"
    memory_allocation: 0.85  # Reserve some memory for system
    num_devices: 1

data:
  max_seq_length: 512
  preprocessing_num_workers: 4

monitoring:
  enabled: true
  log_every_n_steps: 50
  save_every_n_steps: 1000
  memory_metrics: true
  profile_steps: 100

distributed:
  strategy: "data_parallel"
  sync_batch_norm: false  # Single GPU setup
