# Advanced distillation configuration combining Tencent-Hunyuan-Large and perplexity-ai/r1-1776
defaults:
  - default

distillation:
  enabled: true
  kd_temperature: 3.0  # Higher temperature for better knowledge transfer
  alpha_kd: 0.7  # Increased weight on knowledge distillation
  alpha_ce: 0.3
  error_threshold: 0.1
  
  # Multi-teacher setup (ensemble distillation)
  teacher_models:
    - path: "tencent/Tencent-Hunyuan-Large"
      weight: 0.5
      temperature: 3.0
      config:
        hidden_size: 8192
        intermediate_size: 22528
        num_attention_heads: 64
        num_layers: 80
        num_key_value_heads: 64
        vocab_size: 100008
        max_position_embeddings: 4096
    
    - path: "perplexity-ai/r1-1776"
      weight: 0.5
      temperature: 2.5
      config:
        hidden_size: 7168
        intermediate_size: 18432
        num_attention_heads: 128
        num_layers: 61
        num_key_value_heads: 128
        vocab_size: 129280
        max_position_embeddings: 163840
  
  student_model:
    config:
      hidden_size: 3072  # Balanced hidden size
      intermediate_size: 12288
      num_attention_heads: 32
      num_layers: 36
      num_key_value_heads: 32
      vocab_size: 130000  # Extended vocabulary to cover both models
      max_position_embeddings: 4096
  
  # Advanced distillation components
  feature_distillation:
    enabled: true
    layers: [0, 8, 16, 24, 32]  # Strategic layers for feature matching
    loss_weight: 0.15
  
  attention_distillation:
    enabled: true
    loss_weight: 0.1
  
  hidden_distillation:
    enabled: true
    loss_weight: 0.15
  
  # Model compression pipeline
  pruning:
    enabled: true
    target_sparsity: 0.3
    begin_step: 2000
    end_step: 20000
    pruning_schedule: "cubic"
  
  quantization:
    enabled: true
    precision: "int8"
    calibration_steps: 100

# Optimized data configuration
data:
  path: "c4"  # Common Crawl dataset
  subset: "en"
  preprocessing:
    max_length: 4096
    batch_size: 32
    num_proc: 32
  
  # Multi-source data mixture
  additional_sources:
    - path: "wikipedia"
      weight: 0.2
    - path: "github-code"
      weight: 0.2
    - path: "arxiv_dataset" 
      weight: 0.1

# TPU-optimized training settings
training:
  batch_size: 32
  gradient_accumulation_steps: 8  # Large effective batch size
  learning_rate: 8e-5
  weight_decay: 0.01
  num_epochs: 1
  max_steps: 100000
  warmup_steps: 2000
  save_every: 5000
  eval_every: 1000
  
  # Error correction and ToT integration
  use_error_correction: true
  use_tot: true
  error_correction_weight: 0.2
  tot_search_strategy: "beam"
  tot_max_thoughts: 3
  tot_beam_width: 3
  error_history_size: 100
  error_threshold_percentile: 85.0

# Advanced hardware optimizations
optimization:
  use_flash_attention: true
  use_gqa: true  # Grouped-Query Attention
  mixed_precision: "bf16"  # BFloat16 for TPU
  gradient_checkpointing: true
  
  # Hardware-specific optimizations
  hardware_optimizations:
    - "FlashMLA"
    - "EPLB"
    - "DualPipe"
    - "DeepGEMM"
    - "DeepEP"
    - "CUTLASS"
    - "3FS"
  
  memory_optimization:
    activation_checkpointing: true
    selective_activation_recomputation: true
    optimize_memory_use: true
    cpu_offloading: false

# Dynamic batching for TPU
dynamic_batch_size:
  enabled: true
  initial_batch_size: 32
  target_batch_size: 192
  min_batch_size: 16
  growth_factor: 2
  shrink_factor: 0.5
  stable_steps: 200

# Evaluation configuration
evaluation:
  eval_batch_size: 64
  num_eval_samples: 2000
  metrics:
    - "perplexity"
    - "accuracy"
    - "memorization"
  
  # Custom metrics for distillation quality
  distillation_metrics:
    - "teacher_student_kl_div"
    - "feature_match_loss"
    - "representation_similarity"

# TPU mesh configuration
tpu_config:
  topology: "2x2x2"  # TPU v3-32 topology
  mesh_shape: [4, 8]  # For data parallelism and model parallelism
  mesh_axes: ["data", "model"]
  dataclass_mesh: [1, 1]  # Equal distribution
  perplexity_metrics: true