# Training configuration

# Basic training settings
training:
  num_epochs: 100
  batch_size: 32
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  seed: 42
  mixed_precision: true
  deterministic: true
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000

# Optimizer settings
optimizer:
  name: adamw
  params:
    lr: 1.0e-4
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0.01
  scheduler:
    name: cosine_warmup
    params:
      warmup_steps: 2000
      total_steps: 100000
      min_lr: 1.0e-5
  gradient_checkpointing: true
  zero_stage: 2

# Loss settings
loss:
  label_smoothing: 0.1
  aux_loss_weight: 0.01
  z_loss_weight: 0.01
  contrastive_loss_weight: 0.0
  expert_balancing_weight: 0.01

# Dataset settings
dataset:
  train_path: data/train
  val_path: data/val
  max_seq_length: 2048
  num_workers: 4
  prefetch_factor: 2
  shuffle_buffer_size: 10000
  cache_dir: .cache
  preprocessing:
    lowercase: false
    remove_special_chars: false
    normalize_unicode: true

# Distributed training
distributed:
  backend: nccl
  world_size: 8
  find_unused_parameters: false
  ddp_timeout: 1800
  sync_batch_norm: true
  gradient_as_bucket_view: true
  expert_parallel: true
  pipeline_parallel_size: 1
  tensor_parallel_size: 1

# TPU specific settings
tpu:
  enabled: true
  num_cores: 8
  topology: 2x2x2
  optimization_level: 3
  device_placement: balanced
  profiler_port: 8466

# Expert sharding settings
expert_sharding:
  strategy: uniform
  recompute_experts: true
  expert_capacity_factor: 1.25
  min_expert_capacity: 4
  expert_padding: none
  load_balancing: true
  jitter_noise: 0.1
  expert_streaming: true

# Checkpointing
checkpointing:
  save_path: checkpoints
  save_top_k: 3
  monitor: val_loss
  mode: min
  save_last: true
  save_optimizer: true
  backup_dir: gs://model-backups

# Early stopping
early_stopping:
  enabled: true
  monitor: val_loss
  mode: min
  patience: 5
  min_delta: 1.0e-4
  check_finite: true

# Logging
logging:
  path: logs
  project: vishwamai
  entity: research
  log_level: INFO
  monitor_memory: true
  profile_steps: [100, 200]
  save_code: true
  metrics:
    - loss
    - perplexity
    - expert_metrics
    - memory_metrics
    - throughput

# Debug settings
debug:
  enabled: false
  detect_anomaly: false
  profile_memory: false
  trace_outputs: false
  break_on_error: false
  verbose: false

# Profiling
profiling:
  enabled: true
  trace_memory: true
  trace_cuda: true
  trace_cpu: true
  export_chrome_trace: true
  profile_Expert_routing: true
  profile_attention: true
  record_shapes: true
  record_flops: true
  warmup_steps: 10
  active_steps: 100
  repeat: 3

# Hardware optimization
hardware:
  use_flash_attention: true
  use_fused_adam: true
  use_fused_layer_norm: true
  use_kernel_optimizations: true
  cudnn_benchmark: true
  cudnn_deterministic: false
  pin_memory: true
  async_prefetch: true
  optimize_memory_use: true
  optimize_device_placement: true
