{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Advanced Pre-training on A100 GPUs\n",
    "\n",
    "This notebook implements advanced pre-training for VishwamAI using:\n",
    "\n",
    "- Mixed precision FP8/FP16 training\n",
    "- Fully Sharded Data Parallel (FSDP)\n",
    "- Gradient checkpointing\n",
    "- Efficient memory management\n",
    "- Multi-dataset curriculum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def setup_environment():\n",
    "    # Get current working directory\n",
    "    original_path = os.getcwd()\n",
    "    \n",
    "    # Clone VishwamAI repository if not exists\n",
    "    if not os.path.exists('VishwamAI'):\n",
    "        print(\"Cloning VishwamAI repository...\")\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/VishwamAI/VishwamAI.git'], check=True)\n",
    "    \n",
    "    # Change to VishwamAI directory\n",
    "    os.chdir('VishwamAI')\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    \n",
    "    # Install requirements\n",
    "    print(\"Installing requirements...\")\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)\n",
    "    \n",
    "    # Install additional dependencies for distributed training\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'accelerate', 'datasets', 'transformers'], check=True)\n",
    "    \n",
    "    # Add repository to Python path\n",
    "    sys.path.append(os.getcwd())\n",
    "    \n",
    "    # Create necessary directories\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    print(\"Setup complete!\")\n",
    "    return original_path\n",
    "\n",
    "original_path = setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset handling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Distributed training\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# VishwamAI imports\n",
    "from vishwamai.model import VishwamAI, ModelConfig\n",
    "from vishwamai.training.advanced_training import AdvancedTrainer\n",
    "from vishwamai.utils.config import TrainingConfig\n",
    "from vishwamai.data.dataset import create_combined_dataset\n",
    "from vishwamai.utils.logging import PretrainingLogger\n",
    "from vishwamai.utils.checkpoint import CheckpointManager\n",
    "from vishwamai.utils.hub_utils import HuggingFaceUploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model and training parameters\n",
    "model_config = ModelConfig(\n",
    "    vocab_size=64000,\n",
    "    hidden_size=8192,\n",
    "    num_layers=120,\n",
    "    num_heads=64,\n",
    "    intermediate_size=32768,\n",
    "    max_position_embeddings=32768,\n",
    "    use_moe=True,\n",
    "    num_experts=8,\n",
    "    use_memory=True,\n",
    "    memory_size=4096,\n",
    "    enable_emergent=True,\n",
    "    tree_search_depth=3\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=2000,\n",
    "    max_grad_norm=1.0,\n",
    "    fp8_training=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging and load config\n",
    "logger = PretrainingLogger('configs/pretrain_config.yaml')\n",
    "\n",
    "# Monitor GPU stats\n",
    "def log_gpu_stats():\n",
    "    stats = {\n",
    "        'gpu_memory_used': torch.cuda.memory_allocated() / 1e9,  # GB\n",
    "        'gpu_memory_cached': torch.cuda.memory_reserved() / 1e9,  # GB\n",
    "        'gpu_utilization': torch.cuda.utilization()\n",
    "    }\n",
    "    logger.log_hardware_stats(stats)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distributed training\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision='fp8',\n",
    "    gradient_accumulation_steps=training_config.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "datasets = [\n",
    "    ('openai/gsm8k', 'main'),\n",
    "    ('cais/mmlu', 'all'),\n",
    "    ('TIGER-Lab/MMLU-Pro', 'main'),\n",
    "    ('deepmind/math_dataset', 'algebra'),\n",
    "    ('wikimedia/wikipedia', '20231101.en'),\n",
    "    ('HuggingFace/c4', 'en'),\n",
    "    ('sentence-transformers/codesearchnet', 'all')\n",
    "]\n",
    "\n",
    "train_dataset = create_combined_dataset(datasets)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = VishwamAI(model_config)\n",
    "\n",
    "# Wrap model in FSDP\n",
    "model = FSDP(\n",
    "    model,\n",
    "    auto_wrap_policy=transformer_auto_wrap_policy,\n",
    "    mixed_precision=True,\n",
    "    device_id=torch.cuda.current_device()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    config=model_config,\n",
    "    training_config=training_config,\n",
    "    use_tree_search=True\n",
    ")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    compression=True,\n",
    "    shard_size=1024*1024*1024  # 1GB shards\n",
    ")\n",
    "\n",
    "# Initialize HuggingFace uploader\n",
    "hub_uploader = HuggingFaceUploader(\n",
    "    repo_id=\"VishwamAI/VishwamAI\",  # Updated organization/repo path\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    "    private=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository access\n",
    "try:\n",
    "    hub_uploader.api.repo_info(repo_id=\"VishwamAI/VishwamAI\")\n",
    "    print(\"Successfully connected to HuggingFace repository\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing repository: {e}\")\n",
    "    print(\"Please ensure you have correct access rights to the VishwamAI organization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Log GPU stats before training\n",
    "        log_gpu_stats()\n",
    "        \n",
    "        # Train epoch\n",
    "        metrics = trainer.train_epoch(\n",
    "            dataloader=train_loader,\n",
    "            epoch=epoch,\n",
    "            use_curriculum=True,\n",
    "            checkpoint_dir='checkpoints'\n",
    "        )\n",
    "        \n",
    "        # Log metrics\n",
    "        logger.log_metrics(metrics, step=epoch)\n",
    "        \n",
    "        # Save checkpoint and upload to Hub\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint_path = f\"checkpoints/model_epoch_{epoch+1}\"\n",
    "            \n",
    "            # Save local checkpoint\n",
    "            checkpoint_manager.save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=trainer.optimizer,\n",
    "                filepath=checkpoint_path,\n",
    "                extra_data={\n",
    "                    'epoch': epoch,\n",
    "                    'metrics': metrics\n",
    "                },\n",
    "                quantize=True\n",
    "            )\n",
    "            \n",
    "            # Upload to HuggingFace Hub\n",
    "            hub_uploader.upload_checkpoint(\n",
    "                checkpoint_path=checkpoint_path,\n",
    "                commit_message=f\"Upload model checkpoint for epoch {epoch+1}\",\n",
    "                epoch=epoch+1,\n",
    "                metrics=metrics\n",
    "            )\n",
    "            \n",
    "            # Upload metrics separately\n",
    "            hub_uploader.upload_metrics(metrics, epoch+1)\n",
    "            \n",
    "            # Log checkpoint\n",
    "            logger.log_checkpoint(checkpoint_path, epoch)\n",
    "            \n",
    "            # Clean up local checkpoint to save space\n",
    "            if epoch > 2:  # Keep only last 2 checkpoints locally\n",
    "                old_checkpoint = f\"checkpoints/model_epoch_{epoch-1}\"\n",
    "                if os.path.exists(old_checkpoint):\n",
    "                    os.remove(old_checkpoint)\n",
    "                    \n",
    "except Exception as e:\n",
    "    logger.log_error(e)\n",
    "    raise e\n",
    "finally:\n",
    "    logger.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on benchmark datasets\n",
    "from vishwamai.evaluation import evaluate_model\n",
    "\n",
    "eval_datasets = [\n",
    "    ('MMMU/MMMU', 'validation'),\n",
    "    ('google/IFEval', 'main'),\n",
    "    ('microsoft/SCBench', 'evaluation'),\n",
    "    ('princeton-nlp/SWE-bench', 'test')\n",
    "]\n",
    "\n",
    "results = evaluate_model(\n",
    "    model=model,\n",
    "    datasets=eval_datasets,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for dataset, metrics in results.items():\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-Student Training Setup\n",
    "from vishwamai.training.knowledge_distillation import DistillationTrainer\n",
    "\n",
    "# Load teacher model (pre-trained VishwamAI)\n",
    "teacher_model = VishwamAI(model_config)\n",
    "teacher_model.load_state_dict(torch.load(\"checkpoints/model_epoch_10.pt\"))\n",
    "\n",
    "# Initialize smaller student model\n",
    "student_config = model_config.copy()\n",
    "student_config.num_layers //= 2\n",
    "student_config.hidden_size //= 2\n",
    "student_model = VishwamAI(student_config)\n",
    "\n",
    "# Setup distillation trainer\n",
    "distill_trainer = DistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    temperature=2.0,\n",
    "    alpha=0.5  # Balance between distillation and task loss\n",
    ")\n",
    "\n",
    "# Train student model\n",
    "distill_trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    num_epochs=5,\n",
    "    checkpoint_dir=\"student_checkpoints\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
