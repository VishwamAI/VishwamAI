{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress tracking setup\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Operation completed in {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install Git LFS\n",
    "!apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Verify GPU availability and requirements\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Memory optimization for T4\n",
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"Using GPU: {gpu_name}\")\n",
    "\n",
    "# Set memory optimization flags for T4\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Package installation with T4 optimized versions\n",
    "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "    transformers==4.34.0 datasets accelerate huggingface_hub wandb bitsandbytes -q\n",
    "%pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import login, create_repo\n",
    "from getpass import getpass\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Get token securely\n",
    "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
    "login(token=hf_token)\n",
    "print(\"Successfully logged in to Hugging Face!\")\n",
    "\n",
    "# Initialize W&B for experiment tracking\n",
    "wandb.login()\n",
    "print(\"Successfully logged in to Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Repository setup with Git LFS\n",
    "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
    "%cd VishwamAI\n",
    "%pip install -e . -q\n",
    "\n",
    "# Create Git LFS configuration\n",
    "%%writefile .lfsconfig\n",
    "[lfs]\n",
    "    url = https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
    "    pushurl = https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
    "[lfs \"transfer\"]\n",
    "    maxretries = 3\n",
    "    maxverifies = 3\n",
    "    maxconcurrenttransfers = 8\n",
    "[filter \"lfs\"]\n",
    "    clean = git-lfs clean -- %f\n",
    "    smudge = git-lfs smudge -- %f\n",
    "    process = git-lfs filter-process\n",
    "    required = true\n",
    "\n",
    "# Create .gitattributes for LFS\n",
    "%%writefile .gitattributes\n",
    "# Model files\n",
    "*.bin filter=lfs diff=lfs merge=lfs -text\n",
    "*.pt filter=lfs diff=lfs merge=lfs -text\n",
    "*.pth filter=lfs diff=lfs merge=lfs -text\n",
    "*.ckpt filter=lfs diff=lfs merge=lfs -text\n",
    "*.safetensors filter=lfs diff=lfs merge=lfs -text\n",
    "\n",
    "# Dataset files\n",
    "*.parquet filter=lfs diff=lfs merge=lfs -text\n",
    "*.arrow filter=lfs diff=lfs merge=lfs -text\n",
    "*.hdf5 filter=lfs diff=lfs merge=lfs -text\n",
    "*.h5 filter=lfs diff=lfs merge=lfs -text\n",
    "\n",
    "# Other large files\n",
    "*.tar.gz filter=lfs diff=lfs merge=lfs -text\n",
    "*.tgz filter=lfs diff=lfs merge=lfs -text\n",
    "*.zip filter=lfs diff=lfs merge=lfs -text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from vishwamai.model_utils import load_model, get_gpu_memory\n",
    "from vishwamai.model import Transformer, ModelArgs\n",
    "from vishwamai.cache_augmentation import CacheConfig, DifferentiableCacheAugmentation\n",
    "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
    "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
    "from vishwamai.reward_function import RewardConfig\n",
    "from vishwamai.trainer import VishwamAIPretrainer\n",
    "\n",
    "# T4-specific performance optimizations\n",
    "import bitsandbytes as bnb\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def setup_hardware():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = get_gpu_memory()\n",
    "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "\n",
    "    # T4-optimized configuration\n",
    "    if 't4' in gpu_name.lower():\n",
    "        variant = \"7B\"  # T4-optimized model\n",
    "        print(\"Using T4-optimized configuration with 8-bit quantization\")\n",
    "    else:\n",
    "        variant = \"167B\"  # Fallback configuration\n",
    "        print(\"Using fallback configuration\")\n",
    "        \n    clear_gpu_memory()\n",
    "    return variant\n",
    "\n",
    "model_variant = setup_hardware()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def load_config():\n",
    "    config_path = \"./vishwamai/configs/config_optimized.json\"\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    if model_variant not in config[\"model_variants\"]:\n",
    "        print(f\"Warning: Model variant '{model_variant}' not found in config, creating T4 optimized config\")\n",
    "        # T4-optimized configuration\n",
    "        t4_config = {\n",
    "            \"max_batch_size\": 4,\n",
    "            \"max_seq_len\": 2048,\n",
    "            \"dtype\": \"fp8\",\n",
    "            \"vocab_size\": 32000,\n",
    "            \"dim\": 1024,\n",
    "            \"inter_dim\": 2816,\n",
    "            \"moe_inter_dim\": 512,\n",
    "            \"n_layers\": 12,\n",
    "            \"n_dense_layers\": 1,\n",
    "            \"n_heads\": 16,\n",
    "            \"n_routed_experts\": 8,\n",
    "            \"n_shared_experts\": 1,\n",
    "            \"n_activated_experts\": 2,\n",
    "            \"n_expert_groups\": 1,\n",
    "            \"n_limited_groups\": 1,\n",
    "            \"score_func\": \"softmax\",\n",
    "            \"route_scale\": 1.0,\n",
    "            \"q_lora_rank\": 0,\n",
    "            \"kv_lora_rank\": 64,\n",
    "            \"qk_nope_head_dim\": 64,\n",
    "            \"qk_rope_head_dim\": 32,\n",
    "            \"v_head_dim\": 64,\n",
    "            \"original_seq_len\": 2048,\n",
    "            \"rope_theta\": 10000.0,\n",
    "            \"rope_factor\": 20,\n",
    "            \"beta_fast\": 16,\n",
    "            \"beta_slow\": 1,\n",
    "            \"mscale\": 0.5,\n",
    "            \"use_alibi\": False,  # Disable ALiBi for T4\n",
    "            \"use_rope_scaling\": True,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"parallel_attn\": True,\n",
    "            \"rope_condense_ratio\": 1.0\n",
    "        }\n",
    "        return t4_config\n",
    "    \n",
    "    return config[\"model_variants\"][model_variant][\"model_config\"]\n",
    "\n",
    "# Load configuration\n",
    "model_config = load_config()\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepSpeed config for T4 optimization\n",
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True,\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 100,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        }\n",
    "    },\n",
    "    \"train_batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"train_micro_batch_size_per_gpu\": 2,\n",
    "    \"gradient_clipping\": 0.5,\n",
    "    \"steps_per_print\": 10,\n",
    "    \"wall_clock_breakdown\": False\n",
    "}\n",
    "\n",
    "with open('ds_config.json', 'w') as f:\n",
    "    json.dump(ds_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def initialize_components():\n",
    "    print(\"Initializing model and components...\")\n",
    "    clear_gpu_memory()\n",
    "    \n    # Initialize main model with 8-bit quantization for T4\n",
    "    model_args = ModelArgs(**model_config)\n",
    "    model = Transformer(model_args)\n",
    "    model = bnb.nn.LinearWrapper.wrap_model(model, device='cuda', quantize=True)\n",
    "    \n    # Initialize smaller cache augmentation for T4\n",
    "    cache_config = CacheConfig(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"],\n",
    "        max_cache_length=8192,  # Reduced cache size for T4\n",
    "        dropout=0.1\n",
    "    )\n",
    "    cache_module = DifferentiableCacheAugmentation(cache_config).cuda()\n",
    "    \n    # Initialize memory transformer with reduced size\n",
    "    memory_module = ReasoningMemoryTransformer(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"]\n",
    "    ).cuda()\n",
    "    \n    # Initialize tree of thoughts with reduced beam size\n",
    "    tree_module = TreeOfThoughts(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"]\n",
    "    ).cuda()\n",
    "    \n    # Initialize reward config\n",
    "    reward_config = RewardConfig(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"]\n",
    "    )\n",
    "    \n    clear_gpu_memory()\n",
    "    return model, cache_module, memory_module, tree_module, reward_config\n",
    "\n",
    "model, cache_module, memory_module, tree_module, reward_config = initialize_components()\n",
    "\n",
    "print(f\"\\nModel size: {sum(p.numel() for p in model.parameters())/1e9:.1f}B parameters\")\n",
    "print(f\"Sequence length: {model_config['max_seq_len']:,} tokens\")\n",
    "print(f\"Number of experts: {model_config['n_routed_experts']} routed + {model_config['n_shared_experts']} shared\")\n",
    "print(f\"Active experts per token: {model_config['n_activated_experts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Initialize output directory\n",
    "output_dir = \"./pretrain_output\"\n",
    "!mkdir -p $output_dir\n",
    "\n",
    "# Configure training with T4 optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size for T4\n",
    "    gradient_accumulation_steps=16,  # Increased for T4 memory constraints\n",
    "    learning_rate=5e-5,  # Reduced learning rate\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # Mixed precision training\n",
    "    fp16=True,  # Use FP16 instead of BF16 for T4\n",
    "    bf16=False,\n",
    "    # Performance optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,  # Reduced workers for T4\n",
    "    dataloader_pin_memory=True,\n",
    "    group_by_length=True,\n",
    "    # Memory optimizations\n",
    "    max_grad_norm=0.5,  # Reduced for stability\n",
    "    # Monitoring\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    # Hub integration\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
    "    hub_strategy=\"end\",  # Only save at the end to save memory\n",
    "    # Optimizer settings\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",  # Use 8-bit Adam\n",
    "    # Other settings\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    # Memory optimization\n",
    "    deepspeed=\"ds_config.json\"  # Using the config we created\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Load and combine training datasets with memory optimization\n",
    "def load_dataset_with_memory_optimization(ds_name, split):\n",
    "    clear_gpu_memory()\n",
    "    try:\n",
    "        dataset = load_dataset(ds_name, split=split, streaming=True)  # Use streaming for memory efficiency\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {ds_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "train_datasets = []\n",
    "for ds_name in [\"gsm8k\", \"cais/mmlu\"]:\n",
    "    dataset = load_dataset_with_memory_optimization(ds_name, \"train\")\n",
    "    if dataset is not None:\n",
    "        train_datasets.append(dataset)\n",
    "\n",
    "if not train_datasets:\n",
    "    raise ValueError(\"No training datasets could be loaded\")\n",
    "\n",
    "combined_train_dataset = concatenate_datasets(train_datasets)\n",
    "\n",
    "# Load validation dataset\n",
    "eval_dataset = load_dataset_with_memory_optimization(\"cais/mmlu\", \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with all components\n",
    "trainer = VishwamAIPretrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    memory_module=memory_module,\n",
    "    tree_module=tree_module,\n",
    "    cache_module=cache_module,\n",
    "    reward_config=reward_config\n",
    ")\n",
    "\n",
    "# Start training with periodic memory cleanup\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Training with memory management\n",
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"Training interrupted: {e}\")\n",
    "    clear_gpu_memory()\n",
    "    raise e\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time/3600:.2f} hours\")\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def save_model():\n",
    "    clear_gpu_memory()\n",
    "    model_save_path = \"final_model\"\n",
    "    trainer.save_model(model_save_path)\n",
    "    \n    # Initialize Git LFS tracking for the saved model files\n",
    "    !git lfs track \"final_model/*.bin\"\n",
    "    !git lfs track \"final_model/*.pt\"\n",
    "    !git lfs track \"final_model/*.pth\"\n",
    "    \n    print(\"Model and components saved successfully\")\n",
    "    return model_save_path\n",
    "\n",
    "model_save_path = save_model()\n",
    "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def validate_model():\n",
    "    clear_gpu_memory()\n",
    "    # Load all components for validation with 8-bit quantization\n",
    "    test_model = Transformer(ModelArgs(**model_config))\n",
    "    test_model = bnb.nn.LinearWrapper.wrap_model(test_model, device='cuda', quantize=True)\n",
    "    test_model.load_state_dict(torch.load(f\"{model_save_path}/pytorch_model.bin\"))\n",
    "    \n    # Load auxiliary components\n",
    "    test_cache = DifferentiableCacheAugmentation.from_pretrained(model_save_path)\n",
    "    test_memory = ReasoningMemoryTransformer.from_pretrained(model_save_path)\n",
    "    test_tree = TreeOfThoughts.from_pretrained(model_save_path)\n",
    "    \n    test_model.eval()\n",
    "    test_cache.eval()\n",
    "    test_memory.eval()\n",
    "    test_tree.eval()\n",
    "\n",
    "    test_cases = [\n",
    "        \"What is 7 * 12?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"Write a Python function to find prime numbers.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Running validation tests...\")\n",
    "    for test_input in test_cases:\n",
    "        print(f\"\\nTest: {test_input}\")\n",
    "        clear_gpu_memory()\n",
    "        # Note: You'll need to implement tokenization for the actual input\n",
    "        tokens = torch.randint(0, model_config['vocab_size'], (1, 32)).cuda()\n",
    "        \n        with torch.inference_mode():\n",
    "            start = time.time()\n",
    "            output = test_model(tokens)\n",
    "            end = time.time()\n",
    "            \n            # Apply enhancements with memory management\n",
    "            enhanced_states = test_cache(output)\n",
    "            memory_enhanced = test_memory(enhanced_states)\n",
    "            final_output = test_tree(memory_enhanced)\n",
    "            \n        print(f\"Generated response in {end-start:.2f}s\")\n",
    "        # Note: You'll need to implement detokenization for the actual output\n",
    "\n",
    "validate_model()\n",
    "print(\"\\nPretraining and validation completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
