{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress tracking setup\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Operation completed in {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Verify GPU availability and requirements\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "if 'A100' not in gpu_name:\n",
    "    print(\"⚠️ Warning: This model requires an A100 GPU for optimal performance\")\n",
    "    print(\"Current GPU:\", gpu_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Package installation\n",
    "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "    transformers==4.34.0 datasets accelerate huggingface_hub wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import login, create_repo\n",
    "from getpass import getpass\n",
    "import wandb\n",
    "\n",
    "# Get token securely\n",
    "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
    "login(token=hf_token)\n",
    "print(\"Successfully logged in to Hugging Face!\")\n",
    "\n",
    "# Initialize W&B for experiment tracking\n",
    "wandb.login()\n",
    "print(\"Successfully logged in to Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Repository setup\n",
    "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
    "%cd VishwamAI\n",
    "%pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from vishwamai.model_utils import load_model, get_gpu_memory\n",
    "from vishwamai.model import Transformer, ModelArgs\n",
    "from vishwamai.cache_augmentation import CacheConfig, DifferentiableCacheAugmentation\n",
    "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
    "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
    "from vishwamai.reward_function import RewardConfig\n",
    "from vishwamai.trainer import VishwamAIPretrainer\n",
    "\n",
    "# Performance optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def setup_hardware():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = get_gpu_memory()\n",
    "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "\n",
    "    # Optimize for available GPU\n",
    "    if 'a100' in gpu_name.lower():\n",
    "        variant = \"671B\"  # Full model\n",
    "    elif 'v100' in gpu_name.lower():\n",
    "        variant = \"335B\"  # Reduced size\n",
    "    else:\n",
    "        variant = \"167B\"  # Minimal configuration\n",
    "    return variant\n",
    "\n",
    "model_variant = setup_hardware()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def load_config():\n",
    "    config_path = \"./vishwamai/configs/config_optimized.json\"\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    if model_variant not in config[\"model_variants\"]:\n",
    "        raise KeyError(f\"Model variant '{model_variant}' not found in config\")\n",
    "    \n",
    "    return config[\"model_variants\"][model_variant][\"model_config\"]\n",
    "\n",
    "# Load configuration\n",
    "model_config = load_config()\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def initialize_components():\n",
    "    print(\"Initializing model and components...\")\n",
    "    \n    # Initialize main model\n",
    "    model_args = ModelArgs(\n",
    "        max_batch_size=model_config[\"max_batch_size\"],\n",
    "        max_seq_len=model_config[\"max_seq_len\"],\n",
    "        dtype=model_config[\"dtype\"],\n",
    "        vocab_size=model_config[\"vocab_size\"],\n",
    "        dim=model_config[\"dim\"],\n",
    "        inter_dim=model_config[\"inter_dim\"],\n",
    "        moe_inter_dim=model_config[\"moe_inter_dim\"],\n",
    "        n_layers=model_config[\"n_layers\"],\n",
    "        n_dense_layers=model_config[\"n_dense_layers\"],\n",
    "        n_heads=model_config[\"n_heads\"],\n",
    "        n_routed_experts=model_config[\"n_routed_experts\"],\n",
    "        n_shared_experts=model_config[\"n_shared_experts\"],\n",
    "        n_activated_experts=model_config[\"n_activated_experts\"],\n",
    "        n_expert_groups=model_config[\"n_expert_groups\"],\n",
    "        n_limited_groups=model_config[\"n_limited_groups\"],\n",
    "        score_func=model_config[\"score_func\"],\n",
    "        route_scale=model_config[\"route_scale\"],\n",
    "        q_lora_rank=model_config[\"q_lora_rank\"],\n",
    "        kv_lora_rank=model_config[\"kv_lora_rank\"],\n",
    "        qk_nope_head_dim=model_config[\"qk_nope_head_dim\"],\n",
    "        qk_rope_head_dim=model_config[\"qk_rope_head_dim\"],\n",
    "        v_head_dim=model_config[\"v_head_dim\"],\n",
    "        original_seq_len=model_config[\"original_seq_len\"],\n",
    "        rope_theta=model_config[\"rope_theta\"],\n",
    "        rope_factor=model_config[\"rope_factor\"],\n",
    "        beta_fast=model_config[\"beta_fast\"],\n",
    "        beta_slow=model_config[\"beta_slow\"],\n",
    "        mscale=model_config[\"mscale\"]\n",
    "    )\n",
    "    \n",
    "    model = Transformer(model_args).cuda()\n",
    "    \n    # Initialize cache augmentation\n",
    "    cache_config = CacheConfig(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"],\n",
    "        max_cache_length=65536,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    cache_module = DifferentiableCacheAugmentation(cache_config).cuda()\n",
    "    \n    # Initialize memory transformer\n",
    "    memory_module = ReasoningMemoryTransformer(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"]\n",
    "    ).cuda()\n",
    "    \n    # Initialize tree of thoughts\n",
    "    tree_module = TreeOfThoughts(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"]\n",
    "    ).cuda()\n",
    "    \n    # Initialize reward config\n",
    "    reward_config = RewardConfig(\n",
    "        hidden_size=model_config[\"dim\"],\n",
    "        num_heads=model_config[\"n_heads\"]\n",
    "    )\n",
    "    \n    return model, cache_module, memory_module, tree_module, reward_config\n",
    "\n",
    "model, cache_module, memory_module, tree_module, reward_config = initialize_components()\n",
    "\n",
    "print(f\"\\nModel size: {sum(p.numel() for p in model.parameters())/1e9:.1f}B parameters\")\n",
    "print(f\"Sequence length: {model_config['max_seq_len']:,} tokens\")\n",
    "print(f\"Number of experts: {model_config['n_routed_experts']} routed + {model_config['n_shared_experts']} shared\")\n",
    "print(f\"Active experts per token: {model_config['n_activated_experts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Initialize output directory\n",
    "output_dir = \"./pretrain_output\"\n",
    "!mkdir -p $output_dir\n",
    "\n",
    "# Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=model_config['max_batch_size'],\n",
    "    gradient_accumulation_steps=8,  # Adjust based on GPU memory\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1000,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # Mixed precision training\n",
    "    bf16=True,  # Use bfloat16\n",
    "    fp16=False,\n",
    "    # Performance optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    group_by_length=True,\n",
    "    # Monitoring\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    # Hub integration\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    # Optimizer settings\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    # Other settings\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    "    ddp_find_unused_parameters=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Load and combine training datasets\n",
    "train_datasets = []\n",
    "for ds_name in [\"gsm8k\", \"cais/mmlu\"]:\n",
    "    try:\n",
    "        dataset = load_dataset(ds_name, split=\"train\")\n",
    "        train_datasets.append(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {ds_name}: {e}\")\n",
    "\n",
    "if not train_datasets:\n",
    "    raise ValueError(\"No training datasets could be loaded\")\n",
    "\n",
    "combined_train_dataset = concatenate_datasets(train_datasets)\n",
    "\n",
    "# Load validation dataset\n",
    "try:\n",
    "    eval_dataset = load_dataset(\"cais/mmlu\", split=\"validation\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load validation dataset: {e}\")\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with all components\n",
    "trainer = VishwamAIPretrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    memory_module=memory_module,\n",
    "    tree_module=tree_module,\n",
    "    cache_module=cache_module,\n",
    "    reward_config=reward_config\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def save_model():\n",
    "    model_save_path = \"final_model\"\n",
    "    trainer.save_model(model_save_path)\n",
    "    print(\"Model and components saved successfully\")\n",
    "    return model_save_path\n",
    "\n",
    "model_save_path = save_model()\n",
    "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_time\n",
    "def validate_model():\n",
    "    # Load all components for validation\n",
    "    test_model = Transformer(ModelArgs(**model_config)).cuda()\n",
    "    test_model.load_state_dict(torch.load(f\"{model_save_path}/pytorch_model.bin\"))\n",
    "    \n    # Load auxiliary components\n",
    "    test_cache = DifferentiableCacheAugmentation.from_pretrained(model_save_path)\n",
    "    test_memory = ReasoningMemoryTransformer.from_pretrained(model_save_path)\n",
    "    test_tree = TreeOfThoughts.from_pretrained(model_save_path)\n",
    "    \n    test_model.eval()\n",
    "    test_cache.eval()\n",
    "    test_memory.eval()\n",
    "    test_tree.eval()\n",
    "\n",
    "    test_cases = [\n",
    "        \"What is 7 * 12?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"Write a Python function to find prime numbers.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Running validation tests...\")\n",
    "    for test_input in test_cases:\n",
    "        print(f\"\\nTest: {test_input}\")\n",
    "        # Note: You'll need to implement tokenization for the actual input\n",
    "        tokens = torch.randint(0, model_config['vocab_size'], (1, 32)).cuda()\n",
    "        \n        with torch.inference_mode():\n",
    "            start = time.time()\n",
    "            output = test_model(tokens)\n",
    "            end = time.time()\n",
    "            \n            # Apply enhancements\n",
    "            enhanced_states = test_cache(output)\n",
    "            memory_enhanced = test_memory(enhanced_states)\n",
    "            final_output = test_tree(memory_enhanced)\n",
    "            \n        print(f\"Generated response in {end-start:.2f}s\")\n",
    "        # Note: You'll need to implement detokenization for the actual output\n",
    "\n",
    "validate_model()\n",
    "print(\"\\nPretraining and validation completed!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
