{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guIJHEO3HjY9",
        "outputId": "783448d1-b8bc-4773-8ce5-70f38f3e9df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Git LFS initialized.\n",
            "Cloning into 'VishwamAI'...\n",
            "remote: Enumerating objects: 1072, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 1072 (delta 8), reused 12 (delta 3), pack-reused 1031 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1072/1072), 28.52 MiB | 10.47 MiB/s, done.\n",
            "Resolving deltas: 100% (541/541), done.\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "Encountered 2 file(s) that should have been pointers, but weren't:\n",
            "\tgsm8k/test-00000-of-00001.parquet\n",
            "\tgsm8k/train-00000-of-00001.parquet\n",
            "/content/VishwamAI\n",
            "\u001b[31mERROR: file:///content/VishwamAI does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mWARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.bin\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.pt\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.pth\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.ckpt\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.safetensors\" already supported\n"
          ]
        }
      ],
      "source": [
        "# Install Git LFS\n",
        "!apt-get install git-lfs -y\n",
        "!git lfs install\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
        "%cd VishwamAI\n",
        "\n",
        "# Install the package\n",
        "!pip install -e . -q\n",
        "\n",
        "\n",
        "# Configure Git LFS\n",
        "!git config lfs.url https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
        "!git config lfs.pushurl https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
        "\n",
        "# Set up Git LFS tracking\n",
        "!git lfs track \"*.bin\"\n",
        "!git lfs track \"*.pt\"\n",
        "!git lfs track \"*.pth\"\n",
        "!git lfs track \"*.ckpt\"\n",
        "!git lfs track \"*.safetensors\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wja3KBfdoELM",
        "outputId": "000e1538-4f79-4c75-e6fd-e113f46ed02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Feb 17 04:16:58 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Using GPU: Tesla T4\n",
            "CPU times: user 1.66 s, sys: 351 ms, total: 2.01 s\n",
            "Wall time: 3.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Verify GPU availability and requirements\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Memory optimization for T4\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "print(f\"Using GPU: {gpu_name}\")\n",
        "\n",
        "# Set memory optimization flags for T4\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8ucxo0oELN",
        "outputId": "f1e58242-32ff-4211-c43f-3f77180f5a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m427.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.34.0 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.34.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting deepspeed\n",
            "  Downloading deepspeed-0.16.3.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->deepspeed)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->deepspeed)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->deepspeed)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->deepspeed)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->deepspeed)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->deepspeed)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.3-py3-none-any.whl size=1550059 sha256=603af916e9635366607aafa74014c1083cad62035ba17eb15c972c93fe5645d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/dc/d4/7e7e07b11bc7c0e2a1a495b967acf58de61261eed4596fb23b\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: nvidia-ml-py, hjson, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deepspeed\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed deepspeed-0.16.3 hjson-3.1.0 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py-12.570.86 nvidia-nvjitlink-cu12-12.4.127\n",
            "CPU times: user 1.46 s, sys: 216 ms, total: 1.68 s\n",
            "Wall time: 2min 51s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Package installation with T4 optimized versions\n",
        "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
        "    transformers==4.34.0 datasets accelerate huggingface_hub wandb bitsandbytes -q\n",
        "%pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-KlPRYJoELL",
        "outputId": "477505cd-e3e4-4c55-98cb-4cde30bc7696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "CPU times: user 29.9 ms, sys: 2.8 ms, total: 32.7 ms\n",
            "Wall time: 2.32 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Install Git LFS\n",
        "!apt-get install git-lfs\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlmaEAzJrTHy",
        "outputId": "e6b9f35c-3d09-461a-97df-5fbbf68d8600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.2 datasets-3.3.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ou4YTR5UoELI"
      },
      "outputs": [],
      "source": [
        "# First cell - Add all required imports\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from tqdm.notebook import tqdm\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Import VishwamAI components\n",
        "from vishwamai.model import Transformer, ModelArgs\n",
        "from vishwamai.model_utils import get_gpu_memory, load_model\n",
        "from vishwamai.cache_augmentation import CacheAugmentation\n",
        "from vishwamai.neural_memory import NeuralMemory\n",
        "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
        "from vishwamai.reward_function import RewardConfig\n",
        "from vishwamai.trainer import VishwamAIPretrainer\n",
        "\n",
        "# Define GPU memory utility function\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize components before using\n",
        "model = None\n",
        "cache_module = None\n",
        "memory_module = None\n",
        "tree_module = None\n",
        "reward_config = None\n",
        "train_dataset = None\n",
        "eval_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "IbKumFgsoELN",
        "outputId": "131124cc-d428-470f-f3fd-b46066faac6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face access token: ··········\n",
            "Successfully logged in to Hugging Face!\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maivishwam\u001b[0m (\u001b[33maivishwam-vishwamai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully logged in to Weights & Biases!\n",
            "CPU times: user 2.41 s, sys: 292 ms, total: 2.7 s\n",
            "Wall time: 23.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from huggingface_hub import login, create_repo\n",
        "from getpass import getpass\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Get token securely\n",
        "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
        "login(token=hf_token)\n",
        "print(\"Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Initialize W&B for experiment tracking\n",
        "wandb.login()\n",
        "print(\"Successfully logged in to Weights & Biases!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMLbaWe9oELO",
        "outputId": "974628cd-da4d-477c-c461-00fcf44811c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: Tesla T4 (14.7 GB)\n",
            "Using T4-optimized configuration with 8-bit quantization\n",
            "Function setup_hardware took 0.4726 seconds to execute.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "def track_time(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds to execute.\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@track_time\n",
        "def setup_hardware():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = get_gpu_memory()\n",
        "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "\n",
        "    # T4-optimized configuration\n",
        "    if 't4' in gpu_name.lower():\n",
        "        variant = \"7B\"  # T4-optimized model\n",
        "        print(\"Using T4-optimized configuration with 8-bit quantization\")\n",
        "    else:\n",
        "        variant = \"167B\"  # Fallback configuration\n",
        "        print(\"Using fallback configuration\")\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return variant\n",
        "\n",
        "model_variant = setup_hardware()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHFppk6soELP",
        "outputId": "fe6471cc-f175-4840-e359-8b98fbb6f497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Model variant '7B' not found in config, creating T4 optimized config\n",
            "Function load_config took 0.0029 seconds to execute.\n",
            "Configuration loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "@track_time\n",
        "def load_config():\n",
        "    config_path = \"./vishwamai/configs/config_optimized.json\"\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    if model_variant not in config[\"model_variants\"]:\n",
        "        print(f\"Warning: Model variant '{model_variant}' not found in config, creating T4 optimized config\")\n",
        "        # T4-optimized configuration\n",
        "        t4_config = {\n",
        "            \"max_batch_size\": 4,\n",
        "            \"max_seq_len\": 2048,\n",
        "            \"dtype\": \"fp8\",\n",
        "            \"vocab_size\": 32000,\n",
        "            \"dim\": 1024,\n",
        "            \"inter_dim\": 2816,\n",
        "            \"moe_inter_dim\": 512,\n",
        "            \"n_layers\": 12,\n",
        "            \"n_dense_layers\": 1,\n",
        "            \"n_heads\": 16,\n",
        "            \"n_routed_experts\": 8,\n",
        "            \"n_shared_experts\": 1,\n",
        "            \"n_activated_experts\": 2,\n",
        "            \"n_expert_groups\": 1,\n",
        "            \"n_limited_groups\": 1,\n",
        "            \"score_func\": \"softmax\",\n",
        "            \"route_scale\": 1.0,\n",
        "            \"q_lora_rank\": 0,\n",
        "            \"kv_lora_rank\": 64,\n",
        "            \"qk_nope_head_dim\": 64,\n",
        "            \"qk_rope_head_dim\": 32,\n",
        "            \"v_head_dim\": 64,\n",
        "            \"original_seq_len\": 2048,\n",
        "            \"rope_theta\": 10000.0,\n",
        "            \"rope_factor\": 20,\n",
        "            \"beta_fast\": 16,\n",
        "            \"beta_slow\": 1,\n",
        "            \"mscale\": 0.5,\n",
        "            \"use_alibi\": False,  # Disable ALiBi for T4\n",
        "            \"use_rope_scaling\": True,\n",
        "            \"gradient_checkpointing\": True,\n",
        "            \"parallel_attn\": True,\n",
        "            \"rope_condense_ratio\": 1.0\n",
        "        }\n",
        "        return t4_config\n",
        "\n",
        "    return config[\"model_variants\"][model_variant][\"model_config\"]\n",
        "\n",
        "# Load configuration\n",
        "model_config = load_config()\n",
        "print(\"Configuration loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_83lo883oELQ"
      },
      "outputs": [],
      "source": [
        "# Create DeepSpeed config for T4 optimization\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True,\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 100,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"allgather_bucket_size\": 5e8,\n",
        "        \"reduce_bucket_size\": 5e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": 32,\n",
        "    \"gradient_accumulation_steps\": 16,\n",
        "    \"train_micro_batch_size_per_gpu\": 2,\n",
        "    \"gradient_clipping\": 0.5,\n",
        "    \"steps_per_print\": 10,\n",
        "    \"wall_clock_breakdown\": False\n",
        "}\n",
        "\n",
        "with open('ds_config.json', 'w') as f:\n",
        "    json.dump(ds_config, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D507tdS0oELQ"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def initialize_components():\n",
        "    print(\"Initializing model and components...\")\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    # Initialize main model with 8-bit quantization for T4\n",
        "    model_args = ModelArgs(**model_config)\n",
        "    model = Transformer(model_args)\n",
        "\n",
        "    # Replace LinearWrapper with current bitsandbytes 8-bit quantization\n",
        "    import bitsandbytes as bnb\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            model._modules[name] = bnb.nn.Linear8bitLt(\n",
        "                module.in_features,\n",
        "                module.out_features,\n",
        "                module.bias is not None,\n",
        "                has_fp16_weights=False,\n",
        "                threshold=6.0\n",
        "            )\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Initialize smaller cache augmentation for T4\n",
        "    cache_config = CacheConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"],\n",
        "        max_cache_length=8192,  # Reduced cache size for T4\n",
        "        dropout=0.1\n",
        "    )\n",
        "    cache_module = DifferentiableCacheAugmentation(cache_config).cuda()\n",
        "\n",
        "    # Initialize advanced memory transformer\n",
        "    memory_config = AdvancedMemoryConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_attention_heads=model_config[\"n_heads\"],\n",
        "        memory_size=8192,  # Adjust based on available GPU memory\n",
        "        use_hierarchical=True,\n",
        "        use_compressed=True\n",
        "    )\n",
        "    memory_module = AdvancedReasoningMemoryTransformer(memory_config).cuda()\n",
        "\n",
        "    # Initialize tree of thoughts with reduced beam size\n",
        "    tree_module = TreeOfThoughts(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"]\n",
        "    ).cuda()\n",
        "\n",
        "    # Initialize reward config\n",
        "    reward_config = RewardConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"]\n",
        "    )\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return model, cache_module, memory_module, tree_module, reward_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdnvhouMoELR",
        "outputId": "5369f5ef-d0ac-4c7c-e523-453a38260a56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Initialize output directory\n",
        "output_dir = \"./pretrain_output\"\n",
        "!mkdir -p $output_dir\n",
        "\n",
        "# Configure training with T4 optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,  # Reduced batch size for T4\n",
        "    gradient_accumulation_steps=16,  # Increased for T4 memory constraints\n",
        "    learning_rate=5e-5,  # Reduced learning rate\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # Mixed precision training\n",
        "    fp16=True,  # Use FP16 instead of BF16 for T4\n",
        "    bf16=False,\n",
        "    # Performance optimizations\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,  # Reduced workers for T4\n",
        "    dataloader_pin_memory=True,\n",
        "    group_by_length=True,\n",
        "    # Memory optimizations\n",
        "    max_grad_norm=0.5,  # Reduced for stability\n",
        "    # Monitoring\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    # Hub integration\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
        "    hub_strategy=\"end\",  # Only save at the end to save memory\n",
        "    # Optimizer settings\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",  # Use 8-bit Adam\n",
        "    # Other settings\n",
        "    remove_unused_columns=False,\n",
        "    seed=42,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    # Memory optimization\n",
        "    deepspeed=\"ds_config.json\"  # Using the config we created\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "b931361f78ba4ec6a3619bf1845f6b6d",
            "160a062a3d204a059ee8fea1596b9359",
            "ad430066d1604a8381a7a67514be9bbf",
            "a2cbc7d473fb46cc99b46e2a5ee188b7",
            "976cc81566984bafb97b99b9cc7ead6f",
            "316dfc1538b94fa0bb64bac5cdda78c0",
            "329852ea49e7417b9abce064e3050cef",
            "896020a67c1e4767a7e479cc7ca295b3",
            "0d5a28bfd9974322bfec965d64c1b544",
            "3ef04b2334694b54a0c8132dc415a2db",
            "cd25d562700647aba4e41f7c0edb0132",
            "b9eb5b5c645e46228f11a7afda81c1eb",
            "d940f106c4104dbfa7094f044eeb77a2",
            "516b768af56f4d5696563bebd51dafb3",
            "3d8f5eb7b1514147ae0519dfb948a635",
            "df9568cb5cf94e04bb844e1f9a824a70",
            "d9cbb5ecd7374cd787cdbc24170cb7bc",
            "8999215feeea417393ba61b29463465e",
            "643174bb2015441d91ccea92a1305315",
            "be42c92d8a9c49159135ab36ac45161f",
            "018281dab8084206b0aed3ae76294733",
            "1501b9ad3ad14bbf922da73ef5e48f34"
          ]
        },
        "id": "Z9mhvSB3oELR",
        "outputId": "a519a242-8517-46e9-d34b-617b2ef5c372"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b931361f78ba4ec6a3619bf1845f6b6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9eb5b5c645e46228f11a7afda81c1eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to load cais/mmlu: Config name is missing.\n",
            "Please pick one among the available configs: ['abstract_algebra', 'all', 'anatomy', 'astronomy', 'auxiliary_train', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
            "Example of usage:\n",
            "\t`load_dataset('cais/mmlu', 'abstract_algebra')`\n",
            "Failed to load cais/mmlu: Config name is missing.\n",
            "Please pick one among the available configs: ['abstract_algebra', 'all', 'anatomy', 'astronomy', 'auxiliary_train', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
            "Example of usage:\n",
            "\t`load_dataset('cais/mmlu', 'abstract_algebra')`\n"
          ]
        }
      ],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Load and combine training datasets with memory optimization\n",
        "def load_dataset_with_memory_optimization(ds_name, split):\n",
        "    clear_gpu_memory()\n",
        "    try:\n",
        "        dataset = load_dataset(ds_name, split=split, streaming=True)  # Use streaming for memory efficiency\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {ds_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "train_datasets = []\n",
        "for ds_name in [\"gsm8k\", \"cais/mmlu\"]:\n",
        "    dataset = load_dataset_with_memory_optimization(ds_name, \"train\")\n",
        "    if dataset is not None:\n",
        "        train_datasets.append(dataset)\n",
        "\n",
        "if not train_datasets:\n",
        "    raise ValueError(\"No training datasets could be loaded\")\n",
        "\n",
        "combined_train_dataset = concatenate_datasets(train_datasets)\n",
        "\n",
        "# Load validation dataset\n",
        "eval_dataset = load_dataset_with_memory_optimization(\"cais/mmlu\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "new-cell",
        "outputId": "311ae6e2-b7f9-4c05-a506-593dac44eb69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing components...\n",
            "Initializing model and components...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'CacheConfig' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a8efaf152105>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize model and components before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing components...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Components initialized successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-852b71f0feb5>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Function {func.__name__} took {end_time - start_time:.4f} seconds to execute.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-1ce15c4e8757>\u001b[0m in \u001b[0;36minitialize_components\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Initialize smaller cache augmentation for T4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     cache_config = CacheConfig(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_heads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CacheConfig' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize model and components before training\n",
        "print(\"Initializing components...\")\n",
        "model, cache_module, memory_module, tree_module, reward_config = initialize_components()\n",
        "print(\"Components initialized successfully\")\n",
        "\n",
        "# Make sure datasets are defined\n",
        "if 'train_dataset' not in locals() or 'eval_dataset' not in locals():\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = load_dataset(\"gsm8k\", split=\"train\", streaming=True)\n",
        "    eval_dataset = load_dataset(\"cais/mmlu\", split=\"validation\", streaming=True)\n",
        "    print(\"Datasets loaded successfully\")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "@track_time\n",
        "def train_model():\n",
        "    global trainer  # Make trainer accessible globally\n",
        "    trainer = VishwamAIPretrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        memory_module=memory_module,\n",
        "        tree_module=tree_module,\n",
        "        cache_module=cache_module,\n",
        "        reward_config=reward_config\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model(\"./final_model\")\n",
        "        trainer.push_to_hub(\n",
        "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "        )\n",
        "        print(\"Model training and saving completed successfully\")\n",
        "        return trainer\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_gpu_memory()\n",
        "        raise e\n",
        "\n",
        "trainer = train_model()  # Store trainer instance for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_4Ddy6coELR"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def train_model():\n",
        "    trainer = VishwamAIPretrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        memory_module=memory_module,\n",
        "        tree_module=tree_module,\n",
        "        cache_module=cache_module,\n",
        "        reward_config=reward_config\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "\n",
        "        # Save model and components\n",
        "        trainer.save_model(\"./final_model\")\n",
        "        print(\"Model saved successfully\")\n",
        "\n",
        "        # Push to hub with LFS\n",
        "        trainer.push_to_hub(\n",
        "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "        )\n",
        "        print(\"Model pushed to HuggingFace Hub\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_gpu_memory()\n",
        "        raise e\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsLdBhzoELR"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def save_model():\n",
        "    clear_gpu_memory()\n",
        "    model_save_path = \"final_model\"\n",
        "    trainer.save_model(model_save_path)\n",
        "\n",
        "    # Initialize Git LFS tracking for the saved model files\n",
        "    !git lfs track \"final_model/*.bin\"\n",
        "    !git lfs track \"final_model/*.pt\"\n",
        "    !git lfs track \"final_model/*.pth\"\n",
        "\n",
        "    print(\"Model and components saved successfully\")\n",
        "    return model_save_path\n",
        "\n",
        "model_save_path = save_model()\n",
        "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2NgnZFXoELS"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def validate_model():\n",
        "    clear_gpu_memory()\n",
        "    # Load all components for validation with 8-bit quantization\n",
        "    test_model = Transformer(ModelArgs(**model_config))\n",
        "    test_model = bnb.nn.LinearWrapper.wrap_model(test_model, device='cuda', quantize=True)\n",
        "    test_model.load_state_dict(torch.load(f\"{model_save_path}/pytorch_model.bin\"))\n",
        "\n",
        "    # Load auxiliary components\n",
        "    test_cache = DifferentiableCacheAugmentation.from_pretrained(model_save_path)\n",
        "    test_memory = ReasoningMemoryTransformer.from_pretrained(model_save_path)\n",
        "    test_tree = TreeOfThoughts.from_pretrained(model_save_path)\n",
        "\n",
        "    test_model.eval()\n",
        "    test_cache.eval()\n",
        "    test_memory.eval()\n",
        "    test_tree.eval()\n",
        "\n",
        "    test_cases = [\n",
        "        \"What is 7 * 12?\",\n",
        "        \"Explain quantum computing in simple terms.\",\n",
        "        \"Write a Python function to find prime numbers.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Running validation tests...\")\n",
        "    for test_input in test_cases:\n",
        "        print(f\"\\nTest: {test_input}\")\n",
        "        clear_gpu_memory()\n",
        "        # Note: You'll need to implement tokenization for the actual input\n",
        "        tokens = torch.randint(0, model_config['vocab_size'], (1, 32)).cuda()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            start = time.time()\n",
        "            output = test_model(tokens)\n",
        "            end = time.time()\n",
        "\n",
        "            # Apply enhancements with memory management\n",
        "            enhanced_states = test_cache(output)\n",
        "            memory_enhanced = test_memory(enhanced_states)\n",
        "            final_output = test_tree(memory_enhanced)\n",
        "\n",
        "        print(f\"Generated response in {end-start:.2f}s\")\n",
        "        # Note: You'll need to implement detokenization for the actual output\n",
        "\n",
        "validate_model()\n",
        "print(\"\\nPretraining and validation completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "018281dab8084206b0aed3ae76294733": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5a28bfd9974322bfec965d64c1b544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1501b9ad3ad14bbf922da73ef5e48f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "160a062a3d204a059ee8fea1596b9359": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316dfc1538b94fa0bb64bac5cdda78c0",
            "placeholder": "​",
            "style": "IPY_MODEL_329852ea49e7417b9abce064e3050cef",
            "value": "README.md: 100%"
          }
        },
        "316dfc1538b94fa0bb64bac5cdda78c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329852ea49e7417b9abce064e3050cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d8f5eb7b1514147ae0519dfb948a635": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018281dab8084206b0aed3ae76294733",
            "placeholder": "​",
            "style": "IPY_MODEL_1501b9ad3ad14bbf922da73ef5e48f34",
            "value": " 138k/138k [00:00&lt;00:00, 2.14MB/s]"
          }
        },
        "3ef04b2334694b54a0c8132dc415a2db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "516b768af56f4d5696563bebd51dafb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_643174bb2015441d91ccea92a1305315",
            "max": 138338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be42c92d8a9c49159135ab36ac45161f",
            "value": 138338
          }
        },
        "643174bb2015441d91ccea92a1305315": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896020a67c1e4767a7e479cc7ca295b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8999215feeea417393ba61b29463465e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "976cc81566984bafb97b99b9cc7ead6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2cbc7d473fb46cc99b46e2a5ee188b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef04b2334694b54a0c8132dc415a2db",
            "placeholder": "​",
            "style": "IPY_MODEL_cd25d562700647aba4e41f7c0edb0132",
            "value": " 53.2k/53.2k [00:00&lt;00:00, 3.39MB/s]"
          }
        },
        "ad430066d1604a8381a7a67514be9bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896020a67c1e4767a7e479cc7ca295b3",
            "max": 53221,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d5a28bfd9974322bfec965d64c1b544",
            "value": 53221
          }
        },
        "b931361f78ba4ec6a3619bf1845f6b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_160a062a3d204a059ee8fea1596b9359",
              "IPY_MODEL_ad430066d1604a8381a7a67514be9bbf",
              "IPY_MODEL_a2cbc7d473fb46cc99b46e2a5ee188b7"
            ],
            "layout": "IPY_MODEL_976cc81566984bafb97b99b9cc7ead6f"
          }
        },
        "b9eb5b5c645e46228f11a7afda81c1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d940f106c4104dbfa7094f044eeb77a2",
              "IPY_MODEL_516b768af56f4d5696563bebd51dafb3",
              "IPY_MODEL_3d8f5eb7b1514147ae0519dfb948a635"
            ],
            "layout": "IPY_MODEL_df9568cb5cf94e04bb844e1f9a824a70"
          }
        },
        "be42c92d8a9c49159135ab36ac45161f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd25d562700647aba4e41f7c0edb0132": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d940f106c4104dbfa7094f044eeb77a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9cbb5ecd7374cd787cdbc24170cb7bc",
            "placeholder": "​",
            "style": "IPY_MODEL_8999215feeea417393ba61b29463465e",
            "value": "dataset_infos.json: 100%"
          }
        },
        "d9cbb5ecd7374cd787cdbc24170cb7bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df9568cb5cf94e04bb844e1f9a824a70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
