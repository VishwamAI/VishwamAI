{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll set up our environment with the required dependencies and GPU optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Git LFS for model handling\n",
    "!apt-get install git-lfs -y\n",
    "!git lfs install\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
    "%cd VishwamAI\n",
    "\n",
    "# Install the package\n",
    "!pip install -e . -q\n",
    "\n",
    "# Configure Git LFS for model storage\n",
    "!git config lfs.url https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
    "!git config lfs.pushurl https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
    "\n",
    "# Set up Git LFS tracking for model files\n",
    "!git lfs track \"*.bin\"\n",
    "!git lfs track \"*.pt\"\n",
    "!git lfs track \"*.pth\"\n",
    "!git lfs track \"*.ckpt\"\n",
    "!git lfs track \"*.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install optimized PyTorch and related packages\n",
    "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "    transformers==4.34.0 datasets accelerate huggingface_hub wandb bitsandbytes -q\n",
    "\n",
    "# Install DeepSpeed for distributed training\n",
    "%pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from tqdm.notebook import tqdm\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Import VishwamAI components\n",
    "from vishwamai.model import Transformer, ModelArgs\n",
    "from vishwamai.model_utils import get_gpu_memory, load_model\n",
    "from vishwamai.cache_augmentation import CacheAugmentation\n",
    "from vishwamai.neural_memory import NeuralMemory\n",
    "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
    "from vishwamai.reward_function import RewardConfig\n",
    "from vishwamai.trainer import VishwamAIPretrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Setup and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def setup_gpu():\n",
    "    \"\"\"Configure GPU and verify setup\"\"\"\n",
    "    !nvidia-smi  # Display GPU info\n",
    "    \n",
    "    # Enable TF32 for better performance on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "    \n",
    "    return gpu_name\n",
    "\n",
    "gpu_name = setup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hugging Face & Weights & Biases Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, create_repo\n",
    "from getpass import getpass\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Get Hugging Face token\n",
    "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
    "login(token=hf_token)\n",
    "print(\"Successfully logged in to Hugging Face!\")\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.login()\n",
    "print(\"Successfully logged in to Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration (40B Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_config():\n",
    "    \"\"\"Load and configure model settings for 40B parameters\"\"\"\n",
    "    config = {\n",
    "        \"max_batch_size\": 1,  # Reduced for memory constraints\n",
    "        \"max_seq_len\": 2048,\n",
    "        \"dtype\": \"fp8\",  # Use fp8 for memory efficiency\n",
    "        \"vocab_size\": 32000,\n",
    "        \"dim\": 6144,  # Increased hidden size\n",
    "        \"inter_dim\": 24576,  # 4x dim for MLP\n",
    "        \"moe_inter_dim\": 12288,  # Increased for better MoE capacity\n",
    "        \"n_layers\": 48,  # Increased depth\n",
    "        \"n_dense_layers\": 2,\n",
    "        \"n_heads\": 48,  # More attention heads\n",
    "        \"n_routed_experts\": 32,  # Increased experts\n",
    "        \"n_shared_experts\": 2,\n",
    "        \"n_activated_experts\": 4,\n",
    "        \"n_expert_groups\": 2,\n",
    "        \"n_limited_groups\": 2,\n",
    "        \"score_func\": \"softmax\",\n",
    "        \"route_scale\": 1.0,\n",
    "        \"q_lora_rank\": 64,  # Using LoRA for memory efficiency\n",
    "        \"kv_lora_rank\": 128,\n",
    "        \"qk_nope_head_dim\": 128,\n",
    "        \"qk_rope_head_dim\": 64,\n",
    "        \"v_head_dim\": 128,\n",
    "        \"original_seq_len\": 2048,\n",
    "        \"rope_theta\": 10000.0,\n",
    "        \"rope_factor\": 20,\n",
    "        \"beta_fast\": 32,\n",
    "        \"beta_slow\": 1,\n",
    "        \"mscale\": 0.5,\n",
    "        \"use_alibi\": True,  # Enable ALiBi for better long-range dependencies\n",
    "        \"use_rope_scaling\": True,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"parallel_attn\": True,\n",
    "        \"rope_condense_ratio\": 1.0\n",
    "    }\n",
    "    \n",
    "    # Calculate and print total parameters\n",
    "    hidden_size = config[\"dim\"]\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    vocab_size = config[\"vocab_size\"]\n",
    "    n_experts = config[\"n_routed_experts\"]\n",
    "    expert_dim = config[\"moe_inter_dim\"]\n",
    "    \n",
    "    params_per_layer = (\n",
    "        4 * hidden_size * hidden_size +\n",
    "        n_experts * (2 * hidden_size * expert_dim + expert_dim) +\n",
    "        4 * hidden_size\n",
    "    )\n",
    "    \n",
    "    total_params = (\n",
    "        vocab_size * hidden_size +\n",
    "        n_layers * params_per_layer +\n",
    "        hidden_size * vocab_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Total parameters: {total_params / 1e9:.2f}B\")\n",
    "    return config\n",
    "\n",
    "model_config = load_model_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration (40B Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepSpeed config optimized for 40B\n",
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": True,\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 100,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,  # Use ZeRO-3 for better memory efficiency\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"stage3_max_live_parameters\": 1e8,\n",
    "        \"stage3_max_reuse_distance\": 1e8,\n",
    "        \"stage3_prefetch_bucket_size\": 5e7,\n",
    "        \"stage3_param_persistence_threshold\": 1e5\n",
    "    },\n",
    "    \"train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 64,\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_clipping\": 0.5,\n",
    "    \"steps_per_print\": 10,\n",
    "    \"wall_clock_breakdown\": False\n",
    "}\n",
    "\n",
    "with open('ds_config.json', 'w') as f:\n",
    "    json.dump(ds_config, f)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrain_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=64,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    group_by_length=True,\n",
    "    max_grad_norm=0.5,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    remove_unused_columns=False,\n",
    "    seed=42,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    deepspeed=\"ds_config.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Load and prepare training datasets\"\"\"\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Load training datasets\n",
    "    train_datasets = []\n",
    "    for ds_name in [\"gsm8k\", \"cais/mmlu\"]:\n",
    "        try:\n",
    "            dataset = load_dataset(ds_name, split=\"train\", streaming=True)\n",
    "            train_datasets.append(dataset)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {ds_name}: {e}\")\n",
    "    \n",
    "    if not train_datasets:\n",
    "        raise ValueError(\"No training datasets could be loaded\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    train_dataset = concatenate_datasets(train_datasets)\n",
    "    \n",
    "    # Load validation dataset\n",
    "    eval_dataset = load_dataset(\"cais/mmlu\", split=\"validation\", streaming=True)\n",
    "    \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "train_dataset, eval_dataset = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_components():\n",
    "    \"\"\"Initialize model and all required components\"\"\"\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Initialize main model\n",
    "    model_args = ModelArgs(**model_config)\n",
    "    model = Transformer(model_args)\n",
    "    \n",
    "    # Apply 8-bit quantization\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            model._modules[name] = bnb.nn.Linear8bitLt(\n",
    "                module.in_features,\n",
    "                module.out_features,\n",
    "                module.bias is not None,\n",
    "                has_fp16_weights=False,\n",
    "                threshold=6.0\n",
    "            )\n",
    "    \n",
    "    model = model.cuda()\n",
    "    \n",
    "    # Initialize components\n",
    "    cache_module = CacheAugmentation(model_config).cuda()\n",
    "    memory_module = NeuralMemory(model_config).cuda()\n",
    "    tree_module = TreeOfThoughts(model_config).cuda()\n",
    "    reward_config = RewardConfig(model_config)\n",
    "    \n",
    "    return model, cache_module, memory_module, tree_module, reward_config\n",
    "\n",
    "model, cache_module, memory_module, tree_module, reward_config = initialize_model_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    trainer = VishwamAIPretrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        memory_module=memory_module,\n",
    "        tree_module=tree_module,\n",
    "        cache_module=cache_module,\n",
    "        reward_config=reward_config\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        trainer.save_model(\"./final_model\")\n",
    "        print(\"Model saved successfully\")\n",
    "        \n",
    "        trainer.push_to_hub(\n",
    "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        )\n",
    "        print(\"Model pushed to HuggingFace Hub\")\n",
    "        \n",
    "        return trainer\n",
    "    except Exception as e:\n",
    "        print(f\"Training interrupted: {e}\")\n",
    "        clear_gpu_memory()\n",
    "        raise e\n",
    "\n",
    "trainer = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_trained_model():\n",
    "    \"\"\"Validate the trained model\"\"\"\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    model_path = \"./final_model\"\n",
    "    test_model = Transformer(ModelArgs(**model_config))\n",
    "    test_model.load_state_dict(torch.load(f\"{model_path}/pytorch_model.bin\"))\n",
    "    test_model = test_model.cuda()\n",
    "    test_model.eval()\n",
    "    \n",
    "    test_cases = [\n",
    "        \"What is 7 * 12?\",\n",
    "        \"Explain quantum computing in simple terms.\",\n",
    "        \"Write a Python function to find prime numbers.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Running validation tests...\")\n",
    "    for test_input in test_cases:\n",
    "        print(f\"\\nTest: {test_input}\")\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        # Use proper tokenization in actual implementation\n",
    "        tokens = torch.randint(0, model_config['vocab_size'], (1, 32)).cuda()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            start = time.time()\n",
    "            output = test_model(tokens)\n",
    "            end = time.time()\n",
    "        \n",
    "        print(f\"Generated response in {end-start:.2f}s\")\n",
    "\n",
    "validate_trained_model()\n",
    "print(\"\\nTraining and validation completed successfully!\")\n",
    "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
