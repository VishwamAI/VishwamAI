{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VishwamAI/VishwamAI/blob/main/vishwamai_colab_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "guIJHEO3HjY9",
        "outputId": "783448d1-b8bc-4773-8ce5-70f38f3e9df0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Git LFS initialized.\n",
            "Cloning into 'VishwamAI'...\n",
            "remote: Enumerating objects: 1072, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 1072 (delta 8), reused 12 (delta 3), pack-reused 1031 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1072/1072), 28.52 MiB | 10.47 MiB/s, done.\n",
            "Resolving deltas: 100% (541/541), done.\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "Encountered 2 file(s) that should have been pointers, but weren't:\n",
            "\tgsm8k/test-00000-of-00001.parquet\n",
            "\tgsm8k/train-00000-of-00001.parquet\n",
            "/content/VishwamAI\n",
            "\u001b[31mERROR: file:///content/VishwamAI does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mWARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.bin\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.pt\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.pth\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.ckpt\" already supported\n",
            "WARNING: These unsafe lfsconfig keys were ignored:\n",
            "\n",
            "  lfs.transfer.maxretries\n",
            "  lfs.transfer.maxverifies\n",
            "  lfs.transfer.maxconcurrenttransfers\n",
            "  filter.lfs.clean\n",
            "  filter.lfs.smudge\n",
            "  filter.lfs.process\n",
            "  filter.lfs.required\n",
            "\"*.safetensors\" already supported\n"
          ]
        }
      ],
      "source": [
        "# Install Git LFS\n",
        "!apt-get install git-lfs -y\n",
        "!git lfs install\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
        "%cd VishwamAI\n",
        "\n",
        "# Install the package\n",
        "!pip install -e . -q\n",
        "\n",
        "\n",
        "# Configure Git LFS\n",
        "!git config lfs.url https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
        "!git config lfs.pushurl https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
        "\n",
        "# Set up Git LFS tracking\n",
        "!git lfs track \"*.bin\"\n",
        "!git lfs track \"*.pt\"\n",
        "!git lfs track \"*.pth\"\n",
        "!git lfs track \"*.ckpt\"\n",
        "!git lfs track \"*.safetensors\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wja3KBfdoELM",
        "outputId": "000e1538-4f79-4c75-e6fd-e113f46ed02e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 17 04:16:58 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Using GPU: Tesla T4\n",
            "CPU times: user 1.66 s, sys: 351 ms, total: 2.01 s\n",
            "Wall time: 3.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Verify GPU availability and requirements\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Memory optimization for T4\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "print(f\"Using GPU: {gpu_name}\")\n",
        "\n",
        "# Set memory optimization flags for T4\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8ucxo0oELN",
        "outputId": "f1e58242-32ff-4211-c43f-3f77180f5a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m427.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.34.0 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.34.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting deepspeed\n",
            "  Downloading deepspeed-0.16.3.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.10.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->deepspeed)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->deepspeed)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->deepspeed)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->deepspeed)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->deepspeed)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->deepspeed)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->deepspeed)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.3-py3-none-any.whl size=1550059 sha256=603af916e9635366607aafa74014c1083cad62035ba17eb15c972c93fe5645d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/dc/d4/7e7e07b11bc7c0e2a1a495b967acf58de61261eed4596fb23b\n",
            "Successfully built deepspeed\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Package installation with T4 optimized versions\n",
        "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
        "    transformers==4.34.0 datasets accelerate huggingface_hub wandb bitsandbytes -q\n",
        "%pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-KlPRYJoELL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Install Git LFS\n",
        "!apt-get install git-lfs\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlmaEAzJrTHy"
      },
      "outputs": [],
      "source": [
        "pip install datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou4YTR5UoELI"
      },
      "outputs": [],
      "source": [
        "# First cell - Add all required imports\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from tqdm.notebook import tqdm\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Import VishwamAI components\n",
        "from vishwamai.model import Transformer, ModelArgs\n",
        "from vishwamai.model_utils import get_gpu_memory, load_model\n",
        "from vishwamai.cache_augmentation import CacheConfig, DifferentiableCacheAugmentation\n",
        "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
        "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
        "from vishwamai.reward_function import RewardConfig\n",
        "from vishwamai.trainer import VishwamAIPretrainer\n",
        "\n",
        "# Define GPU memory utility function\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize components before using\n",
        "model = None\n",
        "cache_module = None\n",
        "memory_module = None\n",
        "tree_module = None\n",
        "reward_config = None\n",
        "train_dataset = None\n",
        "eval_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbKumFgsoELN"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from huggingface_hub import login, create_repo\n",
        "from getpass import getpass\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Get token securely\n",
        "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
        "login(token=hf_token)\n",
        "print(\"Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Initialize W&B for experiment tracking\n",
        "wandb.login()\n",
        "print(\"Successfully logged in to Weights & Biases!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_cyeDm4oELO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from vishwamai.model_utils import load_model, get_gpu_memory\n",
        "from vishwamai.model import Transformer, ModelArgs\n",
        "from vishwamai.cache_augmentation import CacheConfig, DifferentiableCacheAugmentation\n",
        "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
        "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
        "from vishwamai.reward_function import RewardConfig\n",
        "from vishwamai.trainer import VishwamAIPretrainer\n",
        "\n",
        "# T4-specific performance optimizations\n",
        "import bitsandbytes as bnb\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMLbaWe9oELO"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def setup_hardware():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = get_gpu_memory()\n",
        "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "\n",
        "    # T4-optimized configuration\n",
        "    if 't4' in gpu_name.lower():\n",
        "        variant = \"7B\"  # T4-optimized model\n",
        "        print(\"Using T4-optimized configuration with 8-bit quantization\")\n",
        "    else:\n",
        "        variant = \"167B\"  # Fallback configuration\n",
        "        print(\"Using fallback configuration\")\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return variant\n",
        "\n",
        "model_variant = setup_hardware()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHFppk6soELP"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def load_config():\n",
        "    config_path = \"./vishwamai/configs/config_optimized.json\"\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    if model_variant not in config[\"model_variants\"]:\n",
        "        print(f\"Warning: Model variant '{model_variant}' not found in config, creating T4 optimized config\")\n",
        "        # T4-optimized configuration\n",
        "        t4_config = {\n",
        "            \"max_batch_size\": 4,\n",
        "            \"max_seq_len\": 2048,\n",
        "            \"dtype\": \"fp8\",\n",
        "            \"vocab_size\": 32000,\n",
        "            \"dim\": 1024,\n",
        "            \"inter_dim\": 2816,\n",
        "            \"moe_inter_dim\": 512,\n",
        "            \"n_layers\": 12,\n",
        "            \"n_dense_layers\": 1,\n",
        "            \"n_heads\": 16,\n",
        "            \"n_routed_experts\": 8,\n",
        "            \"n_shared_experts\": 1,\n",
        "            \"n_activated_experts\": 2,\n",
        "            \"n_expert_groups\": 1,\n",
        "            \"n_limited_groups\": 1,\n",
        "            \"score_func\": \"softmax\",\n",
        "            \"route_scale\": 1.0,\n",
        "            \"q_lora_rank\": 0,\n",
        "            \"kv_lora_rank\": 64,\n",
        "            \"qk_nope_head_dim\": 64,\n",
        "            \"qk_rope_head_dim\": 32,\n",
        "            \"v_head_dim\": 64,\n",
        "            \"original_seq_len\": 2048,\n",
        "            \"rope_theta\": 10000.0,\n",
        "            \"rope_factor\": 20,\n",
        "            \"beta_fast\": 16,\n",
        "            \"beta_slow\": 1,\n",
        "            \"mscale\": 0.5,\n",
        "            \"use_alibi\": False,  # Disable ALiBi for T4\n",
        "            \"use_rope_scaling\": True,\n",
        "            \"gradient_checkpointing\": True,\n",
        "            \"parallel_attn\": True,\n",
        "            \"rope_condense_ratio\": 1.0\n",
        "        }\n",
        "        return t4_config\n",
        "\n",
        "    return config[\"model_variants\"][model_variant][\"model_config\"]\n",
        "\n",
        "# Load configuration\n",
        "model_config = load_config()\n",
        "print(\"Configuration loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_83lo883oELQ"
      },
      "outputs": [],
      "source": [
        "# Create DeepSpeed config for T4 optimization\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True,\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 100,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"allgather_bucket_size\": 5e8,\n",
        "        \"reduce_bucket_size\": 5e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": 32,\n",
        "    \"gradient_accumulation_steps\": 16,\n",
        "    \"train_micro_batch_size_per_gpu\": 2,\n",
        "    \"gradient_clipping\": 0.5,\n",
        "    \"steps_per_print\": 10,\n",
        "    \"wall_clock_breakdown\": False\n",
        "}\n",
        "\n",
        "with open('ds_config.json', 'w') as f:\n",
        "    json.dump(ds_config, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D507tdS0oELQ"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def initialize_components():\n",
        "    print(\"Initializing model and components...\")\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    # Initialize main model with 8-bit quantization for T4\n",
        "    model_args = ModelArgs(**model_config)\n",
        "    model = Transformer(model_args)\n",
        "\n",
        "    # Replace LinearWrapper with current bitsandbytes 8-bit quantization\n",
        "    import bitsandbytes as bnb\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            model._modules[name] = bnb.nn.Linear8bitLt(\n",
        "                module.in_features,\n",
        "                module.out_features,\n",
        "                module.bias is not None,\n",
        "                has_fp16_weights=False,\n",
        "                threshold=6.0\n",
        "            )\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Initialize smaller cache augmentation for T4\n",
        "    cache_config = CacheConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"],\n",
        "        max_cache_length=8192,  # Reduced cache size for T4\n",
        "        dropout=0.1\n",
        "    )\n",
        "    cache_module = DifferentiableCacheAugmentation(cache_config).cuda()\n",
        "\n",
        "    # Initialize advanced memory transformer\n",
        "    memory_config = AdvancedMemoryConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_attention_heads=model_config[\"n_heads\"],\n",
        "        memory_size=8192,  # Adjust based on available GPU memory\n",
        "        use_hierarchical=True,\n",
        "        use_compressed=True\n",
        "    )\n",
        "    memory_module = AdvancedReasoningMemoryTransformer(memory_config).cuda()\n",
        "\n",
        "    # Initialize tree of thoughts with reduced beam size\n",
        "    tree_module = TreeOfThoughts(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"]\n",
        "    ).cuda()\n",
        "\n",
        "    # Initialize reward config\n",
        "    reward_config = RewardConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"]\n",
        "    )\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return model, cache_module, memory_module, tree_module, reward_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdnvhouMoELR"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Initialize output directory\n",
        "output_dir = \"./pretrain_output\"\n",
        "!mkdir -p $output_dir\n",
        "\n",
        "# Configure training with T4 optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,  # Reduced batch size for T4\n",
        "    gradient_accumulation_steps=16,  # Increased for T4 memory constraints\n",
        "    learning_rate=5e-5,  # Reduced learning rate\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # Mixed precision training\n",
        "    fp16=True,  # Use FP16 instead of BF16 for T4\n",
        "    bf16=False,\n",
        "    # Performance optimizations\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,  # Reduced workers for T4\n",
        "    dataloader_pin_memory=True,\n",
        "    group_by_length=True,\n",
        "    # Memory optimizations\n",
        "    max_grad_norm=0.5,  # Reduced for stability\n",
        "    # Monitoring\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    # Hub integration\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
        "    hub_strategy=\"end\",  # Only save at the end to save memory\n",
        "    # Optimizer settings\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",  # Use 8-bit Adam\n",
        "    # Other settings\n",
        "    remove_unused_columns=False,\n",
        "    seed=42,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    # Memory optimization\n",
        "    deepspeed=\"ds_config.json\"  # Using the config we created\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mhvSB3oELR"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Load and combine training datasets with memory optimization\n",
        "def load_dataset_with_memory_optimization(ds_name, split):\n",
        "    clear_gpu_memory()\n",
        "    try:\n",
        "        dataset = load_dataset(ds_name, split=split, streaming=True)  # Use streaming for memory efficiency\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {ds_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "train_datasets = []\n",
        "for ds_name in [\"gsm8k\", \"cais/mmlu\"]:\n",
        "    dataset = load_dataset_with_memory_optimization(ds_name, \"train\")\n",
        "    if dataset is not None:\n",
        "        train_datasets.append(dataset)\n",
        "\n",
        "if not train_datasets:\n",
        "    raise ValueError(\"No training datasets could be loaded\")\n",
        "\n",
        "combined_train_dataset = concatenate_datasets(train_datasets)\n",
        "\n",
        "# Load validation dataset\n",
        "eval_dataset = load_dataset_with_memory_optimization(\"cais/mmlu\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new-cell"
      },
      "outputs": [],
      "source": [
        "# Initialize model and components before training\n",
        "print(\"Initializing components...\")\n",
        "model, cache_module, memory_module, tree_module, reward_config = initialize_components()\n",
        "print(\"Components initialized successfully\")\n",
        "\n",
        "# Make sure datasets are defined\n",
        "if 'train_dataset' not in locals() or 'eval_dataset' not in locals():\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = load_dataset(\"gsm8k\", split=\"train\", streaming=True)\n",
        "    eval_dataset = load_dataset(\"cais/mmlu\", split=\"validation\", streaming=True)\n",
        "    print(\"Datasets loaded successfully\")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "@track_time\n",
        "def train_model():\n",
        "    global trainer  # Make trainer accessible globally\n",
        "    trainer = VishwamAIPretrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        memory_module=memory_module,\n",
        "        tree_module=tree_module,\n",
        "        cache_module=cache_module,\n",
        "        reward_config=reward_config\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model(\"./final_model\")\n",
        "        trainer.push_to_hub(\n",
        "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "        )\n",
        "        print(\"Model training and saving completed successfully\")\n",
        "        return trainer\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_gpu_memory()\n",
        "        raise e\n",
        "\n",
        "trainer = train_model()  # Store trainer instance for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_4Ddy6coELR"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def train_model():\n",
        "    trainer = VishwamAIPretrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        memory_module=memory_module,\n",
        "        tree_module=tree_module,\n",
        "        cache_module=cache_module,\n",
        "        reward_config=reward_config\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "\n",
        "        # Save model and components\n",
        "        trainer.save_model(\"./final_model\")\n",
        "        print(\"Model saved successfully\")\n",
        "\n",
        "        # Push to hub with LFS\n",
        "        trainer.push_to_hub(\n",
        "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "        )\n",
        "        print(\"Model pushed to HuggingFace Hub\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_gpu_memory()\n",
        "        raise e\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsLdBhzoELR"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def save_model():\n",
        "    clear_gpu_memory()\n",
        "    model_save_path = \"final_model\"\n",
        "    trainer.save_model(model_save_path)\n",
        "\n",
        "    # Initialize Git LFS tracking for the saved model files\n",
        "    !git lfs track \"final_model/*.bin\"\n",
        "    !git lfs track \"final_model/*.pt\"\n",
        "    !git lfs track \"final_model/*.pth\"\n",
        "\n",
        "    print(\"Model and components saved successfully\")\n",
        "    return model_save_path\n",
        "\n",
        "model_save_path = save_model()\n",
        "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2NgnZFXoELS"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def validate_model():\n",
        "    clear_gpu_memory()\n",
        "    # Load all components for validation with 8-bit quantization\n",
        "    test_model = Transformer(ModelArgs(**model_config))\n",
        "    test_model = bnb.nn.LinearWrapper.wrap_model(test_model, device='cuda', quantize=True)\n",
        "    test_model.load_state_dict(torch.load(f\"{model_save_path}/pytorch_model.bin\"))\n",
        "\n",
        "    # Load auxiliary components\n",
        "    test_cache = DifferentiableCacheAugmentation.from_pretrained(model_save_path)\n",
        "    test_memory = ReasoningMemoryTransformer.from_pretrained(model_save_path)\n",
        "    test_tree = TreeOfThoughts.from_pretrained(model_save_path)\n",
        "\n",
        "    test_model.eval()\n",
        "    test_cache.eval()\n",
        "    test_memory.eval()\n",
        "    test_tree.eval()\n",
        "\n",
        "    test_cases = [\n",
        "        \"What is 7 * 12?\",\n",
        "        \"Explain quantum computing in simple terms.\",\n",
        "        \"Write a Python function to find prime numbers.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Running validation tests...\")\n",
        "    for test_input in test_cases:\n",
        "        print(f\"\\nTest: {test_input}\")\n",
        "        clear_gpu_memory()\n",
        "        # Note: You'll need to implement tokenization for the actual input\n",
        "        tokens = torch.randint(0, model_config['vocab_size'], (1, 32)).cuda()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            start = time.time()\n",
        "            output = test_model(tokens)\n",
        "            end = time.time()\n",
        "\n",
        "            # Apply enhancements with memory management\n",
        "            enhanced_states = test_cache(output)\n",
        "            memory_enhanced = test_memory(enhanced_states)\n",
        "            final_output = test_tree(memory_enhanced)\n",
        "\n",
        "        print(f\"Generated response in {end-start:.2f}s\")\n",
        "        # Note: You'll need to implement detokenization for the actual output\n",
        "\n",
        "validate_model()\n",
        "print(\"\\nPretraining and validation completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}