{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Colab Training\n",
    "\n",
    "Training with custom VishwamAI Transformer architecture using advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "!nvidia-smi\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setup directories\n",
    "import os\n",
    "DRIVE_DIR = '/content/drive/MyDrive/VishwamAI'\n",
    "CHECKPOINT_DIR = f'{DRIVE_DIR}/checkpoints'\n",
    "!mkdir -p {CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies and clone repo\n",
    "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
    "%cd VishwamAI\n",
    "\n",
    "%pip install -q torch transformers datasets accelerate bitsandbytes wandb\n",
    "%pip install -e .\n",
    "\n",
    "# Secure Hugging Face authentication\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "def get_huggingface_token():\n",
    "    \"\"\"Get Hugging Face token from environment or prompt\"\"\"\n",
    "    token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "    if not token:\n",
    "        print(\"HUGGINGFACE_TOKEN not found in environment\")\n",
    "        token = getpass.getpass('Enter your Hugging Face token (input will be hidden): ')\n",
    "        # Store temporarily for this session\n",
    "        os.environ['HUGGINGFACE_TOKEN'] = token\n",
    "    return token\n",
    "\n",
    "try:\n",
    "    token = get_huggingface_token()\n",
    "    login(token=token)\n",
    "    print(\"Successfully logged in to Hugging Face\")\n",
    "except Exception as e:\n",
    "    print(f\"Error logging in to Hugging Face: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import VishwamAI components with correct paths\n",
    "from vishwamai.base_layers import Linear  # Import from base_layers instead of utils\n",
    "from vishwamai.Transformer import Transformer\n",
    "from vishwamai import (\n",
    "    create_model,\n",
    "    ModelArgs,\n",
    "    VishwamAITokenizer,\n",
    "    TokenizerConfig\n",
    ")\n",
    "\n",
    "# Import training components\n",
    "from vishwamai.advanced_training import AdvancedTrainer\n",
    "from vishwamai.fp8_cast_bf16 import main\n",
    "from vishwamai.neural_memory import NeuralMemory\n",
    "from vishwamai.tree_of_thoughts import TreeConfig, RewardConfig\n",
    "from vishwamai.curriculum import CurriculumConfig\n",
    "\n",
    "# Import visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Verify imports were successful\n",
    "print(\"Imports completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualization and analysis tools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Initialize performance tracking\n",
    "performance_history = {\n",
    "    'steps': [],\n",
    "    'loss': [],\n",
    "    'learning_rate': [],\n",
    "    'memory_usage': [],\n",
    "    'curriculum_level': [],\n",
    "    'expert_usage': [],\n",
    "    'evaluation_scores': []\n",
    "}\n",
    "\n",
    "performance_df = pd.DataFrame(performance_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance_tracking(stats, step):\n",
    "    \"\"\"Update performance tracking with new statistics\"\"\"\n",
    "    performance_df.loc[len(performance_df)] = {\n",
    "        'steps': step,\n",
    "        'loss': stats['loss'],\n",
    "        'learning_rate': stats['lr'],\n",
    "        'memory_usage': stats['memory_usage']['allocated'],\n",
    "        'curriculum_level': stats['curriculum_stats']['current_difficulty'],\n",
    "        'expert_usage': sum(stats.get('moe_metrics', {}).values()) / len(stats.get('moe_metrics', {})),\n",
    "        'evaluation_scores': stats.get('eval_score', 0)\n",
    "    }\n",
    "\n",
    "def plot_training_progress():\n",
    "    \"\"\"Generate training progress visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Training Progress Overview', fontsize=16)\n",
    "    \n",
    "    axes[0,0].plot(performance_df['steps'], performance_df['loss'])\n",
    "    axes[0,0].set_title('Training Loss')\n",
    "    axes[0,0].set_xlabel('Steps')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    \n",
    "    axes[0,1].plot(performance_df['steps'], performance_df['learning_rate'])\n",
    "    axes[0,1].set_title('Learning Rate')\n",
    "    axes[0,1].set_xlabel('Steps')\n",
    "    axes[0,1].set_ylabel('Learning Rate')\n",
    "    \n",
    "    axes[1,0].plot(performance_df['steps'], performance_df['curriculum_level'])\n",
    "    axes[1,0].set_title('Curriculum Difficulty')\n",
    "    axes[1,0].set_xlabel('Steps')\n",
    "    axes[1,0].set_ylabel('Difficulty Level')\n",
    "    \n",
    "    axes[1,1].plot(performance_df['steps'], performance_df['expert_usage'])\n",
    "    axes[1,1].set_title('Expert Usage')\n",
    "    axes[1,1].set_xlabel('Steps')\n",
    "    axes[1,1].set_ylabel('Average Usage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{DRIVE_DIR}/training_progress.png\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization and performance tracking initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create neural memory and model arguments\n",
    "model_args = ModelArgs(\n",
    "    max_batch_size=4,\n",
    "    max_seq_len=2048,\n",
    "    dtype=\"fp8\",\n",
    "    vocab_size=32000,\n",
    "    dim=1024,\n",
    "    inter_dim=2816,\n",
    "    moe_inter_dim=512,\n",
    "    n_layers=12,\n",
    "    n_dense_layers=1,\n",
    "    n_heads=16,\n",
    "    n_routed_experts=8,\n",
    "    n_shared_experts=1,\n",
    "    n_activated_experts=2,\n",
    "    n_expert_groups=1,\n",
    "    n_limited_groups=1,\n",
    "    score_func=\"softmax\",\n",
    "    route_scale=1.0,\n",
    "    q_lora_rank=0,\n",
    "    kv_lora_rank=64,\n",
    "    qk_nope_head_dim=64,\n",
    "    qk_rope_head_dim=32,\n",
    "    v_head_dim=64,\n",
    "    original_seq_len=2048,\n",
    "    rope_theta=10000.0,\n",
    "    rope_factor=20,\n",
    "    beta_fast=16,\n",
    "    beta_slow=1,\n",
    "    mscale=0.5,\n",
    "    use_alibi=False,\n",
    "    use_rope_scaling=True,\n",
    "    gradient_checkpointing=True,\n",
    "    parallel_attn=True,\n",
    "    rope_condense_ratio=1.0\n",
    ")\n",
    "\n",
    "neural_memory = NeuralMemory(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training components\n",
    "tot_config = TreeConfig(\n",
    "    max_branches=4,\n",
    "    max_depth=3,\n",
    "    beam_width=2,\n",
    "    reward_gamma=0.95\n",
    ")\n",
    "\n",
    "reward_config = RewardConfig(\n",
    "    reasoning_weight=0.4,\n",
    "    accuracy_weight=0.4,\n",
    "    consistency_weight=0.2\n",
    ")\n",
    "\n",
    "curriculum_config = CurriculumConfig(\n",
    "    min_sequence_length=32,\n",
    "    max_sequence_length=512,\n",
    "    min_vocab_complexity=0.3,\n",
    "    max_vocab_complexity=1.0,\n",
    "    min_reasoning_steps=1,\n",
    "    max_reasoning_steps=8,\n",
    "    pacing_function='root',\n",
    "    total_curriculum_steps=10000,\n",
    "    performance_threshold=0.8,\n",
    "    min_samples_before_advance=100,\n",
    "    smoothing_factor=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer on dataset texts\n",
    "print(\"Training tokenizer...\")\n",
    "\n",
    "import logging\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure tokenizer directory exists\n",
    "tokenizer_dir = Path(\"tokenizer\")\n",
    "tokenizer_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_training_data(datasets, max_samples=10000):\n",
    "    logger.info(\"Preparing training data...\")\n",
    "    train_path = tokenizer_dir / \"train.txt\"\n",
    "    \n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for name, dataset in datasets.items():\n",
    "            logger.info(f\"Processing {name} dataset\")\n",
    "            if name in [\"gsm8k\", \"mmlu\"]:\n",
    "                for i, item in enumerate(dataset):\n",
    "                    if i >= max_samples:\n",
    "                        break\n",
    "                    if \"question\" in item and \"answer\" in item:\n",
    "                        f.write(f\"{item['question']}\\n\")\n",
    "                        f.write(f\"{item['answer']}\\n\")\n",
    "            elif name == \"code\":\n",
    "                for i, item in enumerate(dataset):\n",
    "                    if i >= max_samples:\n",
    "                        break\n",
    "                    if \"content\" in item:\n",
    "                        f.write(f\"{item['content']}\\n\")\n",
    "                        \n",
    "    return train_path\n",
    "\n",
    "try:\n",
    "    # Initialize tokenizer with reduced vocabulary size\n",
    "    tokenizer_config = TokenizerConfig(\n",
    "        vocab_size=26519,  # Reduced vocab size\n",
    "        model_prefix=\"vishwamai\",\n",
    "        character_coverage=0.9995,\n",
    "        max_sentence_length=2048,\n",
    "        pad_id=0,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        unk_id=3\n",
    "    )\n",
    "    \n",
    "    # Create training data file\n",
    "    train_path = prepare_training_data(datasets)\n",
    "    logger.info(f\"Training data saved to {train_path}\")\n",
    "    \n",
    "    # Train SentencePiece model\n",
    "    model_prefix = str(tokenizer_dir / tokenizer_config.model_prefix)\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=str(train_path),\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=tokenizer_config.vocab_size,\n",
    "        character_coverage=tokenizer_config.character_coverage,\n",
    "        model_type=\"bpe\",\n",
    "        max_sentence_length=tokenizer_config.max_sentence_length,\n",
    "        pad_id=tokenizer_config.pad_id,\n",
    "        bos_id=tokenizer_config.bos_id,\n",
    "        eos_id=tokenizer_config.eos_id,\n",
    "        unk_id=tokenizer_config.unk_id,\n",
    "        input_sentence_size=10000000,\n",
    "        shuffle_input_sentence=True,\n",
    "        train_extremely_large_corpus=True\n",
    "    )\n",
    "    \n",
    "    # Load the trained tokenizer\n",
    "    tokenizer = VishwamAITokenizer(tokenizer_config)\n",
    "    tokenizer.load(f\"{model_prefix}.model\")\n",
    "    logger.info(\"Tokenizer training complete\")\n",
    "    \n",
    "    # Verify tokenizer works\n",
    "    test_text = \"Hello world\"\n",
    "    encoded = tokenizer.encode(test_text)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    logger.info(f\"Tokenizer test - Encoded: {encoded}\")\n",
    "    logger.info(f\"Tokenizer test - Decoded: {decoded}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during tokenizer training: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process datasets with trained tokenizer\n",
    "print(\"Loading datasets...\")\n",
    "datasets = {\n",
    "    \"gsm8k\": load_dataset(\"gsm8k\", split=\"train\"),\n",
    "    \"mmlu\": load_dataset(\"cais/mmlu\", split=\"train\"),\n",
    "    \"code\": load_dataset(\"codeparrot/github-code\", split=\"train\")\n",
    "}\n",
    "\n",
    "def process_dataset(examples, dataset_type):\n",
    "    if dataset_type in [\"gsm8k\", \"mmlu\"]:\n",
    "        text = [f\"Question: {q}\\nAnswer: {a}\" \n",
    "                for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "    else:\n",
    "        text = examples[\"content\"]\n",
    "\n",
    "    encoded = []\n",
    "    attention_mask = []\n",
    "    max_len = 2048\n",
    "\n",
    "    for t in text:\n",
    "        # Encode with trained tokenizer\n",
    "        ids = tokenizer.encode(\n",
    "            t,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Pad sequence\n",
    "        padding_length = max_len - len(ids)\n",
    "        if padding_length > 0:\n",
    "            ids = ids + [tokenizer.config.pad_id] * padding_length\n",
    "            mask = [1] * (max_len - padding_length) + [0] * padding_length\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "            mask = [1] * max_len\n",
    "            \n",
    "        encoded.append(ids)\n",
    "        attention_mask.append(mask)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": encoded,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "# Process datasets\n",
    "processed_datasets = {}\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    processed_datasets[name] = dataset.map(\n",
    "        lambda x: process_dataset(x, name),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    print(f\"Processed {len(processed_datasets[name])} examples from {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, _ = create_model(config)\n",
    "model = model.to(device)\n",
    "main(model)\n",
    "\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    memory_size=512,\n",
    "    cache_size=256,\n",
    "    tot_config=tot_config,\n",
    "    reward_config=reward_config,\n",
    "    curriculum_config=curriculum_config\n",
    ")\n",
    "\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"vishwamai-training\",\n",
    "    config={\n",
    "        \"model\": config,\n",
    "        \"curriculum\": curriculum_config.__dict__,\n",
    "        \"tot\": tot_config.__dict__\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Performance Tracking\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"vishwamai-training\")\n",
    "\n",
    "performance_data = []\n",
    "\n",
    "try:\n",
    "    for step in tqdm(range(config[\"max_steps\"])):\n",
    "        stats = trainer.train_step()\n",
    "        update_performance_tracking(stats, step)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"loss\": stats[\"loss\"],\n",
    "            \"learning_rate\": stats[\"lr\"],\n",
    "            \"batch_size\": stats[\"batch_size\"],\n",
    "            \"curriculum_level\": stats[\"curriculum_stats\"][\"current_difficulty\"],\n",
    "            \"memory_usage\": stats[\"memory_usage\"][\"allocated\"],\n",
    "            \"moe_loss\": stats.get(\"moe_loss\", 0),\n",
    "            \"gradient_norm\": stats[\"gradient_norm\"],\n",
    "            \"expert_usage\": stats.get(\"moe_metrics\", {})\n",
    "        })\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            plot_training_progress()\n",
    "            checkpoint_path = f\"{CHECKPOINT_DIR}/step_{step}.pt\"\n",
    "            trainer.save_checkpoint(checkpoint_path)\n",
    "            \n",
    "            trainer.push_to_hub(\n",
    "                \"VishwamAI/VishwamAI\",\n",
    "                commit_message=f\"Training checkpoint at step {step}\"\n",
    "            )\n",
    "            \n",
    "        if step % 5000 == 0:\n",
    "            print(f\"\\nEvaluating at step {step}...\")\n",
    "            eval_metrics = trainer.evaluate()\n",
    "            wandb.log({\"eval\": eval_metrics})\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted. Saving final visualization...\")\n",
    "    plot_training_progress()\n",
    "    trainer.save_checkpoint(f\"{CHECKPOINT_DIR}/interrupted.pt\")\n",
    "\n",
    "plot_training_progress()\n",
    "performance_df.to_csv(f\"{DRIVE_DIR}/training_metrics.csv\", index=False)\n",
    "print(\"Training complete with performance tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Performance Graphs\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(performance_df[\"step\"], performance_df[\"loss\"], label=\"Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{DRIVE_DIR}/training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(performance_df[\"step\"], performance_df[\"memory_usage\"], label=\"Memory Usage (GB)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Memory Usage (GB)\")\n",
    "plt.title(\"Memory Usage Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{DRIVE_DIR}/memory_usage.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "\n",
    "eval_datasets = [\n",
    "    \"gsm8k\",\n",
    "    \"TIGER-Lab/MMLU-Pro\",\n",
    "    \"MMMU/MMMU\",\n",
    "    \"microsoft/SCBench\",\n",
    "    \"camel-ai/math\",\n",
    "    \"camel-ai/code\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for dataset in eval_datasets:\n",
    "    print(f\"\\nEvaluating on {dataset}...\")\n",
    "    try:\n",
    "        eval_data = load_dataset(dataset, split=\"test\")\n",
    "        metrics = trainer.evaluate(eval_data)\n",
    "        results[dataset] = metrics\n",
    "        print(f\"{dataset}: {metrics}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {dataset}: {str(e)}\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open(f\"{DRIVE_DIR}/final_evaluation.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
