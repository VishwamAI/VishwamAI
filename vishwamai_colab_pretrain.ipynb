{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Git LFS\n",
        "!apt-get install git-lfs -y\n",
        "!git lfs install\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
        "%cd VishwamAI\n",
        "\n",
        "# Install the package\n",
        "!pip install -e . -q\n",
        "\n",
        "\n",
        "# Configure Git LFS\n",
        "!git config lfs.url https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
        "!git config lfs.pushurl https://huggingface.co/kasinadhsarma/vishwamai-model.git/info/lfs\n",
        "\n",
        "# Set up Git LFS tracking\n",
        "!git lfs track \"*.bin\"\n",
        "!git lfs track \"*.pt\"\n",
        "!git lfs track \"*.pth\"\n",
        "!git lfs track \"*.ckpt\"\n",
        "!git lfs track \"*.safetensors\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou4YTR5UoELI"
      },
      "outputs": [],
      "source": [
        "# First cell - Add all required imports\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.distributed as dist\n",
        "from tqdm.notebook import tqdm\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Import VishwamAI components\n",
        "from vishwamai.model import Transformer, ModelArgs\n",
        "from vishwamai.model_utils import get_gpu_memory, load_model\n",
        "from vishwamai.cache_augmentation import CacheConfig, DifferentiableCacheAugmentation  \n",
        "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
        "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
        "from vishwamai.reward_function import RewardConfig\n",
        "from vishwamai.trainer import VishwamAIPretrainer\n",
        "\n",
        "# Define GPU memory utility function \n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize components before using\n",
        "model = None \n",
        "cache_module = None\n",
        "memory_module = None\n",
        "tree_module = None\n",
        "reward_config = None\n",
        "train_dataset = None\n",
        "eval_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-KlPRYJoELL",
        "outputId": "bee65098-6c23-4bdd-c604-f3bf5ff9afc5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Install Git LFS\n",
        "!apt-get install git-lfs\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wja3KBfdoELM",
        "outputId": "8b5a9d03-b974-4184-dbce-8fb01f3019c3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Verify GPU availability and requirements\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Memory optimization for T4\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "print(f\"Using GPU: {gpu_name}\")\n",
        "\n",
        "# Set memory optimization flags for T4\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8ucxo0oELN",
        "outputId": "3260c33a-6d08-4120-831a-70f8daa22fc8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Package installation with T4 optimized versions\n",
        "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
        "    transformers==4.34.0 datasets accelerate huggingface_hub wandb bitsandbytes -q\n",
        "%pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "IbKumFgsoELN",
        "outputId": "04c0f9a4-d1c6-41d0-dd0e-538e18af04d7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from huggingface_hub import login, create_repo\n",
        "from getpass import getpass\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "# Get token securely\n",
        "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
        "login(token=hf_token)\n",
        "print(\"Successfully logged in to Hugging Face!\")\n",
        "\n",
        "# Initialize W&B for experiment tracking\n",
        "wandb.login()\n",
        "print(\"Successfully logged in to Weights & Biases!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlmaEAzJrTHy",
        "outputId": "045c2d1c-5f00-4a60-c19c-970889e7e8c2"
      },
      "outputs": [],
      "source": [
        "pip install datasets bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_cyeDm4oELO",
        "outputId": "7041be3b-c6d7-490b-9461-6da569d06672"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import torch\n",
        "import json\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from vishwamai.model_utils import load_model, get_gpu_memory\n",
        "from vishwamai.model import Transformer, ModelArgs\n",
        "from vishwamai.cache_augmentation import CacheConfig, DifferentiableCacheAugmentation\n",
        "from vishwamai.neural_memory import ReasoningMemoryTransformer\n",
        "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
        "from vishwamai.reward_function import RewardConfig\n",
        "from vishwamai.trainer import VishwamAIPretrainer\n",
        "\n",
        "# T4-specific performance optimizations\n",
        "import bitsandbytes as bnb\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMLbaWe9oELO",
        "outputId": "8bec5d7f-b310-4ba3-bb46-217c3051164b"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def setup_hardware():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = get_gpu_memory()\n",
        "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
        "\n",
        "    # T4-optimized configuration\n",
        "    if 't4' in gpu_name.lower():\n",
        "        variant = \"7B\"  # T4-optimized model\n",
        "        print(\"Using T4-optimized configuration with 8-bit quantization\")\n",
        "    else:\n",
        "        variant = \"167B\"  # Fallback configuration\n",
        "        print(\"Using fallback configuration\")\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return variant\n",
        "\n",
        "model_variant = setup_hardware()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHFppk6soELP",
        "outputId": "add25fa4-778f-4066-bf9e-86b98a59a951"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def load_config():\n",
        "    config_path = \"./vishwamai/configs/config_optimized.json\"\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    if model_variant not in config[\"model_variants\"]:\n",
        "        print(f\"Warning: Model variant '{model_variant}' not found in config, creating T4 optimized config\")\n",
        "        # T4-optimized configuration\n",
        "        t4_config = {\n",
        "            \"max_batch_size\": 4,\n",
        "            \"max_seq_len\": 2048,\n",
        "            \"dtype\": \"fp8\",\n",
        "            \"vocab_size\": 32000,\n",
        "            \"dim\": 1024,\n",
        "            \"inter_dim\": 2816,\n",
        "            \"moe_inter_dim\": 512,\n",
        "            \"n_layers\": 12,\n",
        "            \"n_dense_layers\": 1,\n",
        "            \"n_heads\": 16,\n",
        "            \"n_routed_experts\": 8,\n",
        "            \"n_shared_experts\": 1,\n",
        "            \"n_activated_experts\": 2,\n",
        "            \"n_expert_groups\": 1,\n",
        "            \"n_limited_groups\": 1,\n",
        "            \"score_func\": \"softmax\",\n",
        "            \"route_scale\": 1.0,\n",
        "            \"q_lora_rank\": 0,\n",
        "            \"kv_lora_rank\": 64,\n",
        "            \"qk_nope_head_dim\": 64,\n",
        "            \"qk_rope_head_dim\": 32,\n",
        "            \"v_head_dim\": 64,\n",
        "            \"original_seq_len\": 2048,\n",
        "            \"rope_theta\": 10000.0,\n",
        "            \"rope_factor\": 20,\n",
        "            \"beta_fast\": 16,\n",
        "            \"beta_slow\": 1,\n",
        "            \"mscale\": 0.5,\n",
        "            \"use_alibi\": False,  # Disable ALiBi for T4\n",
        "            \"use_rope_scaling\": True,\n",
        "            \"gradient_checkpointing\": True,\n",
        "            \"parallel_attn\": True,\n",
        "            \"rope_condense_ratio\": 1.0\n",
        "        }\n",
        "        return t4_config\n",
        "\n",
        "    return config[\"model_variants\"][model_variant][\"model_config\"]\n",
        "\n",
        "# Load configuration\n",
        "model_config = load_config()\n",
        "print(\"Configuration loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_83lo883oELQ"
      },
      "outputs": [],
      "source": [
        "# Create DeepSpeed config for T4 optimization\n",
        "ds_config = {\n",
        "    \"fp16\": {\n",
        "        \"enabled\": True,\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 100,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"allgather_bucket_size\": 5e8,\n",
        "        \"reduce_bucket_size\": 5e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": 32,\n",
        "    \"gradient_accumulation_steps\": 16,\n",
        "    \"train_micro_batch_size_per_gpu\": 2,\n",
        "    \"gradient_clipping\": 0.5,\n",
        "    \"steps_per_print\": 10,\n",
        "    \"wall_clock_breakdown\": False\n",
        "}\n",
        "\n",
        "with open('ds_config.json', 'w') as f:\n",
        "    json.dump(ds_config, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D507tdS0oELQ"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def initialize_components():\n",
        "    print(\"Initializing model and components...\")\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    # Initialize main model with 8-bit quantization for T4\n",
        "    model_args = ModelArgs(**model_config)\n",
        "    model = Transformer(model_args)\n",
        "\n",
        "    # Replace LinearWrapper with current bitsandbytes 8-bit quantization\n",
        "    import bitsandbytes as bnb\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            model._modules[name] = bnb.nn.Linear8bitLt(\n",
        "                module.in_features,\n",
        "                module.out_features,\n",
        "                module.bias is not None,\n",
        "                has_fp16_weights=False,\n",
        "                threshold=6.0\n",
        "            )\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Initialize smaller cache augmentation for T4\n",
        "    cache_config = CacheConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"],\n",
        "        max_cache_length=8192,  # Reduced cache size for T4\n",
        "        dropout=0.1\n",
        "    )\n",
        "    cache_module = DifferentiableCacheAugmentation(cache_config).cuda()\n",
        "\n",
        "    # Initialize advanced memory transformer\n",
        "    memory_config = AdvancedMemoryConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_attention_heads=model_config[\"n_heads\"],\n",
        "        memory_size=8192,  # Adjust based on available GPU memory\n",
        "        use_hierarchical=True,\n",
        "        use_compressed=True\n",
        "    )\n",
        "    memory_module = AdvancedReasoningMemoryTransformer(memory_config).cuda()\n",
        "\n",
        "    # Initialize tree of thoughts with reduced beam size\n",
        "    tree_module = TreeOfThoughts(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"]\n",
        "    ).cuda()\n",
        "\n",
        "    # Initialize reward config\n",
        "    reward_config = RewardConfig(\n",
        "        hidden_size=model_config[\"dim\"],\n",
        "        num_heads=model_config[\"n_heads\"]\n",
        "    )\n",
        "\n",
        "    clear_gpu_memory()\n",
        "    return model, cache_module, memory_module, tree_module, reward_config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdnvhouMoELR",
        "outputId": "d95a370d-80d2-4880-dcab-f6520a5fb553"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Initialize output directory\n",
        "output_dir = \"./pretrain_output\"\n",
        "!mkdir -p $output_dir\n",
        "\n",
        "# Configure training with T4 optimizations\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,  # Reduced batch size for T4\n",
        "    gradient_accumulation_steps=16,  # Increased for T4 memory constraints\n",
        "    learning_rate=5e-5,  # Reduced learning rate\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # Mixed precision training\n",
        "    fp16=True,  # Use FP16 instead of BF16 for T4\n",
        "    bf16=False,\n",
        "    # Performance optimizations\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_num_workers=2,  # Reduced workers for T4\n",
        "    dataloader_pin_memory=True,\n",
        "    group_by_length=True,\n",
        "    # Memory optimizations\n",
        "    max_grad_norm=0.5,  # Reduced for stability\n",
        "    # Monitoring\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    # Hub integration\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
        "    hub_strategy=\"end\",  # Only save at the end to save memory\n",
        "    # Optimizer settings\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",  # Use 8-bit Adam\n",
        "    # Other settings\n",
        "    remove_unused_columns=False,\n",
        "    seed=42,\n",
        "    ddp_find_unused_parameters=False,\n",
        "    # Memory optimization\n",
        "    deepspeed=\"ds_config.json\"  # Using the config we created\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "4ecbe6a10cb94f9dabd795b13c246488",
            "70eb6b9ec335422b99449f3f7dd4a425",
            "d5d6c297b3f945f6b9576c9b42fe0c6e",
            "6fba7f5653ec412494048be0a9f20e47",
            "924187da33f343639b142750b2deabdd",
            "634dc60c148f4f62ab5e4622dbfddff5",
            "a12411b1ca884abdbfb73f34471243f3",
            "2bbe091a56c94a38b8d3750ef24f2f87",
            "f79caf69e622498ab2a69ca3166f64b7",
            "6ce0261c60884cd08f4c961e36938678",
            "fd3e0081178f4d4c98bb6942f725f32d",
            "680f691dd8f044a4b7d6ce5853e9b4ee",
            "d7e9ab21ba7c4dc4b83f7e0cef9f4444",
            "29ef180cd9124dff88a226346ea5334c",
            "d360f26f7a52466f9ec5404f5c428a76",
            "0914868678b34ec69721e5470a4c4bf5",
            "02367749ebc144f691ae485b070702a0",
            "93b1eb39575f4b4982bd12ad9e070080",
            "68a37863fedb4df9be407efe91027014",
            "8543b4cffada4069baf83b02931aedc9",
            "2b41434cb15642f3913db7e723501d16",
            "7a5d0c457ee945d783cd7736b3070246"
          ]
        },
        "id": "Z9mhvSB3oELR",
        "outputId": "30ed13e0-ca4a-4b04-d3a9-c776d54968c2"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Load and combine training datasets with memory optimization\n",
        "def load_dataset_with_memory_optimization(ds_name, split):\n",
        "    clear_gpu_memory()\n",
        "    try:\n",
        "        dataset = load_dataset(ds_name, split=split, streaming=True)  # Use streaming for memory efficiency\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {ds_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "train_datasets = []\n",
        "for ds_name in [\"gsm8k\", \"cais/mmlu\"]:\n",
        "    dataset = load_dataset_with_memory_optimization(ds_name, \"train\")\n",
        "    if dataset is not None:\n",
        "        train_datasets.append(dataset)\n",
        "\n",
        "if not train_datasets:\n",
        "    raise ValueError(\"No training datasets could be loaded\")\n",
        "\n",
        "combined_train_dataset = concatenate_datasets(train_datasets)\n",
        "\n",
        "# Load validation dataset\n",
        "eval_dataset = load_dataset_with_memory_optimization(\"cais/mmlu\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new-cell"
      },
      "outputs": [],
      "source": [
        "# Initialize model and components before training\n",
        "print(\"Initializing components...\")\n",
        "model, cache_module, memory_module, tree_module, reward_config = initialize_components()\n",
        "print(\"Components initialized successfully\")\n",
        "\n",
        "# Make sure datasets are defined\n",
        "if 'train_dataset' not in locals() or 'eval_dataset' not in locals():\n",
        "    print(\"Loading datasets...\")\n",
        "    train_dataset = load_dataset(\"gsm8k\", split=\"train\", streaming=True)\n",
        "    eval_dataset = load_dataset(\"cais/mmlu\", split=\"validation\", streaming=True)\n",
        "    print(\"Datasets loaded successfully\")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "@track_time\n",
        "def train_model():\n",
        "    global trainer  # Make trainer accessible globally\n",
        "    trainer = VishwamAIPretrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        memory_module=memory_module,\n",
        "        tree_module=tree_module, \n",
        "        cache_module=cache_module,\n",
        "        reward_config=reward_config\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model(\"./final_model\")\n",
        "        trainer.push_to_hub(\n",
        "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "        )\n",
        "        print(\"Model training and saving completed successfully\")\n",
        "        return trainer\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_gpu_memory()\n",
        "        raise e\n",
        "\n",
        "trainer = train_model()  # Store trainer instance for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "V_4Ddy6coELR",
        "outputId": "80d35ea5-a610-448e-9080-b526e67a90df"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def train_model():\n",
        "    trainer = VishwamAIPretrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        memory_module=memory_module,\n",
        "        tree_module=tree_module,\n",
        "        cache_module=cache_module,\n",
        "        reward_config=reward_config\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "\n",
        "        # Save model and components\n",
        "        trainer.save_model(\"./final_model\")\n",
        "        print(\"Model saved successfully\")\n",
        "\n",
        "        # Push to hub with LFS\n",
        "        trainer.push_to_hub(\n",
        "            commit_message=f\"Training completed - {time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "        )\n",
        "        print(\"Model pushed to HuggingFace Hub\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "        clear_gpu_memory()\n",
        "        raise e\n",
        "\n",
        "train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "OTsLdBhzoELR",
        "outputId": "1420331b-6285-4d67-f371-e7117c097aff"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def save_model():\n",
        "    clear_gpu_memory()\n",
        "    model_save_path = \"final_model\"\n",
        "    trainer.save_model(model_save_path)\n",
        "\n",
        "    # Initialize Git LFS tracking for the saved model files\n",
        "    !git lfs track \"final_model/*.bin\"\n",
        "    !git lfs track \"final_model/*.pt\"\n",
        "    !git lfs track \"final_model/*.pth\"\n",
        "\n",
        "    print(\"Model and components saved successfully\")\n",
        "    return model_save_path\n",
        "\n",
        "model_save_path = save_model()\n",
        "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2NgnZFXoELS"
      },
      "outputs": [],
      "source": [
        "@track_time\n",
        "def validate_model():\n",
        "    clear_gpu_memory()\n",
        "    # Load all components for validation with 8-bit quantization\n",
        "    test_model = Transformer(ModelArgs(**model_config))\n",
        "    test_model = bnb.nn.LinearWrapper.wrap_model(test_model, device='cuda', quantize=True)\n",
        "    test_model.load_state_dict(torch.load(f\"{model_save_path}/pytorch_model.bin\"))\n",
        "\n",
        "    # Load auxiliary components\n",
        "    test_cache = DifferentiableCacheAugmentation.from_pretrained(model_save_path)\n",
        "    test_memory = ReasoningMemoryTransformer.from_pretrained(model_save_path)\n",
        "    test_tree = TreeOfThoughts.from_pretrained(model_save_path)\n",
        "\n",
        "    test_model.eval()\n",
        "    test_cache.eval()\n",
        "    test_memory.eval()\n",
        "    test_tree.eval()\n",
        "\n",
        "    test_cases = [\n",
        "        \"What is 7 * 12?\",\n",
        "        \"Explain quantum computing in simple terms.\",\n",
        "        \"Write a Python function to find prime numbers.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Running validation tests...\")\n",
        "    for test_input in test_cases:\n",
        "        print(f\"\\nTest: {test_input}\")\n",
        "        clear_gpu_memory()\n",
        "        # Note: You'll need to implement tokenization for the actual input\n",
        "        tokens = torch.randint(0, model_config['vocab_size'], (1, 32)).cuda()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            start = time.time()\n",
        "            output = test_model(tokens)\n",
        "            end = time.time()\n",
        "\n",
        "            # Apply enhancements with memory management\n",
        "            enhanced_states = test_cache(output)\n",
        "            memory_enhanced = test_memory(enhanced_states)\n",
        "            final_output = test_tree(memory_enhanced)\n",
        "\n",
        "        print(f\"Generated response in {end-start:.2f}s\")\n",
        "        # Note: You'll need to implement detokenization for the actual output\n",
        "\n",
        "validate_model()\n",
        "print(\"\\nPretraining and validation completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02367749ebc144f691ae485b070702a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0914868678b34ec69721e5470a4c4bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ef180cd9124dff88a226346ea5334c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a37863fedb4df9be407efe91027014",
            "max": 138338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8543b4cffada4069baf83b02931aedc9",
            "value": 138338
          }
        },
        "2b41434cb15642f3913db7e723501d16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bbe091a56c94a38b8d3750ef24f2f87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ecbe6a10cb94f9dabd795b13c246488": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70eb6b9ec335422b99449f3f7dd4a425",
              "IPY_MODEL_d5d6c297b3f945f6b9576c9b42fe0c6e",
              "IPY_MODEL_6fba7f5653ec412494048be0a9f20e47"
            ],
            "layout": "IPY_MODEL_924187da33f343639b142750b2deabdd"
          }
        },
        "634dc60c148f4f62ab5e4622dbfddff5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680f691dd8f044a4b7d6ce5853e9b4ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7e9ab21ba7c4dc4b83f7e0cef9f4444",
              "IPY_MODEL_29ef180cd9124dff88a226346ea5334c",
              "IPY_MODEL_d360f26f7a52466f9ec5404f5c428a76"
            ],
            "layout": "IPY_MODEL_0914868678b34ec69721e5470a4c4bf5"
          }
        },
        "68a37863fedb4df9be407efe91027014": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce0261c60884cd08f4c961e36938678": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fba7f5653ec412494048be0a9f20e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce0261c60884cd08f4c961e36938678",
            "placeholder": "​",
            "style": "IPY_MODEL_fd3e0081178f4d4c98bb6942f725f32d",
            "value": " 53.2k/53.2k [00:00&lt;00:00, 3.22MB/s]"
          }
        },
        "70eb6b9ec335422b99449f3f7dd4a425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_634dc60c148f4f62ab5e4622dbfddff5",
            "placeholder": "​",
            "style": "IPY_MODEL_a12411b1ca884abdbfb73f34471243f3",
            "value": "README.md: 100%"
          }
        },
        "7a5d0c457ee945d783cd7736b3070246": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8543b4cffada4069baf83b02931aedc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "924187da33f343639b142750b2deabdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b1eb39575f4b4982bd12ad9e070080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a12411b1ca884abdbfb73f34471243f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d360f26f7a52466f9ec5404f5c428a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b41434cb15642f3913db7e723501d16",
            "placeholder": "​",
            "style": "IPY_MODEL_7a5d0c457ee945d783cd7736b3070246",
            "value": " 138k/138k [00:00&lt;00:00, 10.0MB/s]"
          }
        },
        "d5d6c297b3f945f6b9576c9b42fe0c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bbe091a56c94a38b8d3750ef24f2f87",
            "max": 53221,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f79caf69e622498ab2a69ca3166f64b7",
            "value": 53221
          }
        },
        "d7e9ab21ba7c4dc4b83f7e0cef9f4444": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02367749ebc144f691ae485b070702a0",
            "placeholder": "​",
            "style": "IPY_MODEL_93b1eb39575f4b4982bd12ad9e070080",
            "value": "dataset_infos.json: 100%"
          }
        },
        "f79caf69e622498ab2a69ca3166f64b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd3e0081178f4d4c98bb6942f725f32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
