{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od5-dq6Jkqp5"
      },
      "source": [
        "# VishwamAI Colab Training\n",
        "\n",
        "Training with custom VishwamAI Transformer architecture using advanced features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ8aaEIdkqp7",
        "outputId": "dc48c2b8-bd1c-42ea-9719-2d09bcd1bab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Feb 18 07:25:25 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Setup Environment\n",
        "!nvidia-smi\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup directories\n",
        "import os\n",
        "DRIVE_DIR = '/content/drive/MyDrive/VishwamAI'\n",
        "CHECKPOINT_DIR = f'{DRIVE_DIR}/checkpoints'\n",
        "!mkdir -p {CHECKPOINT_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P81h1p2Xkqp9",
        "outputId": "bc845b91-d6b2-4d16-d231-7d4b2f006d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'VishwamAI' already exists and is not an empty directory.\n",
            "/content/VishwamAI\n",
            "Obtaining file:///content/VishwamAI\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from vishwamai==0.1.1) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from vishwamai==0.1.1) (1.26.4)\n",
            "Requirement already satisfied: transformers>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from vishwamai==0.1.1) (4.48.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.11/dist-packages (from vishwamai==0.1.1) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from vishwamai==0.1.1) (4.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vishwamai==0.1.1) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->vishwamai==0.1.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->vishwamai==0.1.1) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.5.0->vishwamai==0.1.1) (0.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->vishwamai==0.1.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0->vishwamai==0.1.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0->vishwamai==0.1.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0->vishwamai==0.1.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.5.0->vishwamai==0.1.1) (2025.1.31)\n",
            "Installing collected packages: vishwamai\n",
            "  Attempting uninstall: vishwamai\n",
            "    Found existing installation: vishwamai 0.1.1\n",
            "    Uninstalling vishwamai-0.1.1:\n",
            "      Successfully uninstalled vishwamai-0.1.1\n",
            "  Running setup.py develop for vishwamai\n",
            "Successfully installed vishwamai-0.1.1\n",
            "HUGGINGFACE_TOKEN not found in environment\n",
            "Enter your Hugging Face token (input will be hidden): ··········\n",
            "Successfully logged in to Hugging Face\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies and clone repo\n",
        "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
        "%cd VishwamAI\n",
        "\n",
        "%pip install -q torch transformers datasets accelerate bitsandbytes wandb triton\n",
        "%pip install -e .\n",
        "\n",
        "# Secure Hugging Face authentication\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "def get_huggingface_token():\n",
        "    \"\"\"Get Hugging Face token from environment or prompt\"\"\"\n",
        "    token = os.getenv('HUGGINGFACE_TOKEN')\n",
        "    if not token:\n",
        "        print(\"HUGGINGFACE_TOKEN not found in environment\")\n",
        "        token = getpass.getpass('Enter your Hugging Face token (input will be hidden): ')\n",
        "        # Store temporarily for this session\n",
        "        os.environ['HUGGINGFACE_TOKEN'] = token\n",
        "    return token\n",
        "\n",
        "try:\n",
        "    token = get_huggingface_token()\n",
        "    login(token=token)\n",
        "    print(\"Successfully logged in to Hugging Face\")\n",
        "except Exception as e:\n",
        "    print(f\"Error logging in to Hugging Face: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-soEwOn8kqp-"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import VishwamAI components with correct paths\n",
        "from vishwamai.base_layers import Linear  # Import from base_layers instead of utils\n",
        "from vishwamai.Transformer import Transformer\n",
        "from vishwamai import (\n",
        "    create_model,\n",
        "    ModelArgs,\n",
        "    VishwamAITokenizer,\n",
        "    TokenizerConfig\n",
        ")\n",
        "\n",
        "# Import training components\n",
        "from vishwamai.advanced_training import AdvancedTrainer\n",
        "from vishwamai.fp8_cast_bf16 import main\n",
        "from vishwamai.neural_memory import NeuralMemory\n",
        "from vishwamai.tree_of_thoughts import TreeConfig, RewardConfig\n",
        "from vishwamai.curriculum import CurriculumConfig\n",
        "from vishwamai.initialize import initialize_model_and_trainer\n",
        "from vishwamai.config import ModelArgs\n",
        "from vishwamai.utils import precompute_freqs_cis\n",
        "from vishwamai.advanced_training import AdvancedTrainer\n",
        "from vishwamai.tree_of_thoughts import TreeConfig, RewardConfig\n",
        "from vishwamai.curriculum import CurriculumConfig\n",
        "from vishwamai.neural_memory import NeuralMemory\n",
        "\n",
        "# Import visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Verify imports were successful\n",
        "print(\"Imports completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdhWR3SekqqA"
      },
      "outputs": [],
      "source": [
        "# Initialize visualization and analysis tools\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure plotting style\n",
        "# plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "# Initialize performance tracking\n",
        "performance_history = {\n",
        "    'steps': [],\n",
        "    'loss': [],\n",
        "    'learning_rate': [],\n",
        "    'memory_usage': [],\n",
        "    'curriculum_level': [],\n",
        "    'expert_usage': [],\n",
        "    'evaluation_scores': []\n",
        "}\n",
        "\n",
        "performance_df = pd.DataFrame(performance_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEpjze3xkqqA"
      },
      "outputs": [],
      "source": [
        "def update_performance_tracking(stats, step):\n",
        "    \"\"\"Update performance tracking with new statistics\"\"\"\n",
        "    performance_df.loc[len(performance_df)] = {\n",
        "        'steps': step,\n",
        "        'loss': stats['loss'],\n",
        "        'learning_rate': stats['lr'],\n",
        "        'memory_usage': stats['memory_usage']['allocated'],\n",
        "        'curriculum_level': stats['curriculum_stats']['current_difficulty'],\n",
        "        'expert_usage': sum(stats.get('moe_metrics', {}).values()) / len(stats.get('moe_metrics', {})),\n",
        "        'evaluation_scores': stats.get('eval_score', 0)\n",
        "    }\n",
        "\n",
        "def plot_training_progress():\n",
        "    \"\"\"Generate training progress visualization\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Training Progress Overview', fontsize=16)\n",
        "\n",
        "    axes[0,0].plot(performance_df['steps'], performance_df['loss'])\n",
        "    axes[0,0].set_title('Training Loss')\n",
        "    axes[0,0].set_xlabel('Steps')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "\n",
        "    axes[0,1].plot(performance_df['steps'], performance_df['learning_rate'])\n",
        "    axes[0,1].set_title('Learning Rate')\n",
        "    axes[0,1].set_xlabel('Steps')\n",
        "    axes[0,1].set_ylabel('Learning Rate')\n",
        "\n",
        "    axes[1,0].plot(performance_df['steps'], performance_df['curriculum_level'])\n",
        "    axes[1,0].set_title('Curriculum Difficulty')\n",
        "    axes[1,0].set_xlabel('Steps')\n",
        "    axes[1,0].set_ylabel('Difficulty Level')\n",
        "\n",
        "    axes[1,1].plot(performance_df['steps'], performance_df['expert_usage'])\n",
        "    axes[1,1].set_title('Expert Usage')\n",
        "    axes[1,1].set_xlabel('Steps')\n",
        "    axes[1,1].set_ylabel('Average Usage')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{DRIVE_DIR}/training_progress.png\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization and performance tracking initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qLkxe1JkqqB"
      },
      "outputs": [],
      "source": [
        "# Create neural memory and model arguments\n",
        "model_args = ModelArgs(\n",
        "    max_batch_size=4,\n",
        "    max_seq_len=2048,\n",
        "    dtype=\"fp8\",\n",
        "    vocab_size=32000,\n",
        "    dim=1024,\n",
        "    inter_dim=2816,\n",
        "    moe_inter_dim=512,\n",
        "    n_layers=12,\n",
        "    n_dense_layers=1,\n",
        "    n_heads=16,\n",
        "    n_routed_experts=8,\n",
        "    n_shared_experts=1,\n",
        "    n_activated_experts=2,\n",
        "    n_expert_groups=1,\n",
        "    n_limited_groups=1,\n",
        "    score_func=\"softmax\",\n",
        "    route_scale=1.0,\n",
        "    q_lora_rank=0,\n",
        "    kv_lora_rank=64,\n",
        "    qk_nope_head_dim=64,\n",
        "    qk_rope_head_dim=32,\n",
        "    v_head_dim=64,\n",
        "    original_seq_len=2048,\n",
        "    rope_theta=10000.0,\n",
        "    rope_factor=20,\n",
        "    beta_fast=16,\n",
        "    beta_slow=1,\n",
        "    mscale=0.5,\n",
        "    use_alibi=False,\n",
        "    use_rope_scaling=True,\n",
        "    gradient_checkpointing=True,\n",
        "    parallel_attn=True,\n",
        "    rope_condense_ratio=1.0\n",
        ")\n",
        "\n",
        "neural_memory = NeuralMemory(\n",
        "    memory_size=512,  # Replace with your desired memory size\n",
        "    hidden_dim=model_args.dim,\n",
        "    num_heads=model_args.n_heads,\n",
        "    sparsity=0.2  # Replace with your desired sparsity value\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMy3s3Y3kqqC"
      },
      "outputs": [],
      "source": [
        "# Initialize training components\n",
        "import inspect\n",
        "\n",
        "# Print the signature of the RewardConfig.__init__ method\n",
        "print(inspect.signature(RewardConfig.__init__))\n",
        "# Use the correct parameter names based on TreeConfig signature\n",
        "tot_config = TreeConfig(\n",
        "    num_beams=4,  # Corresponds to num_thoughts\n",
        "    max_depth=3,  # Corresponds to thought_depth\n",
        "    beam_width=2, # Corresponds to thought_width\n",
        "    reward_gamma=0.95\n",
        ")\n",
        "\n",
        "reward_config = RewardConfig(\n",
        "    math_reasoning_weight=0.2,  # Part of reasoning_weight\n",
        "    logical_coherence_weight=0.2,  # Part of reasoning_weight and consistency_weight\n",
        "    real_world_applicability_weight=0.2, # Keeping as is, could reflect external knowledge\n",
        "    solution_validity_weight=0.4  # Corresponds to accuracy_weight\n",
        ")\n",
        "curriculum_config = CurriculumConfig(\n",
        "    min_sequence_length=32,\n",
        "    max_sequence_length=512,\n",
        "    min_vocab_complexity=0.3,\n",
        "    max_vocab_complexity=1.0,\n",
        "    min_reasoning_steps=1,\n",
        "    max_reasoning_steps=8,\n",
        "    pacing_function='root',\n",
        "    total_curriculum_steps=10000,\n",
        "    performance_threshold=0.8,\n",
        "    min_samples_before_advance=100,\n",
        "    smoothing_factor=0.95\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOWsKJrakqqC"
      },
      "outputs": [],
      "source": [
        "# Train tokenizer on dataset texts\n",
        "print(\"Training tokenizer...\")\n",
        "\n",
        "import logging\n",
        "import sentencepiece as spm\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset # Import load_dataset here\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ensure tokenizer directory exists\n",
        "tokenizer_dir = Path(\"tokenizer\")\n",
        "tokenizer_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Prepare training data\n",
        "def prepare_training_data(datasets, max_samples=10000):\n",
        "    logger.info(\"Preparing training data...\")\n",
        "    train_path = tokenizer_dir / \"train.txt\"\n",
        "\n",
        "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for name, dataset in datasets.items():\n",
        "            logger.info(f\"Processing {name} dataset\")\n",
        "            if name in [\"gsm8k\", \"mmlu\"]:\n",
        "                for i, item in enumerate(dataset):\n",
        "                    if i >= max_samples:\n",
        "                        break\n",
        "                    if \"question\" in item and \"answer\" in item:\n",
        "                        f.write(f\"{item['question']}\\n\")\n",
        "                        f.write(f\"{item['answer']}\\n\")\n",
        "            elif name == \"code\":\n",
        "                for i, item in enumerate(dataset):\n",
        "                    if i >= max_samples:\n",
        "                        break\n",
        "                    if \"content\" in item:\n",
        "                        f.write(f\"{item['content']}\\n\")\n",
        "\n",
        "    return train_path\n",
        "\n",
        "try:\n",
        "    # Initialize tokenizer with reduced vocabulary size\n",
        "    tokenizer_config = TokenizerConfig(\n",
        "        vocab_size=26519,  # Reduced vocab size\n",
        "        model_prefix=\"vishwamai\",\n",
        "        character_coverage=0.9995,\n",
        "        max_sentence_length=2048,\n",
        "        pad_id=0,\n",
        "        bos_id=1,\n",
        "        eos_id=2,\n",
        "        unk_id=3\n",
        "    )\n",
        "\n",
        "    # Load datasets before calling prepare_training_data\n",
        "    datasets = { # Define datasets here\n",
        "        \"gsm8k\": load_dataset(\"gsm8k\", split=\"train\"),\n",
        "        \"mmlu\": load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"validation\"),  # Use validation or test split\n",
        "    }\n",
        "\n",
        "    # Create training data file\n",
        "    train_path = prepare_training_data(datasets)\n",
        "    logger.info(f\"Training data saved to {train_path}\")\n",
        "\n",
        "    # Train SentencePiece model\n",
        "    model_prefix = str(tokenizer_dir / tokenizer_config.model_prefix)\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=str(train_path),\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=tokenizer_config.vocab_size,\n",
        "        character_coverage=tokenizer_config.character_coverage,\n",
        "        model_type=\"bpe\",\n",
        "        max_sentence_length=tokenizer_config.max_sentence_length,\n",
        "        pad_id=tokenizer_config.pad_id,\n",
        "        bos_id=tokenizer_config.bos_id,\n",
        "        eos_id=tokenizer_config.eos_id,\n",
        "        unk_id=tokenizer_config.unk_id,\n",
        "        input_sentence_size=10000000,\n",
        "        shuffle_input_sentence=True,\n",
        "        train_extremely_large_corpus=True\n",
        "    )\n",
        "\n",
        "    # Load the trained tokenizer\n",
        "    tokenizer = VishwamAITokenizer(tokenizer_config)\n",
        "    tokenizer.load(f\"{model_prefix}.model\")\n",
        "    logger.info(\"Tokenizer training complete\")\n",
        "\n",
        "    # Verify tokenizer works\n",
        "    test_text = \"Hello world\"\n",
        "    encoded = tokenizer.encode(test_text)\n",
        "    decoded = tokenizer.decode(encoded)\n",
        "    logger.info(f\"Tokenizer test - Encoded: {encoded}\")\n",
        "    logger.info(f\"Tokenizer test - Decoded: {decoded}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during tokenizer training: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7MJM2NwkqqD"
      },
      "outputs": [],
      "source": [
        "# Load and process datasets with trained tokenizer\n",
        "print(\"Loading datasets...\")\n",
        "datasets = {\n",
        "        \"gsm8k\": load_dataset(\"gsm8k\", split=\"train\"),\n",
        "        \"mmlu\": load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"validation\"),  # Use validation or test split\n",
        "}\n",
        "\n",
        "def process_dataset(examples, dataset_type):\n",
        "    if dataset_type in [\"gsm8k\", \"mmlu\"]:\n",
        "        text = [f\"Question: {q}\\nAnswer: {a}\"\n",
        "                for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
        "    else:\n",
        "        text = examples[\"content\"]\n",
        "\n",
        "    encoded = []\n",
        "    attention_mask = []\n",
        "    max_len = 2048\n",
        "\n",
        "    for t in text:\n",
        "        # Encode with trained tokenizer\n",
        "        ids = tokenizer.encode(\n",
        "            t,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # Pad sequence\n",
        "        padding_length = max_len - len(ids)\n",
        "        if padding_length > 0:\n",
        "            ids = ids + [tokenizer.config.pad_id] * padding_length\n",
        "            mask = [1] * (max_len - padding_length) + [0] * padding_length\n",
        "        else:\n",
        "            ids = ids[:max_len]\n",
        "            mask = [1] * max_len\n",
        "\n",
        "        encoded.append(ids)\n",
        "        attention_mask.append(mask)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": encoded,\n",
        "        \"attention_mask\": attention_mask\n",
        "    }\n",
        "\n",
        "# Process datasets\n",
        "processed_datasets = {}\n",
        "for name, dataset in datasets.items():\n",
        "    print(f\"Processing {name}...\")\n",
        "    processed_datasets[name] = dataset.map(\n",
        "        lambda x: process_dataset(x, name),\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "    print(f\"Processed {len(processed_datasets[name])} examples from {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAcuNUpwkqqD"
      },
      "outputs": [],
      "source": [
        "# Initialize model and trainer\n",
        "#from vishwamai.initialize import initialize_model_and_trainer\n",
        "from vishwamai.model_factory import create_model  # Import create_model directly\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Starting model initialization...\")\n",
        "print(f\"Using model dimensions: {model_args.dim}\")\n",
        "print(f\"Using sequence length: {model_args.max_seq_len}\")\n",
        "\n",
        "# Add max_steps to model_args if not present\n",
        "if not hasattr(model_args, 'max_steps'):\n",
        "    setattr(model_args, 'max_steps', 100000)  # Set default max steps\n",
        "\n",
        "# Integrated initialize_model_and_trainer function\n",
        "def initialize_model_and_trainer(model_args, checkpoint_dir, tot_config, reward_config, curriculum_config):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Check for latest checkpoint\n",
        "    checkpoint_dir = Path(checkpoint_dir)  # Convert to Path object\n",
        "    checkpoints = list(checkpoint_dir.glob(\"step_*.pt\"))\n",
        "    latest_checkpoint = None\n",
        "    if checkpoints:\n",
        "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.stem.split('_')[1]))\n",
        "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
        "\n",
        "    try:\n",
        "        if latest_checkpoint:\n",
        "            # Load from checkpoint\n",
        "            print(\"Restoring from checkpoint...\")\n",
        "            checkpoint = torch.load(latest_checkpoint)\n",
        "            model, _ = create_model(model_args, device) # Pass model_args and device\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            start_step = checkpoint.get('step', 0)\n",
        "        else:\n",
        "            # Fresh start\n",
        "            print(\"Starting fresh training...\")\n",
        "            model, _ = create_model(model_args, device) # Pass model_args and device\n",
        "            start_step = 0\n",
        "\n",
        "        model = model.to(device)\n",
        "        main(model)  # Apply FP8/BF16 optimizations\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = AdvancedTrainer(\n",
        "            model=model,\n",
        "            config=model_args, # Use model_args as config\n",
        "            device=device,\n",
        "            memory_size=512,\n",
        "            cache_size=256,\n",
        "            tot_config=tot_config,\n",
        "            reward_config=reward_config,\n",
        "            curriculum_config=curriculum_config\n",
        "        )\n",
        "\n",
        "        if latest_checkpoint:\n",
        "            trainer.load_state_dict(checkpoint['trainer_state_dict'])\n",
        "\n",
        "        # Initialize or resume wandb (Assuming wandb is already imported)\n",
        "        run_id = os.getenv('WANDB_RUN_ID')\n",
        "        if run_id and latest_checkpoint:\n",
        "            wandb.init(project=\"vishwamai-training\", id=run_id, resume=\"must\")\n",
        "        else:\n",
        "            wandb.init(\n",
        "                project=\"vishwamai-training\",\n",
        "                config={\n",
        "                    \"model\": model_args,\n",
        "                    \"curriculum\": curriculum_config.__dict__,\n",
        "                    \"tot\": tot_config.__dict__\n",
        "                }\n",
        "            )\n",
        "            os.environ['WANDB_RUN_ID'] = wandb.run.id\n",
        "\n",
        "        print(f\"Model initialized on {device}\")\n",
        "        print(f\"Memory usage: {torch.cuda.memory_allocated(device)/1e9:.2f} GB\")\n",
        "        print(f\"Starting from step: {start_step}\")\n",
        "\n",
        "        return model, trainer, start_step\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during initialization: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute initialization with complete arguments\n",
        "try:\n",
        "    model, trainer, start_step = initialize_model_and_trainer(\n",
        "        model_args=model_args,\n",
        "        checkpoint_dir=CHECKPOINT_DIR,\n",
        "        tot_config=tot_config,\n",
        "        reward_config=reward_config,\n",
        "        curriculum_config=curriculum_config\n",
        "    )\n",
        "    print(f\"Successfully initialized model and trainer at step {start_step}\")\n",
        "    print(f\"Model will train for {model_args.max_steps} steps\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during initialization: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiLf42cTkqqE"
      },
      "outputs": [],
      "source": [
        "# Training Loop with Performance Tracking\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"vishwamai-training\")\n",
        "\n",
        "performance_data = []\n",
        "\n",
        "try:\n",
        "    for step in tqdm(range(config[\"max_steps\"])):\n",
        "        stats = trainer.train_step()\n",
        "        update_performance_tracking(stats, step)\n",
        "\n",
        "        wandb.log({\n",
        "            \"loss\": stats[\"loss\"],\n",
        "            \"learning_rate\": stats[\"lr\"],\n",
        "            \"batch_size\": stats[\"batch_size\"],\n",
        "            \"curriculum_level\": stats[\"curriculum_stats\"][\"current_difficulty\"],\n",
        "            \"memory_usage\": stats[\"memory_usage\"][\"allocated\"],\n",
        "            \"moe_loss\": stats.get(\"moe_loss\", 0),\n",
        "            \"gradient_norm\": stats[\"gradient_norm\"],\n",
        "            \"expert_usage\": stats.get(\"moe_metrics\", {})\n",
        "        })\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            plot_training_progress()\n",
        "            checkpoint_path = f\"{CHECKPOINT_DIR}/step_{step}.pt\"\n",
        "            trainer.save_checkpoint(checkpoint_path)\n",
        "\n",
        "            trainer.push_to_hub(\n",
        "                \"VishwamAI/VishwamAI\",\n",
        "                commit_message=f\"Training checkpoint at step {step}\"\n",
        "            )\n",
        "\n",
        "        if step % 5000 == 0:\n",
        "            print(f\"\\nEvaluating at step {step}...\")\n",
        "            eval_metrics = trainer.evaluate()\n",
        "            wandb.log({\"eval\": eval_metrics})\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted. Saving final visualization...\")\n",
        "    plot_training_progress()\n",
        "    trainer.save_checkpoint(f\"{CHECKPOINT_DIR}/interrupted.pt\")\n",
        "\n",
        "plot_training_progress()\n",
        "performance_df.to_csv(f\"{DRIVE_DIR}/training_metrics.csv\", index=False)\n",
        "print(\"Training complete with performance tracking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px86BSipkqqE"
      },
      "outputs": [],
      "source": [
        "# Generate Performance Graphs\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(performance_df[\"step\"], performance_df[\"loss\"], label=\"Loss\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Time\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{DRIVE_DIR}/training_loss.png\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(performance_df[\"step\"], performance_df[\"memory_usage\"], label=\"Memory Usage (GB)\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Memory Usage (GB)\")\n",
        "plt.title(\"Memory Usage Over Time\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"{DRIVE_DIR}/memory_usage.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpOIrXCTkqqE"
      },
      "outputs": [],
      "source": [
        "# Final Evaluation\n",
        "print(\"Running final evaluation...\")\n",
        "\n",
        "eval_datasets = [\n",
        "    \"gsm8k\",\n",
        "    \"TIGER-Lab/MMLU-Pro\",\n",
        "    \"MMMU/MMMU\",\n",
        "    \"microsoft/SCBench\",\n",
        "    \"camel-ai/math\",\n",
        "    \"camel-ai/code\"\n",
        "]\n",
        "\n",
        "results = {}\n",
        "for dataset in eval_datasets:\n",
        "    print(f\"\\nEvaluating on {dataset}...\")\n",
        "    try:\n",
        "        eval_data = load_dataset(dataset, split=\"test\")\n",
        "        metrics = trainer.evaluate(eval_data)\n",
        "        results[dataset] = metrics\n",
        "        print(f\"{dataset}: {metrics}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {dataset}: {str(e)}\")\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "with open(f\"{DRIVE_DIR}/final_evaluation.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-setup"
      },
      "outputs": [],
      "source": [
        "# GPU Setup and Verification\n",
        "def setup_gpu():\n",
        "    \"\"\"Setup and verify GPU configuration.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"This notebook requires a GPU runtime. Please change runtime type to GPU.\")\n",
        "    \n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"Using GPU: {gpu_name}\")\n",
        "    print(f\"Total GPU memory: {memory_gb:.1f} GB\")\n",
        "    \n",
        "    # Set memory efficient settings\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    \n",
        "    if 'T4' in gpu_name:\n",
        "        print(\"Detected T4 GPU - Adjusting model configuration for optimal performance\")\n",
        "        return 'T4', min(memory_gb * 0.8, 12)  # Use 80% of available memory, max 12GB\n",
        "    elif 'V100' in gpu_name:\n",
        "        print(\"Detected V100 GPU - Using high-performance configuration\")\n",
        "        return 'V100', min(memory_gb * 0.85, 28)  # Use 85% of available memory\n",
        "    elif 'A100' in gpu_name:\n",
        "        print(\"Detected A100 GPU - Using maximum performance configuration\")\n",
        "        return 'A100', min(memory_gb * 0.9, 36)  # Use 90% of available memory\n",
        "    else:\n",
        "        print(\"Unknown GPU type - Using conservative settings\")\n",
        "        return 'unknown', min(memory_gb * 0.7, 8)  # Use 70% of available memory\n",
        "\n",
        "# Execute GPU setup\n",
        "gpu_type, available_memory = setup_gpu()\n",
        "\n",
        "# Adjust model configuration based on GPU\n",
        "def adjust_model_config(model_args, gpu_type, available_memory):\n",
        "    \"\"\"Adjust model configuration based on available GPU.\"\"\"\n",
        "    \n",
        "    # Base memory requirements per parameter\n",
        "    mem_per_param = 2  # bytes per parameter in FP16\n",
        "    \n",
        "    if gpu_type == 'T4':\n",
        "        # Conservative settings for T4\n",
        "        model_args.max_batch_size = 2\n",
        "        model_args.max_seq_len = 1024\n",
        "        model_args.dim = 1024\n",
        "        model_args.n_layers = 12\n",
        "        model_args.n_heads = 16\n",
        "        model_args.n_routed_experts = 8\n",
        "        \n",
        "    elif gpu_type == 'V100':\n",
        "        # Moderate settings for V100\n",
        "        model_args.max_batch_size = 4\n",
        "        model_args.max_seq_len = 2048\n",
        "        model_args.dim = 2048\n",
        "        model_args.n_layers = 24\n",
        "        model_args.n_heads = 32\n",
        "        model_args.n_routed_experts = 16\n",
        "        \n",
        "    elif gpu_type == 'A100':\n",
        "        # Maximum settings for A100\n",
        "        model_args.max_batch_size = 8\n",
        "        model_args.max_seq_len = 4096\n",
        "        model_args.dim = 4096\n",
        "        model_args.n_layers = 32\n",
        "        model_args.n_heads = 64\n",
        "        model_args.n_routed_experts = 32\n",
        "    \n",
        "    # Calculate approximate model size\n",
        "    num_params = (model_args.dim * model_args.n_layers * 4 * \n",
        "                 model_args.max_seq_len * model_args.n_heads)\n",
        "    estimated_memory = num_params * mem_per_param / 1e9  # Convert to GB\n",
        "    \n",
        "    if estimated_memory > available_memory:\n",
        "        reduction_factor = (available_memory / estimated_memory) ** 0.5\n",
        "        print(f\"Warning: Reducing model size by {(1-reduction_factor)*100:.1f}% to fit in GPU memory\")\n",
        "        \n",
        "        model_args.dim = int(model_args.dim * reduction_factor)\n",
        "        model_args.n_layers = int(model_args.n_layers * reduction_factor)\n",
        "        model_args.n_heads = max(8, int(model_args.n_heads * reduction_factor))\n",
        "        model_args.n_routed_experts = max(4, int(model_args.n_routed_experts * reduction_factor))\n",
        "    \n",
        "    print(\"\\nAdjusted model configuration:\")\n",
        "    print(f\"Dimension: {model_args.dim}\")\n",
        "    print(f\"Layers: {model_args.n_layers}\")\n",
        "    print(f\"Heads: {model_args.n_heads}\")\n",
        "    print(f\"Experts: {model_args.n_routed_experts}\")\n",
        "    print(f\"Sequence Length: {model_args.max_seq_len}\")\n",
        "    print(f\"Batch Size: {model_args.max_batch_size}\")\n",
        "    \n",
        "    return model_args\n",
        "\n",
        "# Update model initialization\n",
        "try:\n",
        "    # Adjust model configuration based on GPU\n",
        "    model_args = adjust_model_config(model_args, gpu_type, available_memory)\n",
        "    \n",
        "    # Initialize model and trainer\n",
        "    model, trainer, start_step = initialize_model_and_trainer(\n",
        "        model_args=model_args,\n",
        "        checkpoint_dir=CHECKPOINT_DIR,\n",
        "        tot_config=tot_config,\n",
        "        reward_config=reward_config,\n",
        "        curriculum_config=curriculum_config\n",
        "    )\n",
        "    \n",
        "    print(\"Model initialization successful!\")\n",
        "    print(f\"Starting training from step {start_step}\")\n",
        "    print(f\"Model will train for {model_args.max_steps} steps\")\n",
        "    \n",
        "except torch.cuda.OutOfMemoryError as e:\n",
        "    print(\"Error: GPU out of memory!\")\n",
        "    print(\"Try reducing model size or batch size\")\n",
        "    raise e\n",
        "except Exception as e:\n",
        "    print(f\"Error during initialization: {str(e)}\")\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
