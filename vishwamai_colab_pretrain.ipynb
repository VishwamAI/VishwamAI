{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vishwamai Model Pretraining on Google Colab\n",
    "\n",
    "This notebook sets up and runs pretraining for the Vishwamai model using Google Colab's resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're using a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
    "%cd VishwamAI\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "from vishwamai.model_utils import load_model\n",
    "from vishwamai.trainer import Trainer, TrainingArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tokenizer():\n",
    "    \"\"\"Initialize BERT tokenizer with custom configuration\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"bert-large-uncased\",\n",
    "        model_max_length=2048,  # Increased for GPU training\n",
    "        do_lower_case=True,\n",
    "        truncation_side=\"right\",\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True\n",
    "    )\n",
    "    \n",
    "    special_tokens = {\n",
    "        \"additional_special_tokens\": [\n",
    "            \"[MEMORY]\",\n",
    "            \"[REASONING]\",\n",
    "            \"[CACHE]\",\n",
    "            \"[STEP]\",\n",
    "        ]\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer, max_length=2048):\n",
    "    \"\"\"Load and prepare full datasets for pretraining\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    datasets = {\n",
    "        \"gsm8k\": load_dataset(\"openai/gsm8k\", split=\"train\"),\n",
    "        \"mmlu\": load_dataset(\"cais/mmlu\", split=\"train\"),\n",
    "        \"mmlu_pro\": load_dataset(\"TIGER-Lab/MMLU-Pro\", split=\"train\"),\n",
    "        \"mmmlu\": load_dataset(\"openai/MMMLU\", split=\"train\")\n",
    "    }\n",
    "    \n",
    "    def prepare_text(examples):\n",
    "        if \"question\" in examples:\n",
    "            text = examples[\"question\"]\n",
    "            if \"solution\" in examples:\n",
    "                text = f\"[STEP] Question: {text} [STEP] Solution: {examples['solution']}\"\n",
    "        else:\n",
    "            text = examples[\"text\"] if \"text\" in examples else str(examples)\n",
    "        text = f\"[MEMORY] [CACHE] [REASONING] {text}\"\n",
    "        return text\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        text = prepare_text(examples)\n",
    "        return tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_special_tokens_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "    \n",
    "    tokenized_datasets = {}\n",
    "    for name, dataset in datasets.items():\n",
    "        print(f\"Processing {name} dataset...\")\n",
    "        tokenized_datasets[name] = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "    \n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(tokenized_datasets, batch_size=16):  # Increased for GPU\n",
    "    \"\"\"Create DataLoaders for training\"\"\"\n",
    "    def collate_fn(examples):\n",
    "        input_ids = torch.stack([example['input_ids'] for example in examples])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in examples])\n",
    "        labels = input_ids.clone()\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    dataloaders = {}\n",
    "    for name, dataset in tokenized_datasets.items():\n",
    "        dataloaders[name] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training\n",
    "tokenizer = setup_tokenizer()\n",
    "print(\"Preparing datasets...\")\n",
    "tokenized_datasets = prepare_datasets(tokenizer)\n",
    "dataloaders = create_dataloaders(tokenized_datasets)\n",
    "\n",
    "# Create combined dataset\n",
    "combined_dataloader = DataLoader(\n",
    "    torch.utils.data.ConcatDataset([dl.dataset for dl in dataloaders.values()]),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "config_path = \"vishwamai/configs/config_optimized.json\"\n",
    "\n",
    "# Adjust model size based on available GPU memory\n",
    "model = load_model(\n",
    "    config_path,\n",
    "    device=\"cuda\",  # Use GPU\n",
    "    hidden_size=2048,  # Reduced from original\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=32,\n",
    "    intermediate_size=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArgs(\n",
    "    output_dir=\"pretrain_checkpoints\",\n",
    "    num_epochs=3,\n",
    "    batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    use_fsdp=True,\n",
    "    mixed_precision=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=combined_dataloader,\n",
    "    eval_dataloader=dataloaders['gsm8k'],\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "print(\"Starting pretraining...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and upload to HuggingFace Hub\n",
    "output_dir = \"vishwamai_pretrained\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving model...\")\n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Copy config and model card\n",
    "import shutil\n",
    "shutil.copy(\"MODEL_CARD.md\", os.path.join(output_dir, \"README.md\"))\n",
    "shutil.copy(config_path, os.path.join(output_dir, \"config.json\"))\n",
    "\n",
    "# Upload to HuggingFace Hub\n",
    "repo_name = \"kasinadhsarma/vishwamai-model\"\n",
    "api = HfApi()\n",
    "\n",
    "try:\n",
    "    api.create_repo(repo_name, private=True)\n",
    "except Exception as e:\n",
    "    print(f\"Repository creation error (might already exist): {e}\")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=output_dir,\n",
    "    repo_id=repo_name,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"Model uploaded successfully to: https://huggingface.co/{repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "Vishwamai Pretraining",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
