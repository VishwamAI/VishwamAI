{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "intro"
      },
      "source": [
       "# VishwamAI Training on Google Colab (T4)\n",
       "\n",
       "This notebook trains VishwamAI on Google Colab's T4 GPU and saves to Hugging Face Hub.\n",
       "\n",
       "**Requirements:**\n",
       "- Google Colab with T4 GPU\n",
       "- Hugging Face account and token\n",
       "- ~16GB GPU memory"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "check_gpu"
      },
      "outputs": [],
      "source": [
       "!nvidia-smi"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "install_deps"
      },
      "outputs": [],
      "source": [
       "!pip install -q transformers datasets torch accelerate\n",
       "!pip install -q sentencepiece protobuf"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "imports"
      },
      "outputs": [],
      "source": [
       "import os\n",
       "import torch\n",
       "from transformers import (\n",
       "    AutoModelForCausalLM,\n",
       "    AutoTokenizer,\n",
       "    Trainer,\n",
       "    TrainingArguments,\n",
       "    default_data_collator\n",
       ")\n",
       "from datasets import load_dataset\n",
       "from huggingface_hub import notebook_login\n",
       "from datetime import datetime"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "login_hf"
      },
      "outputs": [],
      "source": [
       "# Login to Hugging Face\n",
       "notebook_login()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "config_section"
      },
      "source": [
       "## Configuration"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "config"
      },
      "outputs": [],
      "source": [
       "# Training config\n",
       "CONFIG = {\n",
       "    'model_name': 't5-base',  # Base model to start from\n",
       "    'repo_id': 'kasinadhsarma/vishwamai-model',  # Your HF repo\n",
       "    'max_length': 512,  # Sequence length\n",
       "    'batch_size': 4,  # Per device batch size\n",
       "    'grad_accum': 4,  # Gradient accumulation steps\n",
       "    'epochs': 3,\n",
       "    'lr': 2e-5,\n",
       "    'warmup_steps': 100,\n",
       "    'save_steps': 200,\n",
       "    'eval_steps': 200\n",
       "}"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "data_section"
      },
      "source": [
       "## Data Processing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "data_processing"
      },
      "outputs": [],
      "source": [
       "def process_gsm8k(example):\n",
       "    \"\"\"Process GSM8K examples\"\"\"\n",
       "    return {\n",
       "        'input_text': f\"solve: {example['question']}\",\n",
       "        'target_text': example['answer'].split('####')[1].strip()\n",
       "    }\n",
       "\n",
       "def process_mmlu(example):\n",
       "    \"\"\"Process MMLU examples\"\"\"\n",
       "    options = ['A', 'B', 'C', 'D']\n",
       "    formatted_options = '\\n'.join(\n",
       "        f\"{opt}) {example[opt]}\" for opt in options\n",
       "    )\n",
       "    return {\n",
       "        'input_text': f\"answer: {example['question']}\\n\\nOptions:\\n{formatted_options}\",\n",
       "        'target_text': f\"The answer is {options[example['answer']]}\"\n",
       "    }\n",
       "\n",
       "# Load datasets\n",
       "print(\"Loading datasets...\")\n",
       "gsm8k_train = load_dataset('gsm8k', 'main', split='train')\n",
       "gsm8k_test = load_dataset('gsm8k', 'main', split='test')\n",
       "\n",
       "mmlu_subjects = ['mathematics', 'computer_science', 'physics']\n",
       "mmlu_datasets = []\n",
       "\n",
       "for subject in mmlu_subjects:\n",
       "    ds = load_dataset('cais/mmlu', subject)\n",
       "    mmlu_datasets.append(ds)\n",
       "\n",
       "# Process datasets\n",
       "print(\"Processing datasets...\")\n",
       "gsm8k_train = gsm8k_train.map(process_gsm8k)\n",
       "gsm8k_test = gsm8k_test.map(process_gsm8k)\n",
       "\n",
       "mmlu_train = mmlu_datasets[0]['train']\n",
       "mmlu_test = mmlu_datasets[0]['test']\n",
       "\n",
       "for ds in mmlu_datasets[1:]:\n",
       "    mmlu_train = mmlu_train.concatenate(ds['train'])\n",
       "    mmlu_test = mmlu_test.concatenate(ds['test'])\n",
       "\n",
       "mmlu_train = mmlu_train.map(process_mmlu)\n",
       "mmlu_test = mmlu_test.map(process_mmlu)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "model_section"
      },
      "source": [
       "## Model Setup"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "model_setup"
      },
      "outputs": [],
      "source": [
       "# Load model and tokenizer\n",
       "print(\"Loading model and tokenizer...\")\n",
       "model = AutoModelForCausalLM.from_pretrained(CONFIG['model_name'])\n",
       "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
       "\n",
       "# Tokenization function\n",
       "def tokenize_function(examples):\n",
       "    model_inputs = tokenizer(\n",
       "        examples['input_text'],\n",
       "        max_length=CONFIG['max_length'],\n",
       "        padding='max_length',\n",
       "        truncation=True,\n",
       "        return_tensors='pt'\n",
       "    )\n",
       "    \n",
       "    with tokenizer.as_target_tokenizer():\n",
       "        labels = tokenizer(\n",
       "            examples['target_text'],\n",
       "            max_length=CONFIG['max_length'],\n",
       "            padding='max_length',\n",
       "            truncation=True,\n",
       "            return_tensors='pt'\n",
       "        )\n",
       "    \n",
       "    model_inputs['labels'] = labels['input_ids']\n",
       "    return model_inputs\n",
       "\n",
       "# Tokenize datasets\n",
       "print(\"Tokenizing datasets...\")\n",
       "train_dataset = gsm8k_train.map(\n",
       "    tokenize_function,\n",
       "    batched=True,\n",
       "    remove_columns=gsm8k_train.column_names\n",
       ")\n",
       "\n",
       "eval_dataset = gsm8k_test.map(\n",
       "    tokenize_function,\n",
       "    batched=True,\n",
       "    remove_columns=gsm8k_test.column_names\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "training_section"
      },
      "source": [
       "## Training Setup"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "trainer_setup"
      },
      "outputs": [],
      "source": [
       "# Training arguments\n",
       "training_args = TrainingArguments(\n",
       "    output_dir=f\"./results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
       "    per_device_train_batch_size=CONFIG['batch_size'],\n",
       "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
       "    gradient_accumulation_steps=CONFIG['grad_accum'],\n",
       "    num_train_epochs=CONFIG['epochs'],\n",
       "    learning_rate=CONFIG['lr'],\n",
       "    fp16=True,  # Mixed precision training\n",
       "    warmup_steps=CONFIG['warmup_steps'],\n",
       "    logging_steps=10,\n",
       "    save_steps=CONFIG['save_steps'],\n",
       "    eval_steps=CONFIG['eval_steps'],\n",
       "    evaluation_strategy=\"steps\",\n",
       "    load_best_model_at_end=True,\n",
       "    push_to_hub=True,\n",
       "    hub_model_id=CONFIG['repo_id'],\n",
       "    hub_strategy=\"every_save\"\n",
       ")\n",
       "\n",
       "# Initialize trainer\n",
       "trainer = Trainer(\n",
       "    model=model,\n",
       "    args=training_args,\n",
       "    train_dataset=train_dataset,\n",
       "    eval_dataset=eval_dataset,\n",
       "    data_collator=default_data_collator,\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "training_section"
      },
      "source": [
       "## Start Training"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "training"
      },
      "outputs": [],
      "source": [
       "print(f\"Starting training... Model will be saved to: {CONFIG['repo_id']}\")\n",
       "trainer.train()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "eval_section"
      },
      "source": [
       "## Evaluation on MMLU"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "mmlu_eval"
      },
      "outputs": [],
      "source": [
       "# Prepare MMLU evaluation dataset\n",
       "mmlu_eval = mmlu_test.map(\n",
       "    tokenize_function,\n",
       "    batched=True,\n",
       "    remove_columns=mmlu_test.column_names\n",
       ")\n",
       "\n",
       "# Run evaluation\n",
       "print(\"Evaluating on MMLU test set...\")\n",
       "mmlu_metrics = trainer.evaluate(eval_dataset=mmlu_eval)\n",
       "print(\"MMLU Evaluation Results:\")\n",
       "print(mmlu_metrics)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "save_section"
      },
      "source": [
       "## Save Final Model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "save_model"
      },
      "outputs": [],
      "source": [
       "# Push final model to hub\n",
       "print(\"Pushing final model to Hugging Face Hub...\")\n",
       "trainer.push_to_hub(\n",
       "    commit_message=f\"Final training checkpoint - {datetime.now()}\"\n",
       ")\n",
       "print(\"Training completed!\")"
      ]
     }
    ],
    "metadata": {
     "accelerator": "GPU",
     "colab": {
      "gpuType": "T4",
      "provenance": []
     },
     "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 0
   }