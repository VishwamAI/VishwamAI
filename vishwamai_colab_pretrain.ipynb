{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Progress tracking setup\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import Trainer\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Operation completed in {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Verify GPU availability and requirements\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "if 'A100' not in gpu_name:\n",
    "    print(\"⚠️ Warning: This model requires an A100 GPU for optimal performance\")\n",
    "    print(\"Current GPU:\", gpu_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Parallel package installation\n",
    "%pip install torch==2.4.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \\\n",
    "    transformers==4.34.0 datasets accelerate huggingface_hub wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "from huggingface_hub import login, create_repo\n",
    "from getpass import getpass\n",
    "import wandb\n",
    "\n",
    "# Get token securely\n",
    "hf_token = getpass(\"Enter your Hugging Face access token: \")\n",
    "login(token=hf_token)\n",
    "print(\"Successfully logged in to Hugging Face!\")\n",
    "\n",
    "# Initialize W&B for experiment tracking\n",
    "wandb.login()\n",
    "print(\"Successfully logged in to Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Quick repository setup\n",
    "!git clone https://github.com/VishwamAI/VishwamAI.git\n",
    "%cd VishwamAI\n",
    "%pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from vishwamai.model_utils import load_model, get_gpu_memory\n",
    "from vishwamai.neural_memory import ReasoningMemoryTransformer, MemoryConfig\n",
    "from vishwamai.tree_of_thoughts import TreeOfThoughts\n",
    "from vishwamai.cache_augmentation import DifferentiableCacheAugmentation, CacheConfig\n",
    "from vishwamai.trainer import VishwamAIPretrainer\n",
    "from huggingface_hub import HfFolder, Repository\n",
    "\n",
    "# Performance optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@track_time\n",
    "def setup_hardware():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = get_gpu_memory()\n",
    "    print(f\"Using GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "\n",
    "    # Optimize for available GPU\n",
    "    if 'a100' in gpu_name.lower():\n",
    "        return 'A100_optimized', 128, 65536  # Full 671B model\n",
    "    elif 'v100' in gpu_name.lower():\n",
    "        return 'V100_optimized', 64, 32768   # Reduced size\n",
    "    else:\n",
    "        return 'T4_optimized', 32, 16384     # Minimal configuration\n",
    "\n",
    "gpu_type, expert_count, cache_size = setup_hardware()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@track_time\n",
    "def load_config():\n",
    "    config_path = \"./vishwamai/configs/config_671b.json\"\n",
    "\n",
    "    # Load JSON file\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Ensure 'gpu_type' exists in 'colab_specific'\n",
    "    if \"colab_specific\" not in config or gpu_type not in config[\"colab_specific\"]:\n",
    "        raise KeyError(f\"GPU type '{gpu_type}' not found in config['colab_specific']\")\n",
    "\n",
    "    gpu_config = config[\"colab_specific\"][gpu_type]\n",
    "\n",
    "    # Update model configuration dynamically\n",
    "    config[\"model_config\"] = {\n",
    "        \"dim\": 8192,\n",
    "        \"num_attention_heads\": 64,\n",
    "        \"num_hidden_layers\": 120,\n",
    "        \"vocab_size\": 64000,\n",
    "        \"max_position_embeddings\": 32768,\n",
    "        \"batch_size\": gpu_config.get(\"batch_size\", 8),\n",
    "        \"num_experts\": expert_count,\n",
    "        \"experts_per_token\": min(16, expert_count // 8),\n",
    "        \"memory_size\": gpu_config.get(\"memory_size\", 2048),\n",
    "        \"tree_beam_width\": gpu_config.get(\"tree_beam_width\", 4),\n",
    "        \"cache_size\": cache_size\n",
    "    }\n",
    "\n",
    "    return config, gpu_config\n",
    "\n",
    "# Load configuration\n",
    "config, gpu_config = load_config()\n",
    "print(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@track_time\n",
    "def initialize_components():\n",
    "    print(\"Initializing model and components...\")\n",
    "\n",
    "    model = load_model(\n",
    "        config_path=\"./vishwamai/configs/config_671b.json\",\n",
    "        device=\"cuda\",\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "    memory = ReasoningMemoryTransformer(\n",
    "        MemoryConfig(\n",
    "            hidden_size=config['model_config']['dim'],\n",
    "            memory_size=config['model_config']['memory_size'],\n",
    "            num_memory_layers=3,\n",
    "            dropout=0.1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tree_thoughts = TreeOfThoughts(\n",
    "        model=model,\n",
    "        beam_width=config['model_config']['tree_beam_width']\n",
    "    )\n",
    "\n",
    "    cache = DifferentiableCacheAugmentation(\n",
    "        CacheConfig(\n",
    "            hidden_size=config['model_config']['dim'],\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            max_cache_length=config['model_config']['cache_size']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return model, memory, tree_thoughts, cache\n",
    "\n",
    "model, memory, tree_thoughts, cache = initialize_components()\n",
    "\n",
    "print(f\"\\nModel size: {sum(p.numel() for p in model.parameters())/1e9:.1f}B parameters\")\n",
    "print(f\"Memory slots: {config['model_config']['memory_size']:,}\")\n",
    "print(f\"Cache entries: {config['model_config']['cache_size']:,}\")\n",
    "print(f\"Context length: {config['model_config']['max_position_embeddings']:,} tokens\")\n",
    "print(f\"Active experts: {config['model_config']['experts_per_token']} per token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Initialize output directory and repository\n",
    "output_dir = \"./pretrain_output\"\n",
    "!mkdir -p $output_dir\n",
    "\n",
    "# Configure training with FSDP optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=gpu_config['batch_size'],\n",
    "    gradient_accumulation_steps=gpu_config['gradient_accumulation'],\n",
    "    learning_rate=1.2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # Distributed training\n",
    "    fsdp=\"full_shard\",\n",
    "    fsdp_transformer_layer_cls_to_wrap=\"VishwamAILayer\",\n",
    "    # Performance optimizations\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    group_by_length=True,\n",
    "    # Features\n",
    "    use_moe=True,\n",
    "    use_neural_memory=True,\n",
    "    use_tree_of_thoughts=True,\n",
    "    use_cache_augmentation=True,\n",
    "    # Monitoring\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    # Hub integration\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"kasinadhsarma/vishwamai-model\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    # Other optimizations\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    length_penalty=1.0,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Combine selected datasets for training\n",
    "train_datasets = []\n",
    "for ds_name in [\"gsm8k\", \"leetcode1\", \"leetcode2\", \"math\"]:\n",
    "    if ds_name in datasets:\n",
    "        train_datasets.append(datasets[ds_name])\n",
    "if not train_datasets:\n",
    "    raise ValueError(\"No available training datasets found.\")\n",
    "\n",
    "combined_train_dataset = concatenate_datasets(train_datasets)\n",
    "\n",
    "# Select a development (evaluation) dataset\n",
    "development_dataset = datasets.get(\"mmlu\") or datasets.get(\"mmlu_pro\")\n",
    "\n",
    "trainer = VishwamAIPretrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train_dataset,\n",
    "    eval_dataset=development_dataset,\n",
    "    memory_module=memory,\n",
    "    tree_module=tree_thoughts,\n",
    "    cache_module=cache\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start training with monitoring\n",
    "print(\"Starting pretraining pipeline...\")\n",
    "start_time = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nPretraining completed in {training_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@track_time\n",
    "def save_model_components():\n",
    "    model_save_path = \"final_model\"\n",
    "    trainer.save_model(model_save_path)\n",
    "    print(\"Model and components saved successfully\")\n",
    "    return model_save_path\n",
    "\n",
    "model_save_path = save_model_components()\n",
    "print(f\"Model available at: https://huggingface.co/kasinadhsarma/vishwamai-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@track_time\n",
    "def validate_model():\n",
    "    test_model = load_model(\n",
    "        config_path=\"configs/config_671b.json\",\n",
    "        device=\"cuda\",\n",
    "        pretrained_path=model_save_path\n",
    "    )\n",
    "\n",
    "    test_cases = [\n",
    "        \"Solve this math problem: What is the area of a circle with radius 5?\",\n",
    "        \"Explain the concept of quantum entanglement.\",\n",
    "        \"Write a Python function to find the nth Fibonacci number using dynamic programming.\"\n",
    "    ]\n",
    "\n",
    "    print(\"Running validation tests...\")\n",
    "    for test_input in test_cases:\n",
    "        print(f\"\\nTest: {test_input}\")\n",
    "        encoded = model.tokenizer.encode(test_input, return_tensors=\"pt\").cuda()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            start = time.time()\n",
    "            output = test_model.generate(\n",
    "                encoded,\n",
    "                max_new_tokens=200,\n",
    "                num_beams=4,\n",
    "                temperature=0.7,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            end = time.time()\n",
    "\n",
    "        response = model.tokenizer.decode(output[0])\n",
    "        print(f\"Response (generated in {end-start:.2f}s):\")\n",
    "        print(response)\n",
    "\n",
    "validate_model()\n",
    "print(\"\\nPretraining and validation completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
