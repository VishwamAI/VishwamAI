{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# VishwamAI Math Integration Tutorial\n",
       "\n",
       "This notebook demonstrates how to integrate mathematical reasoning capabilities with VishwamAI using the GSM8K dataset. We'll cover:\n",
       "\n",
       "1. Dataset preparation and loading\n",
       "2. Model and tokenizer configuration\n",
       "3. Training pipeline setup\n",
       "4. Evaluation and visualization\n",
       "5. Example inference\n",
       "\n",
       "## Setup and Dependencies"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "import torch\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "from vishwamai.model import VishwamaiModel, VishwamaiConfig\n",
       "from vishwamai.conceptual_tokenizer import ConceptualTokenizer, ConceptualTokenizerConfig\n",
       "from vishwamai.training import VishwamaiTrainer\n",
       "\n",
       "# Set random seeds for reproducibility\n",
       "torch.manual_seed(42)\n",
       "np.random.seed(42)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Dataset Preparation\n",
       "\n",
       "We'll use the GSM8K (Grade School Math 8K) dataset, which contains math word problems with step-by-step solutions."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load GSM8K dataset\n",
       "train_df = pd.read_parquet('math/gsm8k/train-00000-of-00001.parquet')\n",
       "test_df = pd.read_parquet('math/gsm8k/test-00000-of-00001.parquet')\n",
       "\n",
       "print(f\"Training examples: {len(train_df)}\")\n",
       "print(f\"Test examples: {len(test_df)}\")\n",
       "\n",
       "# Display sample\n",
       "print(\"\\nSample problem:\")\n",
       "sample_idx = np.random.randint(len(train_df))\n",
       "print(f\"Question: {train_df.iloc[sample_idx]['question']}\")\n",
       "print(f\"Answer: {train_df.iloc[sample_idx]['answer']}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Custom Dataset Class"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "class MathDataset(Dataset):\n",
       "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
       "        self.data = dataframe\n",
       "        self.tokenizer = tokenizer\n",
       "        self.max_length = max_length\n",
       "        \n",
       "    def __len__(self):\n",
       "        return len(self.data)\n",
       "    \n",
       "    def __getitem__(self, idx):\n",
       "        row = self.data.iloc[idx]\n",
       "        question = row['question']\n",
       "        answer = row['answer']\n",
       "        \n",
       "        # Tokenize inputs with padding and truncation\n",
       "        inputs = self.tokenizer.encode(\n",
       "            question,\n",
       "            padding='max_length',\n",
       "            max_length=self.max_length,\n",
       "            truncation=True\n",
       "        )\n",
       "        \n",
       "        labels = self.tokenizer.encode(\n",
       "            answer,\n",
       "            padding='max_length',\n",
       "            max_length=self.max_length,\n",
       "            truncation=True\n",
       "        )\n",
       "        \n",
       "        return {\n",
       "            'input_ids': torch.tensor(inputs, dtype=torch.long),\n",
       "            'labels': torch.tensor(labels, dtype=torch.long),\n",
       "            'attention_mask': torch.ones(len(inputs), dtype=torch.long)\n",
       "        }"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Model Configuration\n",
       "\n",
       "Configure the model and tokenizer with appropriate parameters for math reasoning tasks."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Model configuration\n",
       "model_config = VishwamaiConfig(\n",
       "    hidden_size=3072,\n",
       "    num_hidden_layers=12,\n",
       "    num_attention_heads=12,\n",
       "    intermediate_size=3072,\n",
       "    hidden_dropout_prob=0.1,\n",
       "    attention_dropout_prob=0.1,\n",
       "    max_position_embeddings=512,\n",
       "    type_vocab_size=2,\n",
       "    initializer_range=0.02,\n",
       "    layer_norm_eps=1e-12,\n",
       "    pad_token_id=0,\n",
       "    use_cache=True,\n",
       "    num_decoder_layers=12\n",
       ")\n",
       "\n",
       "# Tokenizer configuration\n",
       "tokenizer_config = ConceptualTokenizerConfig(\n",
       "    vocab_size=32000,\n",
       "    max_length=512,\n",
       "    concept_tokens=[\"math\", \"algebra\", \"arithmetic\", \"geometry\"],\n",
       "    reasoning_tokens=[\"equals\", \"therefore\", \"because\", \"solve\", \"calculate\"],\n",
       "    special_tokens={\n",
       "        \"pad_token\": \"[PAD]\",\n",
       "        \"unk_token\": \"[UNK]\",\n",
       "        \"bos_token\": \"[BOS]\",\n",
       "        \"eos_token\": \"[EOS]\",\n",
       "        \"sep_token\": \"[SEP]\"\n",
       "    }\n",
       ")\n",
       "\n",
       "# Initialize model and tokenizer\n",
       "model = VishwamaiModel(model_config)\n",
       "tokenizer = ConceptualTokenizer(tokenizer_config)\n",
       "\n",
       "print(\"Model and tokenizer initialized successfully.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Training Setup\n",
       "\n",
       "Configure the training pipeline with appropriate hyperparameters and optimization settings."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare datasets\n",
       "train_dataset = MathDataset(train_df, tokenizer)\n",
       "val_dataset = MathDataset(test_df, tokenizer)\n",
       "\n",
       "# Create data loaders\n",
       "train_loader = DataLoader(\n",
       "    train_dataset,\n",
       "    batch_size=16,\n",
       "    shuffle=True,\n",
       "    num_workers=4,\n",
       "    pin_memory=True\n",
       ")\n",
       "\n",
       "val_loader = DataLoader(\n",
       "    val_dataset,\n",
       "    batch_size=16,\n",
       "    shuffle=False,\n",
       "    num_workers=4,\n",
       "    pin_memory=True\n",
       ")\n",
       "\n",
       "# Training configuration\n",
       "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "model = model.to(device)\n",
       "\n",
       "trainer = VishwamaiTrainer(\n",
       "    model=model,\n",
       "    tokenizer=tokenizer,\n",
       "    train_dataset=train_loader,\n",
       "    eval_dataset=val_loader,\n",
       "    device=device,\n",
       "    optimizer_kwargs={\n",
       "        'lr': 5e-5,\n",
       "        'weight_decay': 0.01,\n",
       "        'beta1': 0.9,\n",
       "        'beta2': 0.999\n",
       "    }\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Training and Monitoring"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Training metrics collector\n",
       "metrics_history = []\n",
       "\n",
       "def metric_callback(metrics):\n",
       "    metrics_history.append(metrics)\n",
       "    \n",
       "# Train the model\n",
       "trainer.train(\n",
       "    num_epochs=10,\n",
       "    save_dir=\"./checkpoints\",\n",
       "    evaluation_steps=100,\n",
       "    save_steps=1000,\n",
       "    logging_steps=10,\n",
       "    callback=metric_callback,\n",
       "    fp16=True  # Enable mixed precision training\n",
       ")\n",
       "\n",
       "print(\"Training completed successfully.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Evaluation and Visualization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare metrics for visualization\n",
       "metrics_df = pd.DataFrame(metrics_history)\n",
       "\n",
       "# Set plot style\n",
       "plt.style.use('seaborn')\n",
       "\n",
       "# Create a figure with multiple subplots\n",
       "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
       "\n",
       "# Plot training and validation loss\n",
       "ax1.plot(metrics_df['step'], metrics_df['train_loss'], label='Training Loss')\n",
       "if 'eval_loss' in metrics_df.columns:\n",
       "    ax1.plot(metrics_df['step'], metrics_df['eval_loss'], label='Validation Loss')\n",
       "ax1.set_xlabel('Training Steps')\n",
       "ax1.set_ylabel('Loss')\n",
       "ax1.set_title('Training and Validation Loss')\n",
       "ax1.legend()\n",
       "ax1.grid(True)\n",
       "\n",
       "# Plot accuracy\n",
       "if 'accuracy' in metrics_df.columns:\n",
       "    ax2.plot(metrics_df['step'], metrics_df['accuracy'], label='Accuracy', color='green')\n",
       "    ax2.set_xlabel('Training Steps')\n",
       "    ax2.set_ylabel('Accuracy')\n",
       "    ax2.set_title('Model Accuracy over Training')\n",
       "    ax2.legend()\n",
       "    ax2.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Example Inference\n",
       "\n",
       "Test the model on some example math problems."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def solve_math_problem(problem, max_length=100):\n",
       "    # Prepare input\n",
       "    inputs = tokenizer.encode(\n",
       "        problem,\n",
       "        return_tensors=\"pt\",\n",
       "        max_length=512,\n",
       "        truncation=True\n",
       "    ).to(device)\n",
       "\n",
       "    # Generate output\n",
       "    outputs = model.generate(\n",
       "        inputs,\n",
       "        max_length=max_length,\n",
       "        num_beams=5,\n",
       "        early_stopping=True,\n",
       "        eos_token_id=tokenizer.eos_token_id\n",
       "    )\n",
       "    \n",
       "    # Decode output\n",
       "    solution = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "    return solution\n",
       "\n",
       "# Test examples\n",
       "test_problems = [\n",
       "    \"If John has 5 apples and gives 2 to Mary, how many apples does he have left?\",\n",
       "    \"A train travels at 60 miles per hour. How far will it travel in 2.5 hours?\",\n",
       "    \"What is 15% of 200?\"\n",
       "]\n",
       "\n",
       "for problem in test_problems:\n",
       "    print(f\"\\nProblem: {problem}\")\n",
       "    solution = solve_math_problem(problem)\n",
       "    print(f\"Solution: {solution}\")"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }