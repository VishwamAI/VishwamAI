{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI GSM8k Math Training Integration\n",
    "\n",
    "This notebook demonstrates how to train VishwamAI models on mathematical reasoning tasks using the GSM8k dataset with deep thinking capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from vishwamai.training import VishwamaiTrainer\n",
    "from vishwamai.conceptual_tokenizer import ConceptualTokenizer, ConceptualTokenizerConfig\n",
    "from vishwamai.model import VishwamaiConfig, VishwamaiModel\n",
    "from vishwamai.generate import VishwamaiGenerator, GenerationConfig\n",
    "from vishwamai.deepthinking import CoTGenerationWrapper, GRPOTrainer, ReasoningDataset, create_format_reward_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "Initialize the model and tokenizer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model configuration\n",
    "model_config = VishwamaiConfig(\n",
    "    hidden_size=768,\n",
    "    vocab_size=32000,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    num_key_value_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    max_seq_len=2048,\n",
    "    n_routed_experts=4,\n",
    "    n_activated_experts=2,\n",
    "    rope_theta=10000.0,\n",
    "    layer_norm_eps=1e-5\n",
    ")\n",
    "\n",
    "# Tokenizer configuration\n",
    "tokenizer_config = ConceptualTokenizerConfig(\n",
    "    vocab_size=32000,  # Match model vocab size\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "Load the GSM8k dataset from local parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test datasets\n",
    "train_dataset = load_dataset('parquet', data_files='gsm8k/train-00000-of-00001.parquet')['train']\n",
    "test_dataset = load_dataset('parquet', data_files='gsm8k/test-00000-of-00001.parquet')['train']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Display a sample\n",
    "print(\"\\nSample question:\")\n",
    "print(train_dataset[0]['question'])\n",
    "print(\"\\nSample answer:\")\n",
    "print(train_dataset[0]['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Tokenizer and Model\n",
    "\n",
    "Set up the tokenizer and model with the defined configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = ConceptualTokenizer(tokenizer_config)\n",
    "\n",
    "model_config.torch_device = device\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = VishwamaiModel(model_config).to(device)\n",
    "\n",
    "# Initialize Chain-of-Thought wrapper\n",
    "cot_model = CoTGenerationWrapper(\n",
    "    model=model,  # Model is already on the correct device\n",
    "    tokenizer=tokenizer,\n",
    "    num_self_reflect_steps=2\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Data Collation\n",
    "\n",
    "Create a collate function to prepare batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_collate_fn(batch, tokenizer, dataset_type=\"gsm8k\"):\n",
    "    questions = [item['question'] for item in batch]\n",
    "    answers = [item['answer'] for item in batch]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    tokenized_inputs = tokenizer(\n",
    "        questions,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    tokenized_targets = tokenizer(\n",
    "        answers,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the same device as the model\n",
    "    return {\n",
    "        'input_ids': tokenized_inputs['input_ids'].to(device),\n",
    "        'concept_ids': tokenized_inputs['concept_ids'].to(device),\n",
    "        'labels': tokenized_targets['input_ids'].to(device)\n",
    "    }\n",
    "\n",
    "# Create data loaders with batch size 4\n",
    "batch_size = 4  # Default group size\n",
    "collate_fn = partial(math_collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup with GRPO\n",
    "\n",
    "Configure and initialize the GRPO trainer for optimizing Chain-of-Thought reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reward functions\n",
    "format_reward = create_format_reward_fn(tokenizer)\n",
    "\n",
    "# Define custom accuracy reward\n",
    "def math_accuracy_reward(response: str) -> float:\n",
    "    # Extract numerical answer from response\n",
    "    import re\n",
    "    answer_match = re.search(r'Answer:\\s*\\$?(\\d+)', response)\n",
    "    if not answer_match:\n",
    "        return 0.0\n",
    "    return 1.0  # In practice, compare with ground truth\n",
    "\n",
    "# Initialize GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    reward_fns={\n",
    "        'format': format_reward,\n",
    "        'accuracy': math_accuracy_reward\n",
    "    },\n",
    "    gamma=0.99,\n",
    "    beta=0.1,\n",
    "    eps_clip=0.2,\n",
    "    group_size=batch_size\n",
    ")\n",
    "\n",
    "# Train for a few steps\n",
    "sample_prompts = [train_dataset[i]['question'] for i in range(batch_size)]\n",
    "loss = grpo_trainer.train_step(sample_prompts)\n",
    "print(f\"GRPO Training Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Chain-of-Thought Generation\n",
    "\n",
    "Test the model's ability to solve math problems with explicit reasoning steps and self-reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample problem\n",
    "test_question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and sells the rest at the farmers market daily for $2 per egg. How much money does she make every day at the farmers market?\"\n",
    "\n",
    "print(\"Question:\")\n",
    "print(test_question)\n",
    "print(\"\\nGenerated Solution with Chain-of-Thought:\")\n",
    "solution = cot_model.generate(\n",
    "    test_question,\n",
    "    max_new_tokens=512  # Use standard max length\n",
    ")\n",
    "\n",
    "# Display structured output\n",
    "print(\"\\nThought Process:\")\n",
    "print(solution[0]['thought'])\n",
    "print(\"\\nSteps:\")\n",
    "for step in solution[0]['steps']:\n",
    "    print(step)\n",
    "print(\"\\nReflections:\")\n",
    "for reflection in solution[0]['reflections']:\n",
    "    print(reflection)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(solution[0]['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
