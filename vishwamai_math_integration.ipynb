{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Math Integration - GSM8k Testing\n",
    "\n",
    "Initial testing notebook for mathematical reasoning with minimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from vishwamai.model import VishwamaiConfig, VishwamaiModel\n",
    "from vishwamai.training import VishwamaiTrainer\n",
    "from vishwamai.conceptual_tokenizer import ConceptualTokenizer, ConceptualTokenizerConfig\n",
    "from vishwamai.generate import VishwamaiGenerator, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing PyTorch memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Force using CPU for initial testing\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 100\n",
      "Test samples: 10\n"
     ]
    }
   ],
   "source": [
    "# Load small subset of data for testing\n",
    "def load_test_data(num_samples=100):\n",
    "    train_full = load_dataset('parquet', data_files='gsm8k/train-00000-of-00001.parquet', split='train')\n",
    "    test_full = load_dataset('parquet', data_files='gsm8k/test-00000-of-00001.parquet', split='train')\n",
    "    \n",
    "    # Take small subsets\n",
    "    train_subset = Subset(train_full, range(min(num_samples, len(train_full))))\n",
    "    test_subset = Subset(test_full, range(min(num_samples//10, len(test_full))))\n",
    "    \n",
    "    return train_subset, test_subset, train_full\n",
    "\n",
    "train_dataset, test_dataset, full_dataset = load_test_data()\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "Tokenizer trained\n",
      "Model initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: training_data.txt\n",
      "  input_format: \n",
      "  model_prefix: conceptual\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 196\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 8\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: training_data.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 5548 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=538075\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=96\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 5548 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=288597\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 27229 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 5548\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 12622\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 12622 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11053 obj=11.3441 num_tokens=28941 num_tokens/piece=2.61838\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9251 obj=9.53149 num_tokens=29174 num_tokens/piece=3.15361\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6906 obj=9.59376 num_tokens=31134 num_tokens/piece=4.50825\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6886 obj=9.51143 num_tokens=31153 num_tokens/piece=4.52411\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5156 obj=9.71596 num_tokens=34045 num_tokens/piece=6.60299\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5154 obj=9.65954 num_tokens=34051 num_tokens/piece=6.60671\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3865 obj=9.91708 num_tokens=37502 num_tokens/piece=9.70298\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3864 obj=9.85927 num_tokens=37513 num_tokens/piece=9.70833\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2898 obj=10.1979 num_tokens=41630 num_tokens/piece=14.3651\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2898 obj=10.1289 num_tokens=41627 num_tokens/piece=14.364\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2173 obj=10.5462 num_tokens=46033 num_tokens/piece=21.1841\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2173 obj=10.4698 num_tokens=46038 num_tokens/piece=21.1864\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1629 obj=10.9893 num_tokens=50469 num_tokens/piece=30.9816\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1629 obj=10.8958 num_tokens=50473 num_tokens/piece=30.984\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1221 obj=11.5105 num_tokens=55170 num_tokens/piece=45.1843\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1220 obj=11.4068 num_tokens=55170 num_tokens/piece=45.2213\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=915 obj=12.0618 num_tokens=59592 num_tokens/piece=65.1279\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=915 obj=11.957 num_tokens=59594 num_tokens/piece=65.1301\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=686 obj=12.6828 num_tokens=64587 num_tokens/piece=94.1501\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=686 obj=12.5713 num_tokens=64585 num_tokens/piece=94.1472\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=514 obj=13.4264 num_tokens=70442 num_tokens/piece=137.047\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=514 obj=13.2741 num_tokens=70445 num_tokens/piece=137.053\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=385 obj=15.1322 num_tokens=76339 num_tokens/piece=198.283\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=385 obj=14.9076 num_tokens=76349 num_tokens/piece=198.309\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=288 obj=16.3537 num_tokens=83793 num_tokens/piece=290.948\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=288 obj=16.1513 num_tokens=83792 num_tokens/piece=290.944\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=216 obj=17.9437 num_tokens=92966 num_tokens/piece=430.398\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=216 obj=17.7256 num_tokens=92966 num_tokens/piece=430.398\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=215 obj=17.7384 num_tokens=93282 num_tokens/piece=433.87\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=215 obj=17.7309 num_tokens=93283 num_tokens/piece=433.874\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: conceptual.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: conceptual.vocab\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train tokenizer\n",
    "tokenizer_config = ConceptualTokenizerConfig(\n",
    "    vocab_size=32000,\n",
    "    max_length=128  # Reduced for testing\n",
    ")\n",
    "tokenizer = ConceptualTokenizer(tokenizer_config)\n",
    "\n",
    "# Get sample texts for tokenizer training\n",
    "train_texts = []\n",
    "for i in range(min(1000, len(full_dataset))):\n",
    "    item = full_dataset[i]\n",
    "    train_texts.append(f\"Question: {item['question']}\\nAnswer: {item['answer']}\")\n",
    "\n",
    "print(\"Training tokenizer...\")\n",
    "tokenizer.train_tokenizer(train_texts)\n",
    "print(\"Tokenizer trained\")\n",
    "\n",
    "# Initialize tiny model\n",
    "model_config = VishwamaiConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=128,  # Tiny size for testing\n",
    "    num_hidden_layers=2,  # Minimum layers\n",
    "    num_attention_heads=4,  # Reduced heads\n",
    "    max_seq_len=128,  # Reduced sequence length\n",
    "    intermediate_size=256  # Small FFN size\n",
    ")\n",
    "\n",
    "model = VishwamaiModel(model_config).to(device)\n",
    "print(\"Model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders prepared\n"
     ]
    }
   ],
   "source": [
    "class QuietVishwamaiTrainer(VishwamaiTrainer):\n",
    "    def compute_loss(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Compute loss without debug prints\"\"\"\n",
    "        labels = batch['labels']\n",
    "        model_inputs = {\n",
    "            'input_ids': batch['input_ids'],\n",
    "            'attention_mask': batch['attention_mask']\n",
    "        }\n",
    "        \n",
    "        if 'concept_ids' in batch:\n",
    "            model_inputs['concept_ids'] = batch['concept_ids']\n",
    "        \n",
    "        outputs = self.model(**model_inputs)\n",
    "        \n",
    "        # Get sequence lengths and use minimum\n",
    "        batch_size, seq_length_output, vocab_size = outputs.size()\n",
    "        batch_size_labels, seq_length_labels = labels.size()\n",
    "        min_seq_length = min(seq_length_output, seq_length_labels)\n",
    "        \n",
    "        # Truncate and reshape\n",
    "        outputs = outputs[:, :min_seq_length, :].reshape(-1, vocab_size)\n",
    "        labels = labels[:, :min_seq_length].reshape(-1)\n",
    "        \n",
    "        return torch.nn.functional.cross_entropy(outputs, labels)\n",
    "\n",
    "def math_collate_fn(batch, tokenizer):\n",
    "    questions = [b['question'] for b in batch]\n",
    "    answers = [b['answer'] for b in batch]\n",
    "    \n",
    "    inputs = [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(questions, answers)]\n",
    "    encoded_inputs = [tokenizer.encode(text) for text in inputs]\n",
    "    \n",
    "    max_len = max(len(x) for x in encoded_inputs)\n",
    "    padded_inputs = [x + [tokenizer.pad_token_id] * (max_len - len(x)) for x in encoded_inputs]\n",
    "    attention_masks = [[1] * len(x) + [0] * (max_len - len(x)) for x in encoded_inputs]\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.tensor(padded_inputs),\n",
    "        'attention_mask': torch.tensor(attention_masks),\n",
    "        'labels': torch.tensor(padded_inputs).clone()\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=partial(math_collate_fn, tokenizer=tokenizer),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=partial(math_collate_fn, tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "print(\"Data loaders prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer with quiet version\n",
    "trainer = QuietVishwamaiTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_loader,\n",
    "    eval_dataset=test_loader,\n",
    "    device=device,\n",
    "    optimizer_class=lambda params: torch.optim.AdamW(params, lr=1e-4),\n",
    "    use_wandb=False\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 25/25 [00:10<00:00,  2.38it/s, loss=9.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 10.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 25/25 [00:07<00:00,  3.14it/s, loss=8.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 9.3633\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# Setup save directory\n",
    "save_dir = Path(\"gsm8k_test_model\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Train for few steps\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train(\n",
    "        num_epochs=2,\n",
    "        save_dir=save_dir,\n",
    "        evaluation_steps=10,\n",
    "        save_steps=50,\n",
    "        logging_steps=5,\n",
    "        fp16=False  # Disable mixed precision since we're on CPU\n",
    "    )\n",
    "    print(\"Training completed\")\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer...\n",
      "\n",
      "Question: If John has 5 apples and gives 2 to Mary, how many apples does John have left?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "try:\n",
    "    generator = VishwamaiGenerator(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        config=GenerationConfig(\n",
    "            max_length=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    )\n",
    "\n",
    "    test_question = \"If John has 5 apples and gives 2 to Mary, how many apples does John have left?\"\n",
    "    print(\"Generating answer...\")\n",
    "    generated = generator.generate(test_question)\n",
    "    print(f\"\\nQuestion: {test_question}\")\n",
    "    print(f\"Answer: {generated[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Generation error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
