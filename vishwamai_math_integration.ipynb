{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VishwamAI Math Integration Tutorial\n",
    "\n",
    "This notebook demonstrates how to integrate mathematical reasoning capabilities with VishwamAI using the GSM8K dataset. We'll cover:\n",
    "\n",
    "1. Dataset preparation and loading\n",
    "2. Model and tokenizer configuration\n",
    "3. Training pipeline setup\n",
    "4. Evaluation and visualization\n",
    "5. Example inference\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from vishwamai.model import VishwamaiModel, VishwamaiConfig\n",
    "from vishwamai.conceptual_tokenizer import ConceptualTokenizer, ConceptualTokenizerConfig\n",
    "from vishwamai.training import VishwamaiTrainer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "We'll use the GSM8K (Grade School Math 8K) dataset, which contains math word problems with step-by-step solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 7473\n",
      "Test examples: 1319\n",
      "\n",
      "Sample problem:\n",
      "Question: Bob is tilling a plot of his garden. The plot is 110 feet wide by 120 feet long. His tiller digs a swath two feet wide, and he can till 1 foot of ground in about 2 seconds. How long will it take him to till this plot of land, in minutes?\n",
      "Answer: If Bob goes along the side that's 120 feet long, he will till 110 / 2 = 55 rows.\n",
      "Each of these rows are 120 feet long, so he will push the tiller a total of 120 * 55 = <<120*55=6600>>6,600 feet.\n",
      "He tills 1 linear foot of ground every 2 seconds, so it will take him 2 * 6,600 = 13,200 seconds to till this plot\n",
      "13,200 seconds is 13,2000 / 60 = <<13200/60=220>>220 minutes\n",
      "#### 220\n"
     ]
    }
   ],
   "source": [
    "# Load GSM8K dataset\n",
    "train_df = pd.read_parquet('math/gsm8k/train-00000-of-00001.parquet')\n",
    "test_df = pd.read_parquet('math/gsm8k/test-00000-of-00001.parquet')\n",
    "\n",
    "print(f\"Training examples: {len(train_df)}\")\n",
    "print(f\"Test examples: {len(test_df)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample problem:\")\n",
    "sample_idx = np.random.randint(len(train_df))\n",
    "print(f\"Question: {train_df.iloc[sample_idx]['question']}\")\n",
    "print(f\"Answer: {train_df.iloc[sample_idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Tokenize inputs with padding and truncation\n",
    "        inputs = self.tokenizer.encode(\n",
    "            question,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        labels = self.tokenizer.encode(\n",
    "            answer,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'attention_mask': torch.ones(len(inputs), dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "Configure the model and tokenizer with appropriate parameters for math reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_config = VishwamaiConfig(\n",
    "    hidden_size=3072,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    vocab_size=32000,\n",
    "    layer_norm_eps=1e-12,\n",
    "    pad_token_id=0,\n",
    "    num_key_value_heads=12,  # Setting this equal to num_attention_heads\n",
    "    rope_theta=10000  # Using default value\n",
    ")\n",
    "\n",
    "# Tokenizer configuration\n",
    "tokenizer_config = ConceptualTokenizerConfig(\n",
    "    vocab_size=32000,\n",
    "    max_length=512,\n",
    "    concept_tokens=[\"math\", \"algebra\", \"arithmetic\", \"geometry\"],\n",
    "    reasoning_tokens=[\"equals\", \"therefore\", \"because\", \"solve\", \"calculate\"],\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\"\n",
    ")\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = VishwamaiModel(model_config)\n",
    "tokenizer = ConceptualTokenizer(tokenizer_config)\n",
    "\n",
    "print(\"Model and tokenizer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup\n",
    "\n",
    "Configure the training pipeline with appropriate hyperparameters and optimization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = MathDataset(train_df, tokenizer)\n",
    "val_dataset = MathDataset(test_df, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "device = \"cpu\"  # Use CPU for training\n",
    "model = model.to(device)\n",
    "\n",
    "trainer = VishwamaiTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_loader,\n",
    "    eval_dataset=val_loader,\n",
    "    device=device,\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    "    scheduler_class=torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    use_wandb=False  # Disable wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/468 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_60027/241198457.py\", line 16, in __getitem__\n    inputs = self.tokenizer.encode(\nTypeError: ConceptualTokenizer.encode() got an unexpected keyword argument 'truncation'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     metrics_history\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable mixed precision training\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/VishwamAI/vishwamai/training.py:56\u001b[0m, in \u001b[0;36mVishwamaiTrainer.train\u001b[0;34m(self, num_epochs, save_dir, evaluation_steps, save_steps, logging_steps, gradient_accumulation_steps, max_grad_norm, fp16)\u001b[0m\n\u001b[1;32m     53\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# Compute loss using the compute_loss method\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1465\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1491\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_60027/241198457.py\", line 16, in __getitem__\n    inputs = self.tokenizer.encode(\nTypeError: ConceptualTokenizer.encode() got an unexpected keyword argument 'truncation'\n"
     ]
    }
   ],
   "source": [
    "# Training metrics collector\n",
    "metrics_history = []\n",
    "\n",
    "def metric_callback(metrics):\n",
    "    metrics_history.append(metrics)\n",
    "    \n",
    "# Train the model\n",
    "trainer.train(\n",
    "    num_epochs=10,\n",
    "    save_dir=\"./checkpoints\",\n",
    "    evaluation_steps=100,\n",
    "    save_steps=1000,\n",
    "    logging_steps=10,\n",
    "    fp16=False  # Disable mixed precision training\n",
    ")\n",
    "\n",
    "print(\"Training completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metrics for visualization\n",
    "metrics_df = pd.DataFrame(metrics_history)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot training and validation loss\n",
    "ax1.plot(metrics_df['step'], metrics_df['train_loss'], label='Training Loss')\n",
    "if 'eval_loss' in metrics_df.columns:\n",
    "    ax1.plot(metrics_df['step'], metrics_df['eval_loss'], label='Validation Loss')\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "if 'accuracy' in metrics_df.columns:\n",
    "    ax2.plot(metrics_df['step'], metrics_df['accuracy'], label='Accuracy', color='green')\n",
    "    ax2.set_xlabel('Training Steps')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Model Accuracy over Training')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Inference\n",
    "\n",
    "Test the model on some example math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_math_problem(problem, max_length=100):\n",
    "    # Prepare input\n",
    "    inputs = tokenizer.encode(\n",
    "        problem,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    solution = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return solution\n",
    "\n",
    "# Test examples\n",
    "test_problems = [\n",
    "    \"If John has 5 apples and gives 2 to Mary, how many apples does he have left?\",\n",
    "    \"A train travels at 60 miles per hour. How far will it travel in 2.5 hours?\",\n",
    "    \"What is 15% of 200?\"\n",
    "]\n",
    "\n",
    "for problem in test_problems:\n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    solution = solve_math_problem(problem)\n",
    "    print(f\"Solution: {solution}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
