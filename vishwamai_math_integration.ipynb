{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Optimized VishwamAI Math Integration\n",
    "\n",
    "This notebook demonstrates the mathematical capabilities of VishwamAI optimized for systems with limited memory, including:\n",
    "- Problem generation with varying difficulty levels\n",
    "- Step-by-step problem solving\n",
    "- Socratic method for mathematical reasoning\n",
    "- Memory-efficient training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll import required libraries and set up memory-efficient configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.cuda.amp as amp\n",
    "from accelerate import Accelerator\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "\n",
    "from vishwamai.architecture import VishwamaiModel, VishwamaiConfig, init_model\n",
    "from vishwamai.toknizer import ConceptualTokenizer, ConceptualTokenizerConfig\n",
    "from vishwamai.generate import generate\n",
    "\n",
    "# Initialize accelerator for memory-efficient operations\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# Set up automatic mixed precision\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Initialize the VishwamAI model with memory-optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model configuration optimized for 4GB VRAM\n",
    "config = VishwamaiConfig(\n",
    "    vocab_size=32000,\n",
    "    max_seq_length=2048,  # Reduced from 8192\n",
    "    dim=2048,  # Reduced from 4096\n",
    "    depth=24,  # Reduced from 32\n",
    "    num_heads=16,  # Reduced from 32\n",
    "    mlp_ratio=2.67,  # Reduced from 4.0\n",
    "    dropout=0.1,\n",
    "    pad_token_id=0,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    mask_token_id=3\n",
    ")\n",
    "\n",
    "# Initialize model using memory-efficient initialization\n",
    "model = init_model(config, device, memory_efficient=True)\n",
    "\n",
    "# Initialize tokenizer with mathematical concepts\n",
    "tokenizer_config = ConceptualTokenizerConfig(\n",
    "    vocab_size=config.vocab_size,\n",
    "    max_length=config.max_seq_length,\n",
    "    model_type=\"unigram\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    concept_tokens=[\n",
    "        \"MATH\", \"PROBLEM\", \"SOLUTION\", \"STEP\",\n",
    "        \"EQUATION\", \"VARIABLE\", \"FUNCTION\", \"PROOF\"\n",
    "    ],\n",
    "    reasoning_tokens=[\n",
    "        \"BECAUSE\", \"THEREFORE\", \"IF\", \"THEN\",\n",
    "        \"GIVEN\", \"IMPLIES\", \"SUPPOSE\", \"CONCLUDE\"\n",
    "    ]\n",
    ")\n",
    "tokenizer = ConceptualTokenizer(tokenizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "Load the GSM8K dataset with memory-efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Memory-efficient dataset loading\n",
    "def load_datasets(chunk_size=1000):\n",
    "    # Load GSM8K dataset in chunks\n",
    "    train_data = pd.read_parquet('gsm8k/train-00000-of-00001.parquet', columns=['question', 'answer'])\n",
    "    test_data = pd.read_parquet('gsm8k/test-00000-of-00001.parquet', columns=['question', 'answer'])\n",
    "    \n",
    "    # Use references instead of copies for memory efficiency\n",
    "    socratic_train = train_data\n",
    "    socratic_test = test_data\n",
    "    \n",
    "    return train_data, test_data, socratic_train, socratic_test\n",
    "\n",
    "train_data, test_data, socratic_train, socratic_test = load_datasets()\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient MathAI Class\n",
    "\n",
    "Define the main class with optimized memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MemoryEfficientMathAI:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = accelerator.device\n",
    "        self.model = accelerator.prepare(self.model)\n",
    "        self.scaler = amp.GradScaler()\n",
    "        \n",
    "    def generate_text(self, prompt, max_length=200, temperature=0.7):\n",
    "        \"\"\"Memory-efficient text generation.\"\"\"\n",
    "        if not prompt.startswith(self.tokenizer.config.bos_token):\n",
    "            prompt = self.tokenizer.config.bos_token + prompt\n",
    "            \n",
    "        # Process input in smaller chunks if needed\n",
    "        chunk_size = 128\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = generate(\n",
    "                self.model,\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.config.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.config.eos_token_id,\n",
    "                bos_token_id=self.tokenizer.config.bos_token_id,\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def generate_problem(self, topic, difficulty):\n",
    "        \"\"\"Generate a math problem with memory optimization.\"\"\"\n",
    "        prompt = f\"Generate a {difficulty} math problem about {topic}:\\n\"\n",
    "        torch.cuda.empty_cache()  # Clear cache before generation\n",
    "        return self.generate_text(prompt)\n",
    "    \n",
    "    def solve_problem(self, problem):\n",
    "        \"\"\"Memory-efficient problem solving.\"\"\"\n",
    "        prompt = f\"Solve this math problem step by step:\\n{problem}\\n\\nSolution:\"\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.generate_text(prompt, max_length=500, temperature=0.3)\n",
    "    \n",
    "    def socratic_solve(self, problem):\n",
    "        \"\"\"Memory-efficient Socratic method solution.\"\"\"\n",
    "        prompt = f\"Break down and solve this problem using the Socratic method:\\n{problem}\"\n",
    "        torch.cuda.empty_cache()\n",
    "        return self.generate_text(prompt, max_length=1000, temperature=0.3)\n",
    "\n",
    "# Initialize optimized MathAI\n",
    "math_ai = MemoryEfficientMathAI(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Training\n",
    "\n",
    "Define optimized training functions with gradient accumulation and mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_epoch(model, optimizer, train_dataloader, accumulation_steps=4):\n",
    "    \"\"\"Memory-efficient training with gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            loss = loss / accumulation_steps\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    \"\"\"Memory-efficient evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return total_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage with Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate problems with memory management\n",
    "problems = [\n",
    "    (\"algebra\", \"intermediate\"),\n",
    "    (\"geometry\", \"advanced\"),\n",
    "    (\"calculus\", \"beginner\")\n",
    "]\n",
    "\n",
    "for topic, difficulty in problems:\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Generating {difficulty} {topic} problem:\")\n",
    "    \n",
    "    # Clear cache before each generation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    problem = math_ai.generate_problem(topic, difficulty)\n",
    "    print(\"\\nProblem:\")\n",
    "    print(problem)\n",
    "    \n",
    "    print(\"\\nStandard Solution:\")\n",
    "    solution = math_ai.solve_problem(problem)\n",
    "    print(solution)\n",
    "    \n",
    "    print(\"\\nSocratic Method Solution:\")\n",
    "    socratic = math_ai.socratic_solve(problem)\n",
    "    print(socratic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
